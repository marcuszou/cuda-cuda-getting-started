{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e641a43-a404-4237-b22c-76f7e7ffe194",
   "metadata": {},
   "source": [
    "# PyTorch for Beginners 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1735fab-b06d-4438-83f2-e29592be7382",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Introduction\n",
    "When machine learning with Python, you have multiple options for which library or framework to use. However, if you're moving toward deep learning, you should probably use either TensorFlow or PyTorch, the two most famous deep learning frameworks.\n",
    "\n",
    "In this article, we'll go through a quick introduction to the PyTorch framework, going all the way from the initial concepts to the training and testing of the first image classification model.\n",
    "\n",
    "We won't dive deep into learning complex concepts and mathematics, as this article intends to be a more hands-on approach for how to start with PyTorch as a tool, not with deep learning as a concept.\n",
    "\n",
    "Therefore, we assume you have some intermediate Python knowledge--including classes and object-oriented programming--and you're familiar with the main concepts of deep learning.\n",
    "\n",
    "### PyTorch\n",
    "PyTorch is a powerful, yet easy-to-use deep learning library for Python, mainly used for applications such as computer vision and natural language processing.\n",
    "\n",
    "While TensorFlow was developed by Google, PyTorch was developed by Facebook's AI Research Group, which has recently shifted management of the framework to the newly created PyTorch Foundation, which is under the supervision of the Linux Foundation.\n",
    "\n",
    "The flexibility of PyTorch allows easy integration of new data types and algorithms, and the framework is also efficient and scalable, since it was designed to minimize the number of computations required and to be compatible with a variety of hardware architectures.\n",
    "\n",
    "### Tensors\n",
    "In deep learning, tensors are a fundamental data structure that is very similar to arrays and matrices, with which we can efficiently perform mathematical operations on large sets of data. A tensor can be represented as a matrix, but also as a vector, a scalar, or a higher-dimensional array.\n",
    "\n",
    "To make it easier to visualize, you can think of a tensor as a simple array containing scalars or other arrays. On PyTorch, a tensor is a structure very similar to a `ndarray`, with the difference that they are <font color=\"red\">capable of running on a GPU</font>, which dramatically speeds up the computational process.\n",
    "\n",
    "It's simple to create a tensor from a NumPy `ndarray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f9a654c-4f51-47cc-a26f-f7609dee428b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "ndarray = np.array([0, 1, 2])\n",
    "t = torch.from_numpy(ndarray)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd97fd4-d0ad-4739-8657-1c0d84aca959",
   "metadata": {},
   "source": [
    "A tensor on PyTorch has three attributes:\n",
    "\n",
    "- shape: the size of the tensor\n",
    "- data type: the type of data stored in the tensor\n",
    "- device: the device in which the tensor is stored\n",
    "\n",
    "If we print the attributes from the tensor we created, we'll have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9aa50b-8120-4b9f-ae84-4a2408c8131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.int64\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t.dtype)\n",
    "print(t.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212f19e-65f0-4836-ab2b-c89d186b5ae2",
   "metadata": {},
   "source": [
    "This means we have a one-dimensional tensor with the size 3, containing integers stored in the CPU.\n",
    "\n",
    "We can always instantiate a tensor from a Python list, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7308f48e-8a7a-4b65-89fe-85b639463a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([0, 1, 2])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87bc21-f503-40e3-bd33-dc90a3d5eb33",
   "metadata": {},
   "source": [
    "Tensors can also be multidimensional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d79bc9-4b25-4003-8f55-dc1871fc3f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "ndarray = np.array([[0, 1, 2], [3, 4, 5]])\n",
    "t = torch.from_numpy(ndarray)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed69469-8120-4546-a3f1-0c8431f5b634",
   "metadata": {},
   "source": [
    "It's also possible to create a tensor from another tensor. In this case, the new tensor inherits the characteristics of the initial one. The example below creates a tensor with random numbers based on the previously created tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c5bcde-556a-4e6f-b0d6-fd53c8b02453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0119, 0.6121, 0.7478],\n",
      "        [0.3712, 0.4346, 0.7258]])\n"
     ]
    }
   ],
   "source": [
    "new_t = torch.rand_like(t, dtype=torch.float)\n",
    "print(new_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f720df-c8bf-4f03-98f7-edcdbc5720f3",
   "metadata": {},
   "source": [
    "Note that the rand_like() function creates a new tensor with shape (2, 2). However, as the function returns values from 0 to 1, we had to overwrite the data type to float.\n",
    "\n",
    "We can also create a random tensor simply from the shape we expect it to have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "454f598b-2e7c-46c1-b0c6-1bf477ea573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6515, 0.1857, 0.8237],\n",
      "        [0.9940, 0.7458, 0.0879],\n",
      "        [0.0711, 0.3056, 0.0090]])\n"
     ]
    }
   ],
   "source": [
    "my_shape = (3, 3)\n",
    "rand_t = torch.rand(my_shape)\n",
    "print(rand_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603fb73e-29cd-446b-932b-e3719e07e771",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tensor Operations\n",
    "Just like in NumPy, there are multiple possible operations we can perform with tensors--like slicing, transposing, and multiplying matrices, among others.\n",
    "\n",
    "The slicing of a tensor is done exactly like any other array structure in Python. Consider the tensor below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f24978e2-04b8-4be1-bdb6-1b9f4600903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "zeros_tensor = torch.zeros((2, 3))\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc7c66-84de-4284-8521-aecdfae239b5",
   "metadata": {},
   "source": [
    "We can easily index the first row or the first column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d0c7979-2076-4065-97df-d2eca385a272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(zeros_tensor[1])\n",
    "print(zeros_tensor[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b2b2cb-a986-41e5-9a4e-555900fe1beb",
   "metadata": {},
   "source": [
    "We can also have this tensor transposed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20cef047-7c9a-4926-ade9-a334b1acb9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "transposed = zeros_tensor.T\n",
    "print(transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c97a05-e092-4388-bdd7-d42278691769",
   "metadata": {},
   "source": [
    "Finally, we can multiply the tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4c373d2-5a6d-4059-a665-e0aeac80a2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "ones_tensor = torch.ones(3, 3)\n",
    "product = torch.matmul(zeros_tensor, ones_tensor)\n",
    "print(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5173d0-3eb6-431a-ad73-3202df6d3bf3",
   "metadata": {},
   "source": [
    "Notice that we used the zeros and ones function to create a tensor containing only zeros and ones with the shape we passed.\n",
    "\n",
    "These operations are just a fraction of what PyTorch can do. However, the purpose of this article is not to cover each of them, but to give a general idea of how they work. If you want to learn more, PyTorch has a complete documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eaee03-0572-426f-9ab5-16c7b63c3bd6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loading Data\n",
    "PyTorch comes with a built-in module that provides ready-to-use datasets for many deep learning applications, such as computer vision, speech recognition, and natural language processing. This means that it's possible to build your own neural network without the need to collect and process data yourself.\n",
    "\n",
    "As an example, we'll download the MNIST dataset. The MNIST is a dataset of images of handwritten digits, containing 60 thousand samples and a test set of 10 thousand images.\n",
    "\n",
    "We'll use the `datasets` module from `torchvision` to download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a58ee3-7add-42be-bb2e-0307b549713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a78f37f6-00b9-4be4-88ab-84bb8032f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.MNIST(root=\".\", train=True, download=True, transform=ToTensor())\n",
    "\n",
    "test_data = datasets.MNIST(root=\".\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f5589-efb0-4c78-80b2-441d5c25f38b",
   "metadata": {},
   "source": [
    "You will have to wait for a bit for the downloading from internet finishes.\n",
    "\n",
    "Inside the downloading function, we have the following parameters:\n",
    "\n",
    "- __root__: the directory where the data will be saved. You can pass a string with the directory's path. A dot (as seen in the example) will save the files in the same directory you're in.\n",
    "\n",
    "- __train__: used to inform PyTorch whether you're downloading the train or test set.\n",
    "\n",
    "- __download__: whether to download the data if it's already unavailable at the path you specified.\n",
    "\n",
    "- __transform__: to transform the data. In our code, we select tensor.\n",
    "\n",
    "If we print the first element of the train set, we'll see the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3113f75f-c92f-46a4-a4f7-b21bd94c6919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d649d949-c1be-4985-886d-2b97d4c689fb",
   "metadata": {},
   "source": [
    "The above tensor is just a small part of the entire element, as it would be too big to display.\n",
    "\n",
    "This bunch of numbers may not mean anything to us, and since they represent images, we can use matplotlib to visualize them as actual images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "619f1246-717e-47a1-8df3-471cad23caf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 5, 5\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d7f93-7f12-4850-8cc5-8876068cbe43",
   "metadata": {},
   "source": [
    "We can also use the classes attribute to see the classes inside the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98c863b1-d76c-44bb-9ecf-fc226d978442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - zero',\n",
       " '1 - one',\n",
       " '2 - two',\n",
       " '3 - three',\n",
       " '4 - four',\n",
       " '5 - five',\n",
       " '6 - six',\n",
       " '7 - seven',\n",
       " '8 - eight',\n",
       " '9 - nine']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03438e34-8b2d-49a2-92d9-bebab6ab5686",
   "metadata": {},
   "source": [
    "When the model is trained, it can receive new inputs, then classify as one of these classes.\n",
    "\n",
    "Now that we have downloaded the data, we'll use the DataLoader. This enables us to iterate over the dataset in mini-batches instead of one observation at a time, and to shuffle the data while training the models. Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "984fcae6-9ffb-4609-9c37-a14ff4bf9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaded_train = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "loaded_test = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e77a45-059c-49b0-b41a-f49cdfe59bd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Neural Networks\n",
    "In deep learning, a neural network is a type of algorithm used to model data with complex patterns. A neural network attempts to simulate the functioning of the human brain through multiple layers connected by processing nodes, which behave like human neurons. These layers connected by nodes create a complex net that is able to process and understand huge amounts of complex data.\n",
    "\n",
    "In PyTorch, everything related to neural networks is built using the <font color=\"red\">torch.nn</font> module. The network itself is written as a class that inherits from <font color=\"red\">nn.Module</font>, and, inside the class, we'll use <font color=\"red\">nn</font> to build the layers. The following is a simple implementation taken from the PyTorch [documentation](https://pytorch.org/tutorials/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cba5c9c1-91db-4f79-9fe6-6c7fd88a3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc6ffd-24fa-44b0-961b-f4d9c2f36423",
   "metadata": {},
   "source": [
    "Although it's out of the scope of this article to go deep into what the layers are, how they work, and how to implement them, let's take a quick dive into what the above code does.\n",
    "\n",
    "- The `nn.Flaten` is responsible for transforming the data from multidimensional to one dimension only.\n",
    "\n",
    "- The `nn.Sequential` container creates a sequence of layers inside the network.\n",
    "\n",
    "- Inside the container, we have layers. Each type of layer transforms the data in a different way, and there are numerous ways to implement the layers in a neural network.\n",
    "\n",
    "- The forward function is the function called when the model is executed; however, we should not call it directly.\n",
    "\n",
    "The following line instantiates our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00b41777-dcf3-4f4f-aeca-cbf5d5bdf4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d60d0-b896-4bf0-9fac-b21845ea1ef0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training the Neural Network\n",
    "Now that we have defined our neural network, we can put it to use. Before starting the training, we should first set a loss function. The loss function measures how far our model is from the correct results, and it's what we'll try to minimize during the training of the network. Cross-entropy is a common loss function used for classification tasks, and it's the one we'll use. We should initialize the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a472c9e-1a1b-48ba-84f8-7db80d9595a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1abc4-1299-4001-a851-8634ebc7cacb",
   "metadata": {},
   "source": [
    "One last step before training is to set an optimization algorithm. Such an algorithm will be in charge of adjusting the model during the training process in order to minimize the error measured by the loss function we chose above. A common choice for this kind of task is the stochastic gradient descent algorithm. PyTorch, however, has several other possibilities that you can become familiar with [here](https://pytorch.org/docs/stable/optim.html). Below is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40d6e8f1-47d1-4ac0-9518-b68404427e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f093de-02ad-43fb-981a-2926f736e9d0",
   "metadata": {},
   "source": [
    "The `lr` parameter is the learning rate, which represents the speed at which the model's parameters will be updated during each iteration in training.\n",
    "\n",
    "Finally, it's time to train and test the network. For each of these tasks, we'll implement a function. The train function consists of looping through the data one batch at a time, using the optimizer to adjust the model, and computing the prediction and the loss. This is PyTorch's standard implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca808877-2f0f-4cf3-ad4e-6dc98e6d53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b3c52-bc16-40ed-b6f7-cf4bcb3957d6",
   "metadata": {},
   "source": [
    "Notice that for each iteration, we get the data to feed the model, but also keep track of the number of the batch so we can print the loss and the current batch every 100 iterations.\n",
    "\n",
    "And then we have the test function, which computes the accuracy and the loss, this time using the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8dfc794-22ee-4039-9ca7-d150fbf06983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821bd72-c829-4934-bdad-9ff300289e05",
   "metadata": {},
   "source": [
    "We then set the number of epochs to train our model. An epoch consists of an iteration over the dataset. For instance, if we set `epochs=5`, it means we'll go through the entire dataset 5 times with the neural network training and testing. The more we train, the better the results.\n",
    "\n",
    "This is PyTorch's implementation and the output of such a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62552a36-c05a-4c27-a05d-213e918f48cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305296  [   64/60000]\n",
      "loss: 2.305291  [  128/60000]\n",
      "loss: 2.296259  [  192/60000]\n",
      "loss: 2.293485  [  256/60000]\n",
      "loss: 2.298343  [  320/60000]\n",
      "loss: 2.306224  [  384/60000]\n",
      "loss: 2.290694  [  448/60000]\n",
      "loss: 2.305690  [  512/60000]\n",
      "loss: 2.306690  [  576/60000]\n",
      "loss: 2.299942  [  640/60000]\n",
      "loss: 2.293857  [  704/60000]\n",
      "loss: 2.298441  [  768/60000]\n",
      "loss: 2.303237  [  832/60000]\n",
      "loss: 2.298211  [  896/60000]\n",
      "loss: 2.298782  [  960/60000]\n",
      "loss: 2.302055  [ 1024/60000]\n",
      "loss: 2.298678  [ 1088/60000]\n",
      "loss: 2.308218  [ 1152/60000]\n",
      "loss: 2.299593  [ 1216/60000]\n",
      "loss: 2.297101  [ 1280/60000]\n",
      "loss: 2.298998  [ 1344/60000]\n",
      "loss: 2.288437  [ 1408/60000]\n",
      "loss: 2.295787  [ 1472/60000]\n",
      "loss: 2.296215  [ 1536/60000]\n",
      "loss: 2.300532  [ 1600/60000]\n",
      "loss: 2.299108  [ 1664/60000]\n",
      "loss: 2.287365  [ 1728/60000]\n",
      "loss: 2.302085  [ 1792/60000]\n",
      "loss: 2.298737  [ 1856/60000]\n",
      "loss: 2.298136  [ 1920/60000]\n",
      "loss: 2.301948  [ 1984/60000]\n",
      "loss: 2.293937  [ 2048/60000]\n",
      "loss: 2.296678  [ 2112/60000]\n",
      "loss: 2.296025  [ 2176/60000]\n",
      "loss: 2.294307  [ 2240/60000]\n",
      "loss: 2.300754  [ 2304/60000]\n",
      "loss: 2.297863  [ 2368/60000]\n",
      "loss: 2.301301  [ 2432/60000]\n",
      "loss: 2.294507  [ 2496/60000]\n",
      "loss: 2.300121  [ 2560/60000]\n",
      "loss: 2.293409  [ 2624/60000]\n",
      "loss: 2.295926  [ 2688/60000]\n",
      "loss: 2.295635  [ 2752/60000]\n",
      "loss: 2.298958  [ 2816/60000]\n",
      "loss: 2.298996  [ 2880/60000]\n",
      "loss: 2.297248  [ 2944/60000]\n",
      "loss: 2.293192  [ 3008/60000]\n",
      "loss: 2.296116  [ 3072/60000]\n",
      "loss: 2.303741  [ 3136/60000]\n",
      "loss: 2.295336  [ 3200/60000]\n",
      "loss: 2.299411  [ 3264/60000]\n",
      "loss: 2.293037  [ 3328/60000]\n",
      "loss: 2.308862  [ 3392/60000]\n",
      "loss: 2.303056  [ 3456/60000]\n",
      "loss: 2.304876  [ 3520/60000]\n",
      "loss: 2.298819  [ 3584/60000]\n",
      "loss: 2.286377  [ 3648/60000]\n",
      "loss: 2.297119  [ 3712/60000]\n",
      "loss: 2.293025  [ 3776/60000]\n",
      "loss: 2.293071  [ 3840/60000]\n",
      "loss: 2.291878  [ 3904/60000]\n",
      "loss: 2.297675  [ 3968/60000]\n",
      "loss: 2.297364  [ 4032/60000]\n",
      "loss: 2.294734  [ 4096/60000]\n",
      "loss: 2.287645  [ 4160/60000]\n",
      "loss: 2.294114  [ 4224/60000]\n",
      "loss: 2.291502  [ 4288/60000]\n",
      "loss: 2.298377  [ 4352/60000]\n",
      "loss: 2.291747  [ 4416/60000]\n",
      "loss: 2.296510  [ 4480/60000]\n",
      "loss: 2.294349  [ 4544/60000]\n",
      "loss: 2.294960  [ 4608/60000]\n",
      "loss: 2.301758  [ 4672/60000]\n",
      "loss: 2.294722  [ 4736/60000]\n",
      "loss: 2.300802  [ 4800/60000]\n",
      "loss: 2.298439  [ 4864/60000]\n",
      "loss: 2.298680  [ 4928/60000]\n",
      "loss: 2.293049  [ 4992/60000]\n",
      "loss: 2.297007  [ 5056/60000]\n",
      "loss: 2.293333  [ 5120/60000]\n",
      "loss: 2.296402  [ 5184/60000]\n",
      "loss: 2.300141  [ 5248/60000]\n",
      "loss: 2.303875  [ 5312/60000]\n",
      "loss: 2.286861  [ 5376/60000]\n",
      "loss: 2.302872  [ 5440/60000]\n",
      "loss: 2.291397  [ 5504/60000]\n",
      "loss: 2.290371  [ 5568/60000]\n",
      "loss: 2.295825  [ 5632/60000]\n",
      "loss: 2.304546  [ 5696/60000]\n",
      "loss: 2.292364  [ 5760/60000]\n",
      "loss: 2.297544  [ 5824/60000]\n",
      "loss: 2.297823  [ 5888/60000]\n",
      "loss: 2.294897  [ 5952/60000]\n",
      "loss: 2.287528  [ 6016/60000]\n",
      "loss: 2.292287  [ 6080/60000]\n",
      "loss: 2.297197  [ 6144/60000]\n",
      "loss: 2.297629  [ 6208/60000]\n",
      "loss: 2.290323  [ 6272/60000]\n",
      "loss: 2.296613  [ 6336/60000]\n",
      "loss: 2.299320  [ 6400/60000]\n",
      "loss: 2.288907  [ 6464/60000]\n",
      "loss: 2.286292  [ 6528/60000]\n",
      "loss: 2.304768  [ 6592/60000]\n",
      "loss: 2.292703  [ 6656/60000]\n",
      "loss: 2.280453  [ 6720/60000]\n",
      "loss: 2.296114  [ 6784/60000]\n",
      "loss: 2.296005  [ 6848/60000]\n",
      "loss: 2.297853  [ 6912/60000]\n",
      "loss: 2.294417  [ 6976/60000]\n",
      "loss: 2.302337  [ 7040/60000]\n",
      "loss: 2.287769  [ 7104/60000]\n",
      "loss: 2.299355  [ 7168/60000]\n",
      "loss: 2.300699  [ 7232/60000]\n",
      "loss: 2.283336  [ 7296/60000]\n",
      "loss: 2.284100  [ 7360/60000]\n",
      "loss: 2.288548  [ 7424/60000]\n",
      "loss: 2.299207  [ 7488/60000]\n",
      "loss: 2.294353  [ 7552/60000]\n",
      "loss: 2.287262  [ 7616/60000]\n",
      "loss: 2.296424  [ 7680/60000]\n",
      "loss: 2.289742  [ 7744/60000]\n",
      "loss: 2.295458  [ 7808/60000]\n",
      "loss: 2.288509  [ 7872/60000]\n",
      "loss: 2.294507  [ 7936/60000]\n",
      "loss: 2.292323  [ 8000/60000]\n",
      "loss: 2.300617  [ 8064/60000]\n",
      "loss: 2.294086  [ 8128/60000]\n",
      "loss: 2.293194  [ 8192/60000]\n",
      "loss: 2.296621  [ 8256/60000]\n",
      "loss: 2.292339  [ 8320/60000]\n",
      "loss: 2.293987  [ 8384/60000]\n",
      "loss: 2.292273  [ 8448/60000]\n",
      "loss: 2.292799  [ 8512/60000]\n",
      "loss: 2.292819  [ 8576/60000]\n",
      "loss: 2.294226  [ 8640/60000]\n",
      "loss: 2.300417  [ 8704/60000]\n",
      "loss: 2.296107  [ 8768/60000]\n",
      "loss: 2.294129  [ 8832/60000]\n",
      "loss: 2.292897  [ 8896/60000]\n",
      "loss: 2.289339  [ 8960/60000]\n",
      "loss: 2.289027  [ 9024/60000]\n",
      "loss: 2.293623  [ 9088/60000]\n",
      "loss: 2.296846  [ 9152/60000]\n",
      "loss: 2.290911  [ 9216/60000]\n",
      "loss: 2.295716  [ 9280/60000]\n",
      "loss: 2.291182  [ 9344/60000]\n",
      "loss: 2.284115  [ 9408/60000]\n",
      "loss: 2.295734  [ 9472/60000]\n",
      "loss: 2.288850  [ 9536/60000]\n",
      "loss: 2.294357  [ 9600/60000]\n",
      "loss: 2.297539  [ 9664/60000]\n",
      "loss: 2.288173  [ 9728/60000]\n",
      "loss: 2.294274  [ 9792/60000]\n",
      "loss: 2.297556  [ 9856/60000]\n",
      "loss: 2.285573  [ 9920/60000]\n",
      "loss: 2.289575  [ 9984/60000]\n",
      "loss: 2.293145  [10048/60000]\n",
      "loss: 2.299195  [10112/60000]\n",
      "loss: 2.290651  [10176/60000]\n",
      "loss: 2.293818  [10240/60000]\n",
      "loss: 2.295433  [10304/60000]\n",
      "loss: 2.293287  [10368/60000]\n",
      "loss: 2.293513  [10432/60000]\n",
      "loss: 2.290586  [10496/60000]\n",
      "loss: 2.287378  [10560/60000]\n",
      "loss: 2.288451  [10624/60000]\n",
      "loss: 2.298868  [10688/60000]\n",
      "loss: 2.285404  [10752/60000]\n",
      "loss: 2.290511  [10816/60000]\n",
      "loss: 2.301389  [10880/60000]\n",
      "loss: 2.303493  [10944/60000]\n",
      "loss: 2.290140  [11008/60000]\n",
      "loss: 2.293537  [11072/60000]\n",
      "loss: 2.283695  [11136/60000]\n",
      "loss: 2.283853  [11200/60000]\n",
      "loss: 2.290081  [11264/60000]\n",
      "loss: 2.288717  [11328/60000]\n",
      "loss: 2.289541  [11392/60000]\n",
      "loss: 2.293979  [11456/60000]\n",
      "loss: 2.287559  [11520/60000]\n",
      "loss: 2.293331  [11584/60000]\n",
      "loss: 2.294433  [11648/60000]\n",
      "loss: 2.297200  [11712/60000]\n",
      "loss: 2.285560  [11776/60000]\n",
      "loss: 2.292241  [11840/60000]\n",
      "loss: 2.292418  [11904/60000]\n",
      "loss: 2.293610  [11968/60000]\n",
      "loss: 2.289524  [12032/60000]\n",
      "loss: 2.296099  [12096/60000]\n",
      "loss: 2.293784  [12160/60000]\n",
      "loss: 2.290122  [12224/60000]\n",
      "loss: 2.291219  [12288/60000]\n",
      "loss: 2.294016  [12352/60000]\n",
      "loss: 2.294180  [12416/60000]\n",
      "loss: 2.287648  [12480/60000]\n",
      "loss: 2.285386  [12544/60000]\n",
      "loss: 2.283174  [12608/60000]\n",
      "loss: 2.285718  [12672/60000]\n",
      "loss: 2.289118  [12736/60000]\n",
      "loss: 2.287291  [12800/60000]\n",
      "loss: 2.293394  [12864/60000]\n",
      "loss: 2.300239  [12928/60000]\n",
      "loss: 2.292175  [12992/60000]\n",
      "loss: 2.286011  [13056/60000]\n",
      "loss: 2.294781  [13120/60000]\n",
      "loss: 2.288409  [13184/60000]\n",
      "loss: 2.288603  [13248/60000]\n",
      "loss: 2.290525  [13312/60000]\n",
      "loss: 2.287834  [13376/60000]\n",
      "loss: 2.287747  [13440/60000]\n",
      "loss: 2.281327  [13504/60000]\n",
      "loss: 2.285542  [13568/60000]\n",
      "loss: 2.289042  [13632/60000]\n",
      "loss: 2.288860  [13696/60000]\n",
      "loss: 2.300772  [13760/60000]\n",
      "loss: 2.296062  [13824/60000]\n",
      "loss: 2.293543  [13888/60000]\n",
      "loss: 2.289106  [13952/60000]\n",
      "loss: 2.281807  [14016/60000]\n",
      "loss: 2.301873  [14080/60000]\n",
      "loss: 2.290465  [14144/60000]\n",
      "loss: 2.282602  [14208/60000]\n",
      "loss: 2.287777  [14272/60000]\n",
      "loss: 2.279930  [14336/60000]\n",
      "loss: 2.288172  [14400/60000]\n",
      "loss: 2.289321  [14464/60000]\n",
      "loss: 2.284793  [14528/60000]\n",
      "loss: 2.302194  [14592/60000]\n",
      "loss: 2.294775  [14656/60000]\n",
      "loss: 2.294475  [14720/60000]\n",
      "loss: 2.290187  [14784/60000]\n",
      "loss: 2.287553  [14848/60000]\n",
      "loss: 2.291136  [14912/60000]\n",
      "loss: 2.293932  [14976/60000]\n",
      "loss: 2.280963  [15040/60000]\n",
      "loss: 2.295696  [15104/60000]\n",
      "loss: 2.283214  [15168/60000]\n",
      "loss: 2.300556  [15232/60000]\n",
      "loss: 2.295194  [15296/60000]\n",
      "loss: 2.283528  [15360/60000]\n",
      "loss: 2.280331  [15424/60000]\n",
      "loss: 2.294442  [15488/60000]\n",
      "loss: 2.290008  [15552/60000]\n",
      "loss: 2.283169  [15616/60000]\n",
      "loss: 2.289440  [15680/60000]\n",
      "loss: 2.281390  [15744/60000]\n",
      "loss: 2.288817  [15808/60000]\n",
      "loss: 2.287464  [15872/60000]\n",
      "loss: 2.285816  [15936/60000]\n",
      "loss: 2.286265  [16000/60000]\n",
      "loss: 2.291797  [16064/60000]\n",
      "loss: 2.281217  [16128/60000]\n",
      "loss: 2.278851  [16192/60000]\n",
      "loss: 2.293591  [16256/60000]\n",
      "loss: 2.290484  [16320/60000]\n",
      "loss: 2.283774  [16384/60000]\n",
      "loss: 2.283115  [16448/60000]\n",
      "loss: 2.284685  [16512/60000]\n",
      "loss: 2.274288  [16576/60000]\n",
      "loss: 2.286739  [16640/60000]\n",
      "loss: 2.287137  [16704/60000]\n",
      "loss: 2.288162  [16768/60000]\n",
      "loss: 2.285187  [16832/60000]\n",
      "loss: 2.282000  [16896/60000]\n",
      "loss: 2.287440  [16960/60000]\n",
      "loss: 2.279348  [17024/60000]\n",
      "loss: 2.288515  [17088/60000]\n",
      "loss: 2.289973  [17152/60000]\n",
      "loss: 2.282715  [17216/60000]\n",
      "loss: 2.286657  [17280/60000]\n",
      "loss: 2.287762  [17344/60000]\n",
      "loss: 2.280999  [17408/60000]\n",
      "loss: 2.276730  [17472/60000]\n",
      "loss: 2.282189  [17536/60000]\n",
      "loss: 2.279132  [17600/60000]\n",
      "loss: 2.288166  [17664/60000]\n",
      "loss: 2.288826  [17728/60000]\n",
      "loss: 2.273336  [17792/60000]\n",
      "loss: 2.280822  [17856/60000]\n",
      "loss: 2.282825  [17920/60000]\n",
      "loss: 2.284456  [17984/60000]\n",
      "loss: 2.278201  [18048/60000]\n",
      "loss: 2.288704  [18112/60000]\n",
      "loss: 2.286962  [18176/60000]\n",
      "loss: 2.283047  [18240/60000]\n",
      "loss: 2.283158  [18304/60000]\n",
      "loss: 2.286834  [18368/60000]\n",
      "loss: 2.290329  [18432/60000]\n",
      "loss: 2.282474  [18496/60000]\n",
      "loss: 2.289435  [18560/60000]\n",
      "loss: 2.279865  [18624/60000]\n",
      "loss: 2.288032  [18688/60000]\n",
      "loss: 2.287658  [18752/60000]\n",
      "loss: 2.278005  [18816/60000]\n",
      "loss: 2.284146  [18880/60000]\n",
      "loss: 2.282698  [18944/60000]\n",
      "loss: 2.288251  [19008/60000]\n",
      "loss: 2.278367  [19072/60000]\n",
      "loss: 2.296915  [19136/60000]\n",
      "loss: 2.283904  [19200/60000]\n",
      "loss: 2.280807  [19264/60000]\n",
      "loss: 2.277623  [19328/60000]\n",
      "loss: 2.281932  [19392/60000]\n",
      "loss: 2.290615  [19456/60000]\n",
      "loss: 2.288803  [19520/60000]\n",
      "loss: 2.286178  [19584/60000]\n",
      "loss: 2.278857  [19648/60000]\n",
      "loss: 2.288701  [19712/60000]\n",
      "loss: 2.270931  [19776/60000]\n",
      "loss: 2.282967  [19840/60000]\n",
      "loss: 2.282695  [19904/60000]\n",
      "loss: 2.291938  [19968/60000]\n",
      "loss: 2.276533  [20032/60000]\n",
      "loss: 2.277474  [20096/60000]\n",
      "loss: 2.287602  [20160/60000]\n",
      "loss: 2.288491  [20224/60000]\n",
      "loss: 2.282092  [20288/60000]\n",
      "loss: 2.286397  [20352/60000]\n",
      "loss: 2.278956  [20416/60000]\n",
      "loss: 2.290420  [20480/60000]\n",
      "loss: 2.277979  [20544/60000]\n",
      "loss: 2.285560  [20608/60000]\n",
      "loss: 2.274402  [20672/60000]\n",
      "loss: 2.283036  [20736/60000]\n",
      "loss: 2.281176  [20800/60000]\n",
      "loss: 2.283373  [20864/60000]\n",
      "loss: 2.283058  [20928/60000]\n",
      "loss: 2.284318  [20992/60000]\n",
      "loss: 2.291445  [21056/60000]\n",
      "loss: 2.276804  [21120/60000]\n",
      "loss: 2.270213  [21184/60000]\n",
      "loss: 2.278541  [21248/60000]\n",
      "loss: 2.280794  [21312/60000]\n",
      "loss: 2.283843  [21376/60000]\n",
      "loss: 2.279205  [21440/60000]\n",
      "loss: 2.289625  [21504/60000]\n",
      "loss: 2.279968  [21568/60000]\n",
      "loss: 2.289967  [21632/60000]\n",
      "loss: 2.277076  [21696/60000]\n",
      "loss: 2.277191  [21760/60000]\n",
      "loss: 2.284340  [21824/60000]\n",
      "loss: 2.276892  [21888/60000]\n",
      "loss: 2.295676  [21952/60000]\n",
      "loss: 2.276810  [22016/60000]\n",
      "loss: 2.282555  [22080/60000]\n",
      "loss: 2.290102  [22144/60000]\n",
      "loss: 2.285985  [22208/60000]\n",
      "loss: 2.280689  [22272/60000]\n",
      "loss: 2.284288  [22336/60000]\n",
      "loss: 2.270037  [22400/60000]\n",
      "loss: 2.289787  [22464/60000]\n",
      "loss: 2.274056  [22528/60000]\n",
      "loss: 2.280934  [22592/60000]\n",
      "loss: 2.268999  [22656/60000]\n",
      "loss: 2.280887  [22720/60000]\n",
      "loss: 2.289658  [22784/60000]\n",
      "loss: 2.279037  [22848/60000]\n",
      "loss: 2.277494  [22912/60000]\n",
      "loss: 2.287091  [22976/60000]\n",
      "loss: 2.276524  [23040/60000]\n",
      "loss: 2.281613  [23104/60000]\n",
      "loss: 2.288351  [23168/60000]\n",
      "loss: 2.280222  [23232/60000]\n",
      "loss: 2.277132  [23296/60000]\n",
      "loss: 2.288184  [23360/60000]\n",
      "loss: 2.278606  [23424/60000]\n",
      "loss: 2.284736  [23488/60000]\n",
      "loss: 2.280633  [23552/60000]\n",
      "loss: 2.289305  [23616/60000]\n",
      "loss: 2.279073  [23680/60000]\n",
      "loss: 2.263888  [23744/60000]\n",
      "loss: 2.279037  [23808/60000]\n",
      "loss: 2.283828  [23872/60000]\n",
      "loss: 2.277723  [23936/60000]\n",
      "loss: 2.285053  [24000/60000]\n",
      "loss: 2.277865  [24064/60000]\n",
      "loss: 2.282398  [24128/60000]\n",
      "loss: 2.265471  [24192/60000]\n",
      "loss: 2.276282  [24256/60000]\n",
      "loss: 2.274054  [24320/60000]\n",
      "loss: 2.271806  [24384/60000]\n",
      "loss: 2.278674  [24448/60000]\n",
      "loss: 2.280372  [24512/60000]\n",
      "loss: 2.281032  [24576/60000]\n",
      "loss: 2.274614  [24640/60000]\n",
      "loss: 2.286633  [24704/60000]\n",
      "loss: 2.280914  [24768/60000]\n",
      "loss: 2.276830  [24832/60000]\n",
      "loss: 2.281611  [24896/60000]\n",
      "loss: 2.284312  [24960/60000]\n",
      "loss: 2.278718  [25024/60000]\n",
      "loss: 2.281330  [25088/60000]\n",
      "loss: 2.277038  [25152/60000]\n",
      "loss: 2.283067  [25216/60000]\n",
      "loss: 2.272417  [25280/60000]\n",
      "loss: 2.281856  [25344/60000]\n",
      "loss: 2.273268  [25408/60000]\n",
      "loss: 2.273214  [25472/60000]\n",
      "loss: 2.284392  [25536/60000]\n",
      "loss: 2.284364  [25600/60000]\n",
      "loss: 2.279239  [25664/60000]\n",
      "loss: 2.284059  [25728/60000]\n",
      "loss: 2.276870  [25792/60000]\n",
      "loss: 2.284230  [25856/60000]\n",
      "loss: 2.284377  [25920/60000]\n",
      "loss: 2.269564  [25984/60000]\n",
      "loss: 2.288143  [26048/60000]\n",
      "loss: 2.284172  [26112/60000]\n",
      "loss: 2.279934  [26176/60000]\n",
      "loss: 2.284135  [26240/60000]\n",
      "loss: 2.276504  [26304/60000]\n",
      "loss: 2.288995  [26368/60000]\n",
      "loss: 2.289005  [26432/60000]\n",
      "loss: 2.281792  [26496/60000]\n",
      "loss: 2.272234  [26560/60000]\n",
      "loss: 2.276070  [26624/60000]\n",
      "loss: 2.276016  [26688/60000]\n",
      "loss: 2.269446  [26752/60000]\n",
      "loss: 2.281078  [26816/60000]\n",
      "loss: 2.281106  [26880/60000]\n",
      "loss: 2.286073  [26944/60000]\n",
      "loss: 2.286718  [27008/60000]\n",
      "loss: 2.275194  [27072/60000]\n",
      "loss: 2.278278  [27136/60000]\n",
      "loss: 2.279965  [27200/60000]\n",
      "loss: 2.269112  [27264/60000]\n",
      "loss: 2.274485  [27328/60000]\n",
      "loss: 2.278286  [27392/60000]\n",
      "loss: 2.281754  [27456/60000]\n",
      "loss: 2.276772  [27520/60000]\n",
      "loss: 2.276339  [27584/60000]\n",
      "loss: 2.273627  [27648/60000]\n",
      "loss: 2.278405  [27712/60000]\n",
      "loss: 2.277407  [27776/60000]\n",
      "loss: 2.282474  [27840/60000]\n",
      "loss: 2.279929  [27904/60000]\n",
      "loss: 2.283731  [27968/60000]\n",
      "loss: 2.270865  [28032/60000]\n",
      "loss: 2.277931  [28096/60000]\n",
      "loss: 2.281319  [28160/60000]\n",
      "loss: 2.271107  [28224/60000]\n",
      "loss: 2.278068  [28288/60000]\n",
      "loss: 2.284118  [28352/60000]\n",
      "loss: 2.283584  [28416/60000]\n",
      "loss: 2.283540  [28480/60000]\n",
      "loss: 2.275783  [28544/60000]\n",
      "loss: 2.276928  [28608/60000]\n",
      "loss: 2.281404  [28672/60000]\n",
      "loss: 2.271835  [28736/60000]\n",
      "loss: 2.275549  [28800/60000]\n",
      "loss: 2.270590  [28864/60000]\n",
      "loss: 2.278616  [28928/60000]\n",
      "loss: 2.272080  [28992/60000]\n",
      "loss: 2.290572  [29056/60000]\n",
      "loss: 2.264636  [29120/60000]\n",
      "loss: 2.282618  [29184/60000]\n",
      "loss: 2.282040  [29248/60000]\n",
      "loss: 2.275159  [29312/60000]\n",
      "loss: 2.268145  [29376/60000]\n",
      "loss: 2.271186  [29440/60000]\n",
      "loss: 2.277348  [29504/60000]\n",
      "loss: 2.279360  [29568/60000]\n",
      "loss: 2.282883  [29632/60000]\n",
      "loss: 2.275702  [29696/60000]\n",
      "loss: 2.285059  [29760/60000]\n",
      "loss: 2.279721  [29824/60000]\n",
      "loss: 2.276540  [29888/60000]\n",
      "loss: 2.280381  [29952/60000]\n",
      "loss: 2.264109  [30016/60000]\n",
      "loss: 2.268067  [30080/60000]\n",
      "loss: 2.275092  [30144/60000]\n",
      "loss: 2.270535  [30208/60000]\n",
      "loss: 2.284698  [30272/60000]\n",
      "loss: 2.274101  [30336/60000]\n",
      "loss: 2.279929  [30400/60000]\n",
      "loss: 2.280521  [30464/60000]\n",
      "loss: 2.281708  [30528/60000]\n",
      "loss: 2.276582  [30592/60000]\n",
      "loss: 2.279494  [30656/60000]\n",
      "loss: 2.285616  [30720/60000]\n",
      "loss: 2.280248  [30784/60000]\n",
      "loss: 2.278154  [30848/60000]\n",
      "loss: 2.281455  [30912/60000]\n",
      "loss: 2.270080  [30976/60000]\n",
      "loss: 2.283782  [31040/60000]\n",
      "loss: 2.267116  [31104/60000]\n",
      "loss: 2.273303  [31168/60000]\n",
      "loss: 2.277027  [31232/60000]\n",
      "loss: 2.282890  [31296/60000]\n",
      "loss: 2.276589  [31360/60000]\n",
      "loss: 2.278944  [31424/60000]\n",
      "loss: 2.255745  [31488/60000]\n",
      "loss: 2.271875  [31552/60000]\n",
      "loss: 2.266040  [31616/60000]\n",
      "loss: 2.266537  [31680/60000]\n",
      "loss: 2.273163  [31744/60000]\n",
      "loss: 2.276921  [31808/60000]\n",
      "loss: 2.269613  [31872/60000]\n",
      "loss: 2.273313  [31936/60000]\n",
      "loss: 2.262691  [32000/60000]\n",
      "loss: 2.276510  [32064/60000]\n",
      "loss: 2.282051  [32128/60000]\n",
      "loss: 2.272834  [32192/60000]\n",
      "loss: 2.280345  [32256/60000]\n",
      "loss: 2.280437  [32320/60000]\n",
      "loss: 2.273960  [32384/60000]\n",
      "loss: 2.273872  [32448/60000]\n",
      "loss: 2.278068  [32512/60000]\n",
      "loss: 2.281652  [32576/60000]\n",
      "loss: 2.276706  [32640/60000]\n",
      "loss: 2.270998  [32704/60000]\n",
      "loss: 2.273161  [32768/60000]\n",
      "loss: 2.269338  [32832/60000]\n",
      "loss: 2.274853  [32896/60000]\n",
      "loss: 2.274782  [32960/60000]\n",
      "loss: 2.282664  [33024/60000]\n",
      "loss: 2.282356  [33088/60000]\n",
      "loss: 2.279282  [33152/60000]\n",
      "loss: 2.268691  [33216/60000]\n",
      "loss: 2.266218  [33280/60000]\n",
      "loss: 2.272683  [33344/60000]\n",
      "loss: 2.279227  [33408/60000]\n",
      "loss: 2.282545  [33472/60000]\n",
      "loss: 2.282789  [33536/60000]\n",
      "loss: 2.283162  [33600/60000]\n",
      "loss: 2.275452  [33664/60000]\n",
      "loss: 2.277537  [33728/60000]\n",
      "loss: 2.268259  [33792/60000]\n",
      "loss: 2.267664  [33856/60000]\n",
      "loss: 2.264678  [33920/60000]\n",
      "loss: 2.274011  [33984/60000]\n",
      "loss: 2.268924  [34048/60000]\n",
      "loss: 2.267632  [34112/60000]\n",
      "loss: 2.266520  [34176/60000]\n",
      "loss: 2.265692  [34240/60000]\n",
      "loss: 2.281429  [34304/60000]\n",
      "loss: 2.272737  [34368/60000]\n",
      "loss: 2.268427  [34432/60000]\n",
      "loss: 2.286505  [34496/60000]\n",
      "loss: 2.260834  [34560/60000]\n",
      "loss: 2.283764  [34624/60000]\n",
      "loss: 2.278041  [34688/60000]\n",
      "loss: 2.271630  [34752/60000]\n",
      "loss: 2.272627  [34816/60000]\n",
      "loss: 2.276648  [34880/60000]\n",
      "loss: 2.274083  [34944/60000]\n",
      "loss: 2.285805  [35008/60000]\n",
      "loss: 2.275100  [35072/60000]\n",
      "loss: 2.266322  [35136/60000]\n",
      "loss: 2.270642  [35200/60000]\n",
      "loss: 2.271973  [35264/60000]\n",
      "loss: 2.267897  [35328/60000]\n",
      "loss: 2.280056  [35392/60000]\n",
      "loss: 2.271795  [35456/60000]\n",
      "loss: 2.277842  [35520/60000]\n",
      "loss: 2.274412  [35584/60000]\n",
      "loss: 2.264472  [35648/60000]\n",
      "loss: 2.275445  [35712/60000]\n",
      "loss: 2.271990  [35776/60000]\n",
      "loss: 2.274501  [35840/60000]\n",
      "loss: 2.270899  [35904/60000]\n",
      "loss: 2.280993  [35968/60000]\n",
      "loss: 2.274352  [36032/60000]\n",
      "loss: 2.273203  [36096/60000]\n",
      "loss: 2.279635  [36160/60000]\n",
      "loss: 2.265557  [36224/60000]\n",
      "loss: 2.271965  [36288/60000]\n",
      "loss: 2.262336  [36352/60000]\n",
      "loss: 2.267761  [36416/60000]\n",
      "loss: 2.277486  [36480/60000]\n",
      "loss: 2.277537  [36544/60000]\n",
      "loss: 2.262759  [36608/60000]\n",
      "loss: 2.267272  [36672/60000]\n",
      "loss: 2.263268  [36736/60000]\n",
      "loss: 2.268608  [36800/60000]\n",
      "loss: 2.263917  [36864/60000]\n",
      "loss: 2.277650  [36928/60000]\n",
      "loss: 2.264083  [36992/60000]\n",
      "loss: 2.260251  [37056/60000]\n",
      "loss: 2.277911  [37120/60000]\n",
      "loss: 2.268459  [37184/60000]\n",
      "loss: 2.275951  [37248/60000]\n",
      "loss: 2.269649  [37312/60000]\n",
      "loss: 2.266735  [37376/60000]\n",
      "loss: 2.274612  [37440/60000]\n",
      "loss: 2.269321  [37504/60000]\n",
      "loss: 2.277205  [37568/60000]\n",
      "loss: 2.268183  [37632/60000]\n",
      "loss: 2.267965  [37696/60000]\n",
      "loss: 2.274359  [37760/60000]\n",
      "loss: 2.269411  [37824/60000]\n",
      "loss: 2.272237  [37888/60000]\n",
      "loss: 2.274787  [37952/60000]\n",
      "loss: 2.260417  [38016/60000]\n",
      "loss: 2.270741  [38080/60000]\n",
      "loss: 2.272163  [38144/60000]\n",
      "loss: 2.273785  [38208/60000]\n",
      "loss: 2.273975  [38272/60000]\n",
      "loss: 2.270799  [38336/60000]\n",
      "loss: 2.270342  [38400/60000]\n",
      "loss: 2.257470  [38464/60000]\n",
      "loss: 2.265437  [38528/60000]\n",
      "loss: 2.267349  [38592/60000]\n",
      "loss: 2.268622  [38656/60000]\n",
      "loss: 2.274373  [38720/60000]\n",
      "loss: 2.269150  [38784/60000]\n",
      "loss: 2.264684  [38848/60000]\n",
      "loss: 2.265361  [38912/60000]\n",
      "loss: 2.261625  [38976/60000]\n",
      "loss: 2.280732  [39040/60000]\n",
      "loss: 2.281953  [39104/60000]\n",
      "loss: 2.271544  [39168/60000]\n",
      "loss: 2.266356  [39232/60000]\n",
      "loss: 2.256002  [39296/60000]\n",
      "loss: 2.263494  [39360/60000]\n",
      "loss: 2.268830  [39424/60000]\n",
      "loss: 2.273182  [39488/60000]\n",
      "loss: 2.278652  [39552/60000]\n",
      "loss: 2.274868  [39616/60000]\n",
      "loss: 2.270034  [39680/60000]\n",
      "loss: 2.271348  [39744/60000]\n",
      "loss: 2.277460  [39808/60000]\n",
      "loss: 2.275036  [39872/60000]\n",
      "loss: 2.279003  [39936/60000]\n",
      "loss: 2.273296  [40000/60000]\n",
      "loss: 2.275024  [40064/60000]\n",
      "loss: 2.264649  [40128/60000]\n",
      "loss: 2.264876  [40192/60000]\n",
      "loss: 2.277524  [40256/60000]\n",
      "loss: 2.272491  [40320/60000]\n",
      "loss: 2.260945  [40384/60000]\n",
      "loss: 2.264481  [40448/60000]\n",
      "loss: 2.270422  [40512/60000]\n",
      "loss: 2.264693  [40576/60000]\n",
      "loss: 2.257759  [40640/60000]\n",
      "loss: 2.282826  [40704/60000]\n",
      "loss: 2.271491  [40768/60000]\n",
      "loss: 2.256853  [40832/60000]\n",
      "loss: 2.269838  [40896/60000]\n",
      "loss: 2.268521  [40960/60000]\n",
      "loss: 2.266688  [41024/60000]\n",
      "loss: 2.275503  [41088/60000]\n",
      "loss: 2.266353  [41152/60000]\n",
      "loss: 2.265156  [41216/60000]\n",
      "loss: 2.260848  [41280/60000]\n",
      "loss: 2.266293  [41344/60000]\n",
      "loss: 2.267512  [41408/60000]\n",
      "loss: 2.274399  [41472/60000]\n",
      "loss: 2.272565  [41536/60000]\n",
      "loss: 2.259212  [41600/60000]\n",
      "loss: 2.274452  [41664/60000]\n",
      "loss: 2.264804  [41728/60000]\n",
      "loss: 2.270052  [41792/60000]\n",
      "loss: 2.277770  [41856/60000]\n",
      "loss: 2.266363  [41920/60000]\n",
      "loss: 2.275270  [41984/60000]\n",
      "loss: 2.274056  [42048/60000]\n",
      "loss: 2.268292  [42112/60000]\n",
      "loss: 2.255386  [42176/60000]\n",
      "loss: 2.256770  [42240/60000]\n",
      "loss: 2.264646  [42304/60000]\n",
      "loss: 2.264604  [42368/60000]\n",
      "loss: 2.261700  [42432/60000]\n",
      "loss: 2.275073  [42496/60000]\n",
      "loss: 2.273506  [42560/60000]\n",
      "loss: 2.273296  [42624/60000]\n",
      "loss: 2.264718  [42688/60000]\n",
      "loss: 2.267036  [42752/60000]\n",
      "loss: 2.273410  [42816/60000]\n",
      "loss: 2.267769  [42880/60000]\n",
      "loss: 2.271651  [42944/60000]\n",
      "loss: 2.274682  [43008/60000]\n",
      "loss: 2.268232  [43072/60000]\n",
      "loss: 2.268683  [43136/60000]\n",
      "loss: 2.274448  [43200/60000]\n",
      "loss: 2.265469  [43264/60000]\n",
      "loss: 2.270431  [43328/60000]\n",
      "loss: 2.265177  [43392/60000]\n",
      "loss: 2.271948  [43456/60000]\n",
      "loss: 2.265973  [43520/60000]\n",
      "loss: 2.276907  [43584/60000]\n",
      "loss: 2.258457  [43648/60000]\n",
      "loss: 2.260430  [43712/60000]\n",
      "loss: 2.257203  [43776/60000]\n",
      "loss: 2.257810  [43840/60000]\n",
      "loss: 2.258120  [43904/60000]\n",
      "loss: 2.269981  [43968/60000]\n",
      "loss: 2.261619  [44032/60000]\n",
      "loss: 2.269843  [44096/60000]\n",
      "loss: 2.264609  [44160/60000]\n",
      "loss: 2.262217  [44224/60000]\n",
      "loss: 2.263584  [44288/60000]\n",
      "loss: 2.265947  [44352/60000]\n",
      "loss: 2.263371  [44416/60000]\n",
      "loss: 2.279070  [44480/60000]\n",
      "loss: 2.263032  [44544/60000]\n",
      "loss: 2.260747  [44608/60000]\n",
      "loss: 2.263826  [44672/60000]\n",
      "loss: 2.275365  [44736/60000]\n",
      "loss: 2.270618  [44800/60000]\n",
      "loss: 2.252017  [44864/60000]\n",
      "loss: 2.261090  [44928/60000]\n",
      "loss: 2.263708  [44992/60000]\n",
      "loss: 2.252453  [45056/60000]\n",
      "loss: 2.261892  [45120/60000]\n",
      "loss: 2.268739  [45184/60000]\n",
      "loss: 2.271184  [45248/60000]\n",
      "loss: 2.278287  [45312/60000]\n",
      "loss: 2.261615  [45376/60000]\n",
      "loss: 2.262425  [45440/60000]\n",
      "loss: 2.261224  [45504/60000]\n",
      "loss: 2.269662  [45568/60000]\n",
      "loss: 2.270692  [45632/60000]\n",
      "loss: 2.258515  [45696/60000]\n",
      "loss: 2.264467  [45760/60000]\n",
      "loss: 2.262798  [45824/60000]\n",
      "loss: 2.258856  [45888/60000]\n",
      "loss: 2.262084  [45952/60000]\n",
      "loss: 2.270297  [46016/60000]\n",
      "loss: 2.277426  [46080/60000]\n",
      "loss: 2.257869  [46144/60000]\n",
      "loss: 2.272043  [46208/60000]\n",
      "loss: 2.261143  [46272/60000]\n",
      "loss: 2.259609  [46336/60000]\n",
      "loss: 2.269751  [46400/60000]\n",
      "loss: 2.251919  [46464/60000]\n",
      "loss: 2.257397  [46528/60000]\n",
      "loss: 2.256045  [46592/60000]\n",
      "loss: 2.260529  [46656/60000]\n",
      "loss: 2.259410  [46720/60000]\n",
      "loss: 2.249417  [46784/60000]\n",
      "loss: 2.256791  [46848/60000]\n",
      "loss: 2.256406  [46912/60000]\n",
      "loss: 2.270542  [46976/60000]\n",
      "loss: 2.270146  [47040/60000]\n",
      "loss: 2.261409  [47104/60000]\n",
      "loss: 2.248939  [47168/60000]\n",
      "loss: 2.254850  [47232/60000]\n",
      "loss: 2.256786  [47296/60000]\n",
      "loss: 2.262865  [47360/60000]\n",
      "loss: 2.263224  [47424/60000]\n",
      "loss: 2.273106  [47488/60000]\n",
      "loss: 2.254853  [47552/60000]\n",
      "loss: 2.260986  [47616/60000]\n",
      "loss: 2.258911  [47680/60000]\n",
      "loss: 2.255509  [47744/60000]\n",
      "loss: 2.254600  [47808/60000]\n",
      "loss: 2.262906  [47872/60000]\n",
      "loss: 2.258914  [47936/60000]\n",
      "loss: 2.264298  [48000/60000]\n",
      "loss: 2.278260  [48064/60000]\n",
      "loss: 2.254489  [48128/60000]\n",
      "loss: 2.274588  [48192/60000]\n",
      "loss: 2.262652  [48256/60000]\n",
      "loss: 2.263490  [48320/60000]\n",
      "loss: 2.256716  [48384/60000]\n",
      "loss: 2.263359  [48448/60000]\n",
      "loss: 2.256061  [48512/60000]\n",
      "loss: 2.265400  [48576/60000]\n",
      "loss: 2.255454  [48640/60000]\n",
      "loss: 2.257268  [48704/60000]\n",
      "loss: 2.261402  [48768/60000]\n",
      "loss: 2.253994  [48832/60000]\n",
      "loss: 2.269499  [48896/60000]\n",
      "loss: 2.266098  [48960/60000]\n",
      "loss: 2.266679  [49024/60000]\n",
      "loss: 2.265191  [49088/60000]\n",
      "loss: 2.258141  [49152/60000]\n",
      "loss: 2.258632  [49216/60000]\n",
      "loss: 2.255290  [49280/60000]\n",
      "loss: 2.261222  [49344/60000]\n",
      "loss: 2.257487  [49408/60000]\n",
      "loss: 2.260693  [49472/60000]\n",
      "loss: 2.262599  [49536/60000]\n",
      "loss: 2.260726  [49600/60000]\n",
      "loss: 2.252831  [49664/60000]\n",
      "loss: 2.259286  [49728/60000]\n",
      "loss: 2.259945  [49792/60000]\n",
      "loss: 2.259344  [49856/60000]\n",
      "loss: 2.261508  [49920/60000]\n",
      "loss: 2.260457  [49984/60000]\n",
      "loss: 2.260974  [50048/60000]\n",
      "loss: 2.258251  [50112/60000]\n",
      "loss: 2.266866  [50176/60000]\n",
      "loss: 2.261412  [50240/60000]\n",
      "loss: 2.259034  [50304/60000]\n",
      "loss: 2.266028  [50368/60000]\n",
      "loss: 2.267296  [50432/60000]\n",
      "loss: 2.255491  [50496/60000]\n",
      "loss: 2.262524  [50560/60000]\n",
      "loss: 2.256139  [50624/60000]\n",
      "loss: 2.264062  [50688/60000]\n",
      "loss: 2.258662  [50752/60000]\n",
      "loss: 2.247681  [50816/60000]\n",
      "loss: 2.257893  [50880/60000]\n",
      "loss: 2.258436  [50944/60000]\n",
      "loss: 2.250857  [51008/60000]\n",
      "loss: 2.253726  [51072/60000]\n",
      "loss: 2.271847  [51136/60000]\n",
      "loss: 2.253404  [51200/60000]\n",
      "loss: 2.270292  [51264/60000]\n",
      "loss: 2.261096  [51328/60000]\n",
      "loss: 2.258401  [51392/60000]\n",
      "loss: 2.269598  [51456/60000]\n",
      "loss: 2.256571  [51520/60000]\n",
      "loss: 2.264950  [51584/60000]\n",
      "loss: 2.262293  [51648/60000]\n",
      "loss: 2.265445  [51712/60000]\n",
      "loss: 2.249202  [51776/60000]\n",
      "loss: 2.258901  [51840/60000]\n",
      "loss: 2.254649  [51904/60000]\n",
      "loss: 2.263371  [51968/60000]\n",
      "loss: 2.263202  [52032/60000]\n",
      "loss: 2.254365  [52096/60000]\n",
      "loss: 2.242936  [52160/60000]\n",
      "loss: 2.244040  [52224/60000]\n",
      "loss: 2.273083  [52288/60000]\n",
      "loss: 2.264684  [52352/60000]\n",
      "loss: 2.260447  [52416/60000]\n",
      "loss: 2.258205  [52480/60000]\n",
      "loss: 2.249856  [52544/60000]\n",
      "loss: 2.266090  [52608/60000]\n",
      "loss: 2.260990  [52672/60000]\n",
      "loss: 2.245261  [52736/60000]\n",
      "loss: 2.268168  [52800/60000]\n",
      "loss: 2.251602  [52864/60000]\n",
      "loss: 2.268693  [52928/60000]\n",
      "loss: 2.264371  [52992/60000]\n",
      "loss: 2.263106  [53056/60000]\n",
      "loss: 2.258785  [53120/60000]\n",
      "loss: 2.248190  [53184/60000]\n",
      "loss: 2.270486  [53248/60000]\n",
      "loss: 2.241841  [53312/60000]\n",
      "loss: 2.255910  [53376/60000]\n",
      "loss: 2.259881  [53440/60000]\n",
      "loss: 2.256358  [53504/60000]\n",
      "loss: 2.256313  [53568/60000]\n",
      "loss: 2.255356  [53632/60000]\n",
      "loss: 2.260264  [53696/60000]\n",
      "loss: 2.256582  [53760/60000]\n",
      "loss: 2.261098  [53824/60000]\n",
      "loss: 2.260279  [53888/60000]\n",
      "loss: 2.264275  [53952/60000]\n",
      "loss: 2.261485  [54016/60000]\n",
      "loss: 2.265676  [54080/60000]\n",
      "loss: 2.255068  [54144/60000]\n",
      "loss: 2.260623  [54208/60000]\n",
      "loss: 2.257179  [54272/60000]\n",
      "loss: 2.255986  [54336/60000]\n",
      "loss: 2.246790  [54400/60000]\n",
      "loss: 2.256494  [54464/60000]\n",
      "loss: 2.263276  [54528/60000]\n",
      "loss: 2.257806  [54592/60000]\n",
      "loss: 2.259520  [54656/60000]\n",
      "loss: 2.258103  [54720/60000]\n",
      "loss: 2.244669  [54784/60000]\n",
      "loss: 2.253748  [54848/60000]\n",
      "loss: 2.258919  [54912/60000]\n",
      "loss: 2.250775  [54976/60000]\n",
      "loss: 2.241673  [55040/60000]\n",
      "loss: 2.266436  [55104/60000]\n",
      "loss: 2.252314  [55168/60000]\n",
      "loss: 2.252646  [55232/60000]\n",
      "loss: 2.249686  [55296/60000]\n",
      "loss: 2.263359  [55360/60000]\n",
      "loss: 2.256071  [55424/60000]\n",
      "loss: 2.254411  [55488/60000]\n",
      "loss: 2.253768  [55552/60000]\n",
      "loss: 2.269413  [55616/60000]\n",
      "loss: 2.261152  [55680/60000]\n",
      "loss: 2.253654  [55744/60000]\n",
      "loss: 2.249720  [55808/60000]\n",
      "loss: 2.254158  [55872/60000]\n",
      "loss: 2.263062  [55936/60000]\n",
      "loss: 2.244059  [56000/60000]\n",
      "loss: 2.252773  [56064/60000]\n",
      "loss: 2.250499  [56128/60000]\n",
      "loss: 2.251143  [56192/60000]\n",
      "loss: 2.255248  [56256/60000]\n",
      "loss: 2.261124  [56320/60000]\n",
      "loss: 2.260179  [56384/60000]\n",
      "loss: 2.261447  [56448/60000]\n",
      "loss: 2.250871  [56512/60000]\n",
      "loss: 2.258364  [56576/60000]\n",
      "loss: 2.249707  [56640/60000]\n",
      "loss: 2.255035  [56704/60000]\n",
      "loss: 2.249963  [56768/60000]\n",
      "loss: 2.251356  [56832/60000]\n",
      "loss: 2.258223  [56896/60000]\n",
      "loss: 2.249223  [56960/60000]\n",
      "loss: 2.255611  [57024/60000]\n",
      "loss: 2.257813  [57088/60000]\n",
      "loss: 2.273816  [57152/60000]\n",
      "loss: 2.261071  [57216/60000]\n",
      "loss: 2.256101  [57280/60000]\n",
      "loss: 2.256649  [57344/60000]\n",
      "loss: 2.257165  [57408/60000]\n",
      "loss: 2.249399  [57472/60000]\n",
      "loss: 2.254740  [57536/60000]\n",
      "loss: 2.258633  [57600/60000]\n",
      "loss: 2.259949  [57664/60000]\n",
      "loss: 2.263181  [57728/60000]\n",
      "loss: 2.252344  [57792/60000]\n",
      "loss: 2.263500  [57856/60000]\n",
      "loss: 2.260460  [57920/60000]\n",
      "loss: 2.257256  [57984/60000]\n",
      "loss: 2.248289  [58048/60000]\n",
      "loss: 2.259207  [58112/60000]\n",
      "loss: 2.251811  [58176/60000]\n",
      "loss: 2.249103  [58240/60000]\n",
      "loss: 2.250502  [58304/60000]\n",
      "loss: 2.251549  [58368/60000]\n",
      "loss: 2.257249  [58432/60000]\n",
      "loss: 2.257028  [58496/60000]\n",
      "loss: 2.247851  [58560/60000]\n",
      "loss: 2.249073  [58624/60000]\n",
      "loss: 2.248593  [58688/60000]\n",
      "loss: 2.250600  [58752/60000]\n",
      "loss: 2.261389  [58816/60000]\n",
      "loss: 2.254043  [58880/60000]\n",
      "loss: 2.266241  [58944/60000]\n",
      "loss: 2.252427  [59008/60000]\n",
      "loss: 2.263035  [59072/60000]\n",
      "loss: 2.259411  [59136/60000]\n",
      "loss: 2.258914  [59200/60000]\n",
      "loss: 2.249407  [59264/60000]\n",
      "loss: 2.259706  [59328/60000]\n",
      "loss: 2.255976  [59392/60000]\n",
      "loss: 2.252932  [59456/60000]\n",
      "loss: 2.259529  [59520/60000]\n",
      "loss: 2.251360  [59584/60000]\n",
      "loss: 2.259348  [59648/60000]\n",
      "loss: 2.237571  [59712/60000]\n",
      "loss: 2.257490  [59776/60000]\n",
      "loss: 2.255770  [59840/60000]\n",
      "loss: 2.246448  [59904/60000]\n",
      "loss: 2.257144  [29984/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 2.250450 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.270136  [   64/60000]\n",
      "loss: 2.261728  [  128/60000]\n",
      "loss: 2.246860  [  192/60000]\n",
      "loss: 2.258638  [  256/60000]\n",
      "loss: 2.243150  [  320/60000]\n",
      "loss: 2.249051  [  384/60000]\n",
      "loss: 2.253433  [  448/60000]\n",
      "loss: 2.257870  [  512/60000]\n",
      "loss: 2.249641  [  576/60000]\n",
      "loss: 2.250209  [  640/60000]\n",
      "loss: 2.247289  [  704/60000]\n",
      "loss: 2.242100  [  768/60000]\n",
      "loss: 2.251961  [  832/60000]\n",
      "loss: 2.243901  [  896/60000]\n",
      "loss: 2.254181  [  960/60000]\n",
      "loss: 2.252945  [ 1024/60000]\n",
      "loss: 2.251418  [ 1088/60000]\n",
      "loss: 2.252247  [ 1152/60000]\n",
      "loss: 2.250173  [ 1216/60000]\n",
      "loss: 2.264222  [ 1280/60000]\n",
      "loss: 2.246999  [ 1344/60000]\n",
      "loss: 2.248040  [ 1408/60000]\n",
      "loss: 2.245676  [ 1472/60000]\n",
      "loss: 2.242752  [ 1536/60000]\n",
      "loss: 2.246012  [ 1600/60000]\n",
      "loss: 2.240090  [ 1664/60000]\n",
      "loss: 2.251237  [ 1728/60000]\n",
      "loss: 2.255094  [ 1792/60000]\n",
      "loss: 2.258336  [ 1856/60000]\n",
      "loss: 2.233606  [ 1920/60000]\n",
      "loss: 2.255183  [ 1984/60000]\n",
      "loss: 2.247210  [ 2048/60000]\n",
      "loss: 2.246078  [ 2112/60000]\n",
      "loss: 2.244824  [ 2176/60000]\n",
      "loss: 2.246034  [ 2240/60000]\n",
      "loss: 2.253863  [ 2304/60000]\n",
      "loss: 2.241274  [ 2368/60000]\n",
      "loss: 2.236315  [ 2432/60000]\n",
      "loss: 2.253464  [ 2496/60000]\n",
      "loss: 2.257363  [ 2560/60000]\n",
      "loss: 2.246653  [ 2624/60000]\n",
      "loss: 2.248621  [ 2688/60000]\n",
      "loss: 2.238533  [ 2752/60000]\n",
      "loss: 2.250496  [ 2816/60000]\n",
      "loss: 2.243450  [ 2880/60000]\n",
      "loss: 2.245149  [ 2944/60000]\n",
      "loss: 2.253165  [ 3008/60000]\n",
      "loss: 2.247087  [ 3072/60000]\n",
      "loss: 2.253113  [ 3136/60000]\n",
      "loss: 2.256968  [ 3200/60000]\n",
      "loss: 2.230367  [ 3264/60000]\n",
      "loss: 2.256299  [ 3328/60000]\n",
      "loss: 2.246094  [ 3392/60000]\n",
      "loss: 2.247854  [ 3456/60000]\n",
      "loss: 2.253839  [ 3520/60000]\n",
      "loss: 2.242182  [ 3584/60000]\n",
      "loss: 2.263219  [ 3648/60000]\n",
      "loss: 2.244775  [ 3712/60000]\n",
      "loss: 2.253878  [ 3776/60000]\n",
      "loss: 2.254589  [ 3840/60000]\n",
      "loss: 2.245216  [ 3904/60000]\n",
      "loss: 2.240665  [ 3968/60000]\n",
      "loss: 2.250765  [ 4032/60000]\n",
      "loss: 2.250904  [ 4096/60000]\n",
      "loss: 2.260390  [ 4160/60000]\n",
      "loss: 2.251970  [ 4224/60000]\n",
      "loss: 2.260412  [ 4288/60000]\n",
      "loss: 2.230431  [ 4352/60000]\n",
      "loss: 2.241080  [ 4416/60000]\n",
      "loss: 2.246773  [ 4480/60000]\n",
      "loss: 2.243505  [ 4544/60000]\n",
      "loss: 2.236527  [ 4608/60000]\n",
      "loss: 2.243760  [ 4672/60000]\n",
      "loss: 2.245567  [ 4736/60000]\n",
      "loss: 2.240874  [ 4800/60000]\n",
      "loss: 2.237887  [ 4864/60000]\n",
      "loss: 2.245279  [ 4928/60000]\n",
      "loss: 2.241356  [ 4992/60000]\n",
      "loss: 2.232045  [ 5056/60000]\n",
      "loss: 2.248404  [ 5120/60000]\n",
      "loss: 2.258451  [ 5184/60000]\n",
      "loss: 2.249793  [ 5248/60000]\n",
      "loss: 2.256230  [ 5312/60000]\n",
      "loss: 2.260627  [ 5376/60000]\n",
      "loss: 2.250841  [ 5440/60000]\n",
      "loss: 2.235694  [ 5504/60000]\n",
      "loss: 2.252070  [ 5568/60000]\n",
      "loss: 2.252445  [ 5632/60000]\n",
      "loss: 2.251567  [ 5696/60000]\n",
      "loss: 2.253262  [ 5760/60000]\n",
      "loss: 2.258052  [ 5824/60000]\n",
      "loss: 2.240249  [ 5888/60000]\n",
      "loss: 2.244993  [ 5952/60000]\n",
      "loss: 2.248952  [ 6016/60000]\n",
      "loss: 2.253973  [ 6080/60000]\n",
      "loss: 2.252686  [ 6144/60000]\n",
      "loss: 2.249636  [ 6208/60000]\n",
      "loss: 2.262576  [ 6272/60000]\n",
      "loss: 2.243465  [ 6336/60000]\n",
      "loss: 2.238102  [ 6400/60000]\n",
      "loss: 2.250721  [ 6464/60000]\n",
      "loss: 2.240923  [ 6528/60000]\n",
      "loss: 2.245719  [ 6592/60000]\n",
      "loss: 2.251620  [ 6656/60000]\n",
      "loss: 2.238973  [ 6720/60000]\n",
      "loss: 2.255421  [ 6784/60000]\n",
      "loss: 2.252749  [ 6848/60000]\n",
      "loss: 2.247298  [ 6912/60000]\n",
      "loss: 2.246973  [ 6976/60000]\n",
      "loss: 2.246653  [ 7040/60000]\n",
      "loss: 2.237221  [ 7104/60000]\n",
      "loss: 2.247566  [ 7168/60000]\n",
      "loss: 2.246052  [ 7232/60000]\n",
      "loss: 2.243459  [ 7296/60000]\n",
      "loss: 2.241629  [ 7360/60000]\n",
      "loss: 2.237610  [ 7424/60000]\n",
      "loss: 2.245595  [ 7488/60000]\n",
      "loss: 2.243608  [ 7552/60000]\n",
      "loss: 2.237411  [ 7616/60000]\n",
      "loss: 2.232487  [ 7680/60000]\n",
      "loss: 2.253670  [ 7744/60000]\n",
      "loss: 2.237687  [ 7808/60000]\n",
      "loss: 2.251622  [ 7872/60000]\n",
      "loss: 2.246397  [ 7936/60000]\n",
      "loss: 2.242152  [ 8000/60000]\n",
      "loss: 2.240158  [ 8064/60000]\n",
      "loss: 2.248789  [ 8128/60000]\n",
      "loss: 2.241672  [ 8192/60000]\n",
      "loss: 2.250263  [ 8256/60000]\n",
      "loss: 2.250782  [ 8320/60000]\n",
      "loss: 2.235297  [ 8384/60000]\n",
      "loss: 2.240356  [ 8448/60000]\n",
      "loss: 2.243763  [ 8512/60000]\n",
      "loss: 2.242797  [ 8576/60000]\n",
      "loss: 2.233130  [ 8640/60000]\n",
      "loss: 2.247615  [ 8704/60000]\n",
      "loss: 2.245121  [ 8768/60000]\n",
      "loss: 2.260023  [ 8832/60000]\n",
      "loss: 2.242365  [ 8896/60000]\n",
      "loss: 2.249920  [ 8960/60000]\n",
      "loss: 2.238171  [ 9024/60000]\n",
      "loss: 2.245765  [ 9088/60000]\n",
      "loss: 2.243299  [ 9152/60000]\n",
      "loss: 2.255412  [ 9216/60000]\n",
      "loss: 2.247383  [ 9280/60000]\n",
      "loss: 2.249538  [ 9344/60000]\n",
      "loss: 2.245133  [ 9408/60000]\n",
      "loss: 2.250818  [ 9472/60000]\n",
      "loss: 2.236016  [ 9536/60000]\n",
      "loss: 2.231893  [ 9600/60000]\n",
      "loss: 2.242941  [ 9664/60000]\n",
      "loss: 2.235682  [ 9728/60000]\n",
      "loss: 2.253802  [ 9792/60000]\n",
      "loss: 2.248113  [ 9856/60000]\n",
      "loss: 2.231488  [ 9920/60000]\n",
      "loss: 2.245362  [ 9984/60000]\n",
      "loss: 2.253671  [10048/60000]\n",
      "loss: 2.259087  [10112/60000]\n",
      "loss: 2.249669  [10176/60000]\n",
      "loss: 2.247715  [10240/60000]\n",
      "loss: 2.234762  [10304/60000]\n",
      "loss: 2.240474  [10368/60000]\n",
      "loss: 2.246984  [10432/60000]\n",
      "loss: 2.256601  [10496/60000]\n",
      "loss: 2.234261  [10560/60000]\n",
      "loss: 2.247397  [10624/60000]\n",
      "loss: 2.234534  [10688/60000]\n",
      "loss: 2.248292  [10752/60000]\n",
      "loss: 2.242663  [10816/60000]\n",
      "loss: 2.261277  [10880/60000]\n",
      "loss: 2.239656  [10944/60000]\n",
      "loss: 2.249530  [11008/60000]\n",
      "loss: 2.257743  [11072/60000]\n",
      "loss: 2.238633  [11136/60000]\n",
      "loss: 2.237801  [11200/60000]\n",
      "loss: 2.248580  [11264/60000]\n",
      "loss: 2.240187  [11328/60000]\n",
      "loss: 2.255373  [11392/60000]\n",
      "loss: 2.237483  [11456/60000]\n",
      "loss: 2.246871  [11520/60000]\n",
      "loss: 2.238691  [11584/60000]\n",
      "loss: 2.241650  [11648/60000]\n",
      "loss: 2.234715  [11712/60000]\n",
      "loss: 2.245641  [11776/60000]\n",
      "loss: 2.240660  [11840/60000]\n",
      "loss: 2.233026  [11904/60000]\n",
      "loss: 2.241822  [11968/60000]\n",
      "loss: 2.238635  [12032/60000]\n",
      "loss: 2.222378  [12096/60000]\n",
      "loss: 2.238804  [12160/60000]\n",
      "loss: 2.228873  [12224/60000]\n",
      "loss: 2.219659  [12288/60000]\n",
      "loss: 2.229767  [12352/60000]\n",
      "loss: 2.232862  [12416/60000]\n",
      "loss: 2.259755  [12480/60000]\n",
      "loss: 2.231598  [12544/60000]\n",
      "loss: 2.236363  [12608/60000]\n",
      "loss: 2.238418  [12672/60000]\n",
      "loss: 2.236593  [12736/60000]\n",
      "loss: 2.236246  [12800/60000]\n",
      "loss: 2.234458  [12864/60000]\n",
      "loss: 2.239252  [12928/60000]\n",
      "loss: 2.233060  [12992/60000]\n",
      "loss: 2.226463  [13056/60000]\n",
      "loss: 2.249776  [13120/60000]\n",
      "loss: 2.215656  [13184/60000]\n",
      "loss: 2.229674  [13248/60000]\n",
      "loss: 2.247426  [13312/60000]\n",
      "loss: 2.239211  [13376/60000]\n",
      "loss: 2.239318  [13440/60000]\n",
      "loss: 2.241117  [13504/60000]\n",
      "loss: 2.248573  [13568/60000]\n",
      "loss: 2.240534  [13632/60000]\n",
      "loss: 2.239730  [13696/60000]\n",
      "loss: 2.241357  [13760/60000]\n",
      "loss: 2.238801  [13824/60000]\n",
      "loss: 2.247307  [13888/60000]\n",
      "loss: 2.227389  [13952/60000]\n",
      "loss: 2.235533  [14016/60000]\n",
      "loss: 2.243846  [14080/60000]\n",
      "loss: 2.264606  [14144/60000]\n",
      "loss: 2.247686  [14208/60000]\n",
      "loss: 2.248619  [14272/60000]\n",
      "loss: 2.241328  [14336/60000]\n",
      "loss: 2.233402  [14400/60000]\n",
      "loss: 2.244727  [14464/60000]\n",
      "loss: 2.233133  [14528/60000]\n",
      "loss: 2.236058  [14592/60000]\n",
      "loss: 2.237044  [14656/60000]\n",
      "loss: 2.240548  [14720/60000]\n",
      "loss: 2.248207  [14784/60000]\n",
      "loss: 2.229293  [14848/60000]\n",
      "loss: 2.245013  [14912/60000]\n",
      "loss: 2.241613  [14976/60000]\n",
      "loss: 2.231957  [15040/60000]\n",
      "loss: 2.238771  [15104/60000]\n",
      "loss: 2.226966  [15168/60000]\n",
      "loss: 2.237922  [15232/60000]\n",
      "loss: 2.242212  [15296/60000]\n",
      "loss: 2.233722  [15360/60000]\n",
      "loss: 2.233138  [15424/60000]\n",
      "loss: 2.234003  [15488/60000]\n",
      "loss: 2.234295  [15552/60000]\n",
      "loss: 2.248054  [15616/60000]\n",
      "loss: 2.231197  [15680/60000]\n",
      "loss: 2.228101  [15744/60000]\n",
      "loss: 2.236127  [15808/60000]\n",
      "loss: 2.238851  [15872/60000]\n",
      "loss: 2.226566  [15936/60000]\n",
      "loss: 2.243772  [16000/60000]\n",
      "loss: 2.227453  [16064/60000]\n",
      "loss: 2.241148  [16128/60000]\n",
      "loss: 2.237648  [16192/60000]\n",
      "loss: 2.236302  [16256/60000]\n",
      "loss: 2.236345  [16320/60000]\n",
      "loss: 2.237804  [16384/60000]\n",
      "loss: 2.235881  [16448/60000]\n",
      "loss: 2.243577  [16512/60000]\n",
      "loss: 2.231170  [16576/60000]\n",
      "loss: 2.230882  [16640/60000]\n",
      "loss: 2.232501  [16704/60000]\n",
      "loss: 2.237587  [16768/60000]\n",
      "loss: 2.248511  [16832/60000]\n",
      "loss: 2.225615  [16896/60000]\n",
      "loss: 2.247349  [16960/60000]\n",
      "loss: 2.242476  [17024/60000]\n",
      "loss: 2.229568  [17088/60000]\n",
      "loss: 2.230016  [17152/60000]\n",
      "loss: 2.237474  [17216/60000]\n",
      "loss: 2.247719  [17280/60000]\n",
      "loss: 2.233627  [17344/60000]\n",
      "loss: 2.219917  [17408/60000]\n",
      "loss: 2.228332  [17472/60000]\n",
      "loss: 2.238734  [17536/60000]\n",
      "loss: 2.242030  [17600/60000]\n",
      "loss: 2.226938  [17664/60000]\n",
      "loss: 2.240755  [17728/60000]\n",
      "loss: 2.238632  [17792/60000]\n",
      "loss: 2.233374  [17856/60000]\n",
      "loss: 2.237074  [17920/60000]\n",
      "loss: 2.239527  [17984/60000]\n",
      "loss: 2.261280  [18048/60000]\n",
      "loss: 2.240570  [18112/60000]\n",
      "loss: 2.240125  [18176/60000]\n",
      "loss: 2.244473  [18240/60000]\n",
      "loss: 2.223925  [18304/60000]\n",
      "loss: 2.235837  [18368/60000]\n",
      "loss: 2.222587  [18432/60000]\n",
      "loss: 2.237284  [18496/60000]\n",
      "loss: 2.218984  [18560/60000]\n",
      "loss: 2.243130  [18624/60000]\n",
      "loss: 2.240394  [18688/60000]\n",
      "loss: 2.218359  [18752/60000]\n",
      "loss: 2.232098  [18816/60000]\n",
      "loss: 2.242474  [18880/60000]\n",
      "loss: 2.224227  [18944/60000]\n",
      "loss: 2.231426  [19008/60000]\n",
      "loss: 2.244591  [19072/60000]\n",
      "loss: 2.249372  [19136/60000]\n",
      "loss: 2.240799  [19200/60000]\n",
      "loss: 2.232628  [19264/60000]\n",
      "loss: 2.227567  [19328/60000]\n",
      "loss: 2.230215  [19392/60000]\n",
      "loss: 2.244362  [19456/60000]\n",
      "loss: 2.231021  [19520/60000]\n",
      "loss: 2.233436  [19584/60000]\n",
      "loss: 2.238153  [19648/60000]\n",
      "loss: 2.237418  [19712/60000]\n",
      "loss: 2.232787  [19776/60000]\n",
      "loss: 2.231028  [19840/60000]\n",
      "loss: 2.241482  [19904/60000]\n",
      "loss: 2.227425  [19968/60000]\n",
      "loss: 2.234060  [20032/60000]\n",
      "loss: 2.209378  [20096/60000]\n",
      "loss: 2.230579  [20160/60000]\n",
      "loss: 2.235417  [20224/60000]\n",
      "loss: 2.218570  [20288/60000]\n",
      "loss: 2.246228  [20352/60000]\n",
      "loss: 2.216253  [20416/60000]\n",
      "loss: 2.211314  [20480/60000]\n",
      "loss: 2.223360  [20544/60000]\n",
      "loss: 2.227434  [20608/60000]\n",
      "loss: 2.227837  [20672/60000]\n",
      "loss: 2.238517  [20736/60000]\n",
      "loss: 2.226278  [20800/60000]\n",
      "loss: 2.228450  [20864/60000]\n",
      "loss: 2.232708  [20928/60000]\n",
      "loss: 2.232729  [20992/60000]\n",
      "loss: 2.231082  [21056/60000]\n",
      "loss: 2.233173  [21120/60000]\n",
      "loss: 2.226433  [21184/60000]\n",
      "loss: 2.232128  [21248/60000]\n",
      "loss: 2.235194  [21312/60000]\n",
      "loss: 2.227510  [21376/60000]\n",
      "loss: 2.245203  [21440/60000]\n",
      "loss: 2.237936  [21504/60000]\n",
      "loss: 2.223883  [21568/60000]\n",
      "loss: 2.242008  [21632/60000]\n",
      "loss: 2.225530  [21696/60000]\n",
      "loss: 2.219858  [21760/60000]\n",
      "loss: 2.216626  [21824/60000]\n",
      "loss: 2.213693  [21888/60000]\n",
      "loss: 2.223752  [21952/60000]\n",
      "loss: 2.235759  [22016/60000]\n",
      "loss: 2.231518  [22080/60000]\n",
      "loss: 2.236806  [22144/60000]\n",
      "loss: 2.221479  [22208/60000]\n",
      "loss: 2.226923  [22272/60000]\n",
      "loss: 2.224976  [22336/60000]\n",
      "loss: 2.234749  [22400/60000]\n",
      "loss: 2.230582  [22464/60000]\n",
      "loss: 2.229899  [22528/60000]\n",
      "loss: 2.213094  [22592/60000]\n",
      "loss: 2.215955  [22656/60000]\n",
      "loss: 2.225156  [22720/60000]\n",
      "loss: 2.221809  [22784/60000]\n",
      "loss: 2.226668  [22848/60000]\n",
      "loss: 2.227090  [22912/60000]\n",
      "loss: 2.236543  [22976/60000]\n",
      "loss: 2.210242  [23040/60000]\n",
      "loss: 2.226911  [23104/60000]\n",
      "loss: 2.226620  [23168/60000]\n",
      "loss: 2.243951  [23232/60000]\n",
      "loss: 2.228445  [23296/60000]\n",
      "loss: 2.226329  [23360/60000]\n",
      "loss: 2.238589  [23424/60000]\n",
      "loss: 2.222178  [23488/60000]\n",
      "loss: 2.224709  [23552/60000]\n",
      "loss: 2.228118  [23616/60000]\n",
      "loss: 2.230588  [23680/60000]\n",
      "loss: 2.231690  [23744/60000]\n",
      "loss: 2.228846  [23808/60000]\n",
      "loss: 2.223139  [23872/60000]\n",
      "loss: 2.238896  [23936/60000]\n",
      "loss: 2.220656  [24000/60000]\n",
      "loss: 2.230635  [24064/60000]\n",
      "loss: 2.216578  [24128/60000]\n",
      "loss: 2.225270  [24192/60000]\n",
      "loss: 2.217105  [24256/60000]\n",
      "loss: 2.232577  [24320/60000]\n",
      "loss: 2.234207  [24384/60000]\n",
      "loss: 2.229132  [24448/60000]\n",
      "loss: 2.239932  [24512/60000]\n",
      "loss: 2.240759  [24576/60000]\n",
      "loss: 2.239589  [24640/60000]\n",
      "loss: 2.231853  [24704/60000]\n",
      "loss: 2.230122  [24768/60000]\n",
      "loss: 2.238947  [24832/60000]\n",
      "loss: 2.223920  [24896/60000]\n",
      "loss: 2.225000  [24960/60000]\n",
      "loss: 2.235812  [25024/60000]\n",
      "loss: 2.226077  [25088/60000]\n",
      "loss: 2.241036  [25152/60000]\n",
      "loss: 2.231021  [25216/60000]\n",
      "loss: 2.228808  [25280/60000]\n",
      "loss: 2.240500  [25344/60000]\n",
      "loss: 2.220132  [25408/60000]\n",
      "loss: 2.244444  [25472/60000]\n",
      "loss: 2.231431  [25536/60000]\n",
      "loss: 2.214908  [25600/60000]\n",
      "loss: 2.211304  [25664/60000]\n",
      "loss: 2.227518  [25728/60000]\n",
      "loss: 2.215950  [25792/60000]\n",
      "loss: 2.219667  [25856/60000]\n",
      "loss: 2.223787  [25920/60000]\n",
      "loss: 2.240721  [25984/60000]\n",
      "loss: 2.224360  [26048/60000]\n",
      "loss: 2.233169  [26112/60000]\n",
      "loss: 2.238707  [26176/60000]\n",
      "loss: 2.242393  [26240/60000]\n",
      "loss: 2.234977  [26304/60000]\n",
      "loss: 2.227183  [26368/60000]\n",
      "loss: 2.239855  [26432/60000]\n",
      "loss: 2.226506  [26496/60000]\n",
      "loss: 2.229522  [26560/60000]\n",
      "loss: 2.234322  [26624/60000]\n",
      "loss: 2.216035  [26688/60000]\n",
      "loss: 2.226935  [26752/60000]\n",
      "loss: 2.224526  [26816/60000]\n",
      "loss: 2.219474  [26880/60000]\n",
      "loss: 2.227274  [26944/60000]\n",
      "loss: 2.229467  [27008/60000]\n",
      "loss: 2.227910  [27072/60000]\n",
      "loss: 2.230505  [27136/60000]\n",
      "loss: 2.236289  [27200/60000]\n",
      "loss: 2.235803  [27264/60000]\n",
      "loss: 2.231529  [27328/60000]\n",
      "loss: 2.222373  [27392/60000]\n",
      "loss: 2.231055  [27456/60000]\n",
      "loss: 2.235722  [27520/60000]\n",
      "loss: 2.215830  [27584/60000]\n",
      "loss: 2.217147  [27648/60000]\n",
      "loss: 2.214014  [27712/60000]\n",
      "loss: 2.240771  [27776/60000]\n",
      "loss: 2.225945  [27840/60000]\n",
      "loss: 2.224387  [27904/60000]\n",
      "loss: 2.238278  [27968/60000]\n",
      "loss: 2.238007  [28032/60000]\n",
      "loss: 2.226200  [28096/60000]\n",
      "loss: 2.219116  [28160/60000]\n",
      "loss: 2.223679  [28224/60000]\n",
      "loss: 2.227365  [28288/60000]\n",
      "loss: 2.229694  [28352/60000]\n",
      "loss: 2.218564  [28416/60000]\n",
      "loss: 2.220896  [28480/60000]\n",
      "loss: 2.203164  [28544/60000]\n",
      "loss: 2.229175  [28608/60000]\n",
      "loss: 2.230862  [28672/60000]\n",
      "loss: 2.219808  [28736/60000]\n",
      "loss: 2.209079  [28800/60000]\n",
      "loss: 2.204972  [28864/60000]\n",
      "loss: 2.221342  [28928/60000]\n",
      "loss: 2.209303  [28992/60000]\n",
      "loss: 2.219074  [29056/60000]\n",
      "loss: 2.228376  [29120/60000]\n",
      "loss: 2.212057  [29184/60000]\n",
      "loss: 2.230220  [29248/60000]\n",
      "loss: 2.222332  [29312/60000]\n",
      "loss: 2.214506  [29376/60000]\n",
      "loss: 2.219772  [29440/60000]\n",
      "loss: 2.223485  [29504/60000]\n",
      "loss: 2.229331  [29568/60000]\n",
      "loss: 2.218342  [29632/60000]\n",
      "loss: 2.229375  [29696/60000]\n",
      "loss: 2.203159  [29760/60000]\n",
      "loss: 2.215680  [29824/60000]\n",
      "loss: 2.225939  [29888/60000]\n",
      "loss: 2.214680  [29952/60000]\n",
      "loss: 2.211138  [30016/60000]\n",
      "loss: 2.224049  [30080/60000]\n",
      "loss: 2.215760  [30144/60000]\n",
      "loss: 2.236598  [30208/60000]\n",
      "loss: 2.218370  [30272/60000]\n",
      "loss: 2.236951  [30336/60000]\n",
      "loss: 2.228530  [30400/60000]\n",
      "loss: 2.215059  [30464/60000]\n",
      "loss: 2.241697  [30528/60000]\n",
      "loss: 2.224150  [30592/60000]\n",
      "loss: 2.234369  [30656/60000]\n",
      "loss: 2.202905  [30720/60000]\n",
      "loss: 2.218011  [30784/60000]\n",
      "loss: 2.229733  [30848/60000]\n",
      "loss: 2.214961  [30912/60000]\n",
      "loss: 2.209964  [30976/60000]\n",
      "loss: 2.211065  [31040/60000]\n",
      "loss: 2.231034  [31104/60000]\n",
      "loss: 2.222417  [31168/60000]\n",
      "loss: 2.228125  [31232/60000]\n",
      "loss: 2.234314  [31296/60000]\n",
      "loss: 2.206259  [31360/60000]\n",
      "loss: 2.225888  [31424/60000]\n",
      "loss: 2.223630  [31488/60000]\n",
      "loss: 2.224045  [31552/60000]\n",
      "loss: 2.202184  [31616/60000]\n",
      "loss: 2.225956  [31680/60000]\n",
      "loss: 2.210213  [31744/60000]\n",
      "loss: 2.204782  [31808/60000]\n",
      "loss: 2.219875  [31872/60000]\n",
      "loss: 2.224707  [31936/60000]\n",
      "loss: 2.218553  [32000/60000]\n",
      "loss: 2.217434  [32064/60000]\n",
      "loss: 2.216019  [32128/60000]\n",
      "loss: 2.221383  [32192/60000]\n",
      "loss: 2.214879  [32256/60000]\n",
      "loss: 2.222526  [32320/60000]\n",
      "loss: 2.216354  [32384/60000]\n",
      "loss: 2.207206  [32448/60000]\n",
      "loss: 2.220881  [32512/60000]\n",
      "loss: 2.199670  [32576/60000]\n",
      "loss: 2.192059  [32640/60000]\n",
      "loss: 2.221336  [32704/60000]\n",
      "loss: 2.224337  [32768/60000]\n",
      "loss: 2.209972  [32832/60000]\n",
      "loss: 2.233411  [32896/60000]\n",
      "loss: 2.211594  [32960/60000]\n",
      "loss: 2.227885  [33024/60000]\n",
      "loss: 2.212245  [33088/60000]\n",
      "loss: 2.228632  [33152/60000]\n",
      "loss: 2.212941  [33216/60000]\n",
      "loss: 2.242337  [33280/60000]\n",
      "loss: 2.207661  [33344/60000]\n",
      "loss: 2.228716  [33408/60000]\n",
      "loss: 2.212378  [33472/60000]\n",
      "loss: 2.206265  [33536/60000]\n",
      "loss: 2.210407  [33600/60000]\n",
      "loss: 2.224824  [33664/60000]\n",
      "loss: 2.226359  [33728/60000]\n",
      "loss: 2.209207  [33792/60000]\n",
      "loss: 2.201133  [33856/60000]\n",
      "loss: 2.221372  [33920/60000]\n",
      "loss: 2.216008  [33984/60000]\n",
      "loss: 2.203665  [34048/60000]\n",
      "loss: 2.198386  [34112/60000]\n",
      "loss: 2.207161  [34176/60000]\n",
      "loss: 2.208933  [34240/60000]\n",
      "loss: 2.211857  [34304/60000]\n",
      "loss: 2.225981  [34368/60000]\n",
      "loss: 2.228891  [34432/60000]\n",
      "loss: 2.216366  [34496/60000]\n",
      "loss: 2.230241  [34560/60000]\n",
      "loss: 2.207946  [34624/60000]\n",
      "loss: 2.229483  [34688/60000]\n",
      "loss: 2.211899  [34752/60000]\n",
      "loss: 2.224224  [34816/60000]\n",
      "loss: 2.221009  [34880/60000]\n",
      "loss: 2.216368  [34944/60000]\n",
      "loss: 2.216738  [35008/60000]\n",
      "loss: 2.228537  [35072/60000]\n",
      "loss: 2.216987  [35136/60000]\n",
      "loss: 2.217656  [35200/60000]\n",
      "loss: 2.232571  [35264/60000]\n",
      "loss: 2.228585  [35328/60000]\n",
      "loss: 2.210489  [35392/60000]\n",
      "loss: 2.214096  [35456/60000]\n",
      "loss: 2.213507  [35520/60000]\n",
      "loss: 2.197760  [35584/60000]\n",
      "loss: 2.216721  [35648/60000]\n",
      "loss: 2.236970  [35712/60000]\n",
      "loss: 2.211623  [35776/60000]\n",
      "loss: 2.195569  [35840/60000]\n",
      "loss: 2.217218  [35904/60000]\n",
      "loss: 2.207799  [35968/60000]\n",
      "loss: 2.196101  [36032/60000]\n",
      "loss: 2.226998  [36096/60000]\n",
      "loss: 2.205999  [36160/60000]\n",
      "loss: 2.223238  [36224/60000]\n",
      "loss: 2.214626  [36288/60000]\n",
      "loss: 2.216792  [36352/60000]\n",
      "loss: 2.200871  [36416/60000]\n",
      "loss: 2.223129  [36480/60000]\n",
      "loss: 2.214938  [36544/60000]\n",
      "loss: 2.187640  [36608/60000]\n",
      "loss: 2.211311  [36672/60000]\n",
      "loss: 2.226702  [36736/60000]\n",
      "loss: 2.226821  [36800/60000]\n",
      "loss: 2.214061  [36864/60000]\n",
      "loss: 2.220838  [36928/60000]\n",
      "loss: 2.214551  [36992/60000]\n",
      "loss: 2.208508  [37056/60000]\n",
      "loss: 2.221958  [37120/60000]\n",
      "loss: 2.206320  [37184/60000]\n",
      "loss: 2.224734  [37248/60000]\n",
      "loss: 2.191183  [37312/60000]\n",
      "loss: 2.213710  [37376/60000]\n",
      "loss: 2.213544  [37440/60000]\n",
      "loss: 2.210534  [37504/60000]\n",
      "loss: 2.214945  [37568/60000]\n",
      "loss: 2.193658  [37632/60000]\n",
      "loss: 2.219235  [37696/60000]\n",
      "loss: 2.216308  [37760/60000]\n",
      "loss: 2.216537  [37824/60000]\n",
      "loss: 2.210430  [37888/60000]\n",
      "loss: 2.192913  [37952/60000]\n",
      "loss: 2.217360  [38016/60000]\n",
      "loss: 2.234612  [38080/60000]\n",
      "loss: 2.207750  [38144/60000]\n",
      "loss: 2.209772  [38208/60000]\n",
      "loss: 2.204146  [38272/60000]\n",
      "loss: 2.201039  [38336/60000]\n",
      "loss: 2.209013  [38400/60000]\n",
      "loss: 2.216621  [38464/60000]\n",
      "loss: 2.203576  [38528/60000]\n",
      "loss: 2.201356  [38592/60000]\n",
      "loss: 2.215599  [38656/60000]\n",
      "loss: 2.205026  [38720/60000]\n",
      "loss: 2.239059  [38784/60000]\n",
      "loss: 2.220009  [38848/60000]\n",
      "loss: 2.214867  [38912/60000]\n",
      "loss: 2.213513  [38976/60000]\n",
      "loss: 2.206357  [39040/60000]\n",
      "loss: 2.203150  [39104/60000]\n",
      "loss: 2.196748  [39168/60000]\n",
      "loss: 2.217283  [39232/60000]\n",
      "loss: 2.219903  [39296/60000]\n",
      "loss: 2.224748  [39360/60000]\n",
      "loss: 2.201765  [39424/60000]\n",
      "loss: 2.199338  [39488/60000]\n",
      "loss: 2.209305  [39552/60000]\n",
      "loss: 2.205336  [39616/60000]\n",
      "loss: 2.200615  [39680/60000]\n",
      "loss: 2.219611  [39744/60000]\n",
      "loss: 2.229712  [39808/60000]\n",
      "loss: 2.199281  [39872/60000]\n",
      "loss: 2.203023  [39936/60000]\n",
      "loss: 2.209584  [40000/60000]\n",
      "loss: 2.215145  [40064/60000]\n",
      "loss: 2.207716  [40128/60000]\n",
      "loss: 2.209515  [40192/60000]\n",
      "loss: 2.204847  [40256/60000]\n",
      "loss: 2.196481  [40320/60000]\n",
      "loss: 2.211554  [40384/60000]\n",
      "loss: 2.199936  [40448/60000]\n",
      "loss: 2.207249  [40512/60000]\n",
      "loss: 2.202760  [40576/60000]\n",
      "loss: 2.217406  [40640/60000]\n",
      "loss: 2.227940  [40704/60000]\n",
      "loss: 2.216438  [40768/60000]\n",
      "loss: 2.212117  [40832/60000]\n",
      "loss: 2.205270  [40896/60000]\n",
      "loss: 2.215745  [40960/60000]\n",
      "loss: 2.204626  [41024/60000]\n",
      "loss: 2.225707  [41088/60000]\n",
      "loss: 2.221015  [41152/60000]\n",
      "loss: 2.196422  [41216/60000]\n",
      "loss: 2.198169  [41280/60000]\n",
      "loss: 2.209345  [41344/60000]\n",
      "loss: 2.212328  [41408/60000]\n",
      "loss: 2.205411  [41472/60000]\n",
      "loss: 2.203228  [41536/60000]\n",
      "loss: 2.208212  [41600/60000]\n",
      "loss: 2.227888  [41664/60000]\n",
      "loss: 2.203590  [41728/60000]\n",
      "loss: 2.218383  [41792/60000]\n",
      "loss: 2.213786  [41856/60000]\n",
      "loss: 2.206523  [41920/60000]\n",
      "loss: 2.201961  [41984/60000]\n",
      "loss: 2.187024  [42048/60000]\n",
      "loss: 2.205461  [42112/60000]\n",
      "loss: 2.206683  [42176/60000]\n",
      "loss: 2.212212  [42240/60000]\n",
      "loss: 2.207032  [42304/60000]\n",
      "loss: 2.211751  [42368/60000]\n",
      "loss: 2.202410  [42432/60000]\n",
      "loss: 2.205576  [42496/60000]\n",
      "loss: 2.225875  [42560/60000]\n",
      "loss: 2.228231  [42624/60000]\n",
      "loss: 2.207422  [42688/60000]\n",
      "loss: 2.198110  [42752/60000]\n",
      "loss: 2.211766  [42816/60000]\n",
      "loss: 2.197957  [42880/60000]\n",
      "loss: 2.225858  [42944/60000]\n",
      "loss: 2.217289  [43008/60000]\n",
      "loss: 2.201608  [43072/60000]\n",
      "loss: 2.191467  [43136/60000]\n",
      "loss: 2.226219  [43200/60000]\n",
      "loss: 2.189987  [43264/60000]\n",
      "loss: 2.196589  [43328/60000]\n",
      "loss: 2.184998  [43392/60000]\n",
      "loss: 2.205754  [43456/60000]\n",
      "loss: 2.203949  [43520/60000]\n",
      "loss: 2.188449  [43584/60000]\n",
      "loss: 2.206498  [43648/60000]\n",
      "loss: 2.179559  [43712/60000]\n",
      "loss: 2.207335  [43776/60000]\n",
      "loss: 2.207045  [43840/60000]\n",
      "loss: 2.210324  [43904/60000]\n",
      "loss: 2.209667  [43968/60000]\n",
      "loss: 2.195378  [44032/60000]\n",
      "loss: 2.209784  [44096/60000]\n",
      "loss: 2.186337  [44160/60000]\n",
      "loss: 2.194388  [44224/60000]\n",
      "loss: 2.203205  [44288/60000]\n",
      "loss: 2.189966  [44352/60000]\n",
      "loss: 2.206178  [44416/60000]\n",
      "loss: 2.201638  [44480/60000]\n",
      "loss: 2.208498  [44544/60000]\n",
      "loss: 2.215834  [44608/60000]\n",
      "loss: 2.188403  [44672/60000]\n",
      "loss: 2.198324  [44736/60000]\n",
      "loss: 2.209675  [44800/60000]\n",
      "loss: 2.208174  [44864/60000]\n",
      "loss: 2.214105  [44928/60000]\n",
      "loss: 2.204657  [44992/60000]\n",
      "loss: 2.191474  [45056/60000]\n",
      "loss: 2.178733  [45120/60000]\n",
      "loss: 2.204046  [45184/60000]\n",
      "loss: 2.184130  [45248/60000]\n",
      "loss: 2.200644  [45312/60000]\n",
      "loss: 2.195761  [45376/60000]\n",
      "loss: 2.201595  [45440/60000]\n",
      "loss: 2.183880  [45504/60000]\n",
      "loss: 2.203311  [45568/60000]\n",
      "loss: 2.221061  [45632/60000]\n",
      "loss: 2.187098  [45696/60000]\n",
      "loss: 2.204058  [45760/60000]\n",
      "loss: 2.196774  [45824/60000]\n",
      "loss: 2.188033  [45888/60000]\n",
      "loss: 2.204979  [45952/60000]\n",
      "loss: 2.182538  [46016/60000]\n",
      "loss: 2.199047  [46080/60000]\n",
      "loss: 2.179466  [46144/60000]\n",
      "loss: 2.198789  [46208/60000]\n",
      "loss: 2.198658  [46272/60000]\n",
      "loss: 2.202952  [46336/60000]\n",
      "loss: 2.194223  [46400/60000]\n",
      "loss: 2.194419  [46464/60000]\n",
      "loss: 2.213283  [46528/60000]\n",
      "loss: 2.204324  [46592/60000]\n",
      "loss: 2.179965  [46656/60000]\n",
      "loss: 2.186555  [46720/60000]\n",
      "loss: 2.199704  [46784/60000]\n",
      "loss: 2.210318  [46848/60000]\n",
      "loss: 2.198521  [46912/60000]\n",
      "loss: 2.211777  [46976/60000]\n",
      "loss: 2.194497  [47040/60000]\n",
      "loss: 2.199658  [47104/60000]\n",
      "loss: 2.183697  [47168/60000]\n",
      "loss: 2.199124  [47232/60000]\n",
      "loss: 2.172363  [47296/60000]\n",
      "loss: 2.184729  [47360/60000]\n",
      "loss: 2.193276  [47424/60000]\n",
      "loss: 2.202356  [47488/60000]\n",
      "loss: 2.181352  [47552/60000]\n",
      "loss: 2.193543  [47616/60000]\n",
      "loss: 2.197429  [47680/60000]\n",
      "loss: 2.206664  [47744/60000]\n",
      "loss: 2.202287  [47808/60000]\n",
      "loss: 2.192422  [47872/60000]\n",
      "loss: 2.201102  [47936/60000]\n",
      "loss: 2.217664  [48000/60000]\n",
      "loss: 2.187089  [48064/60000]\n",
      "loss: 2.215003  [48128/60000]\n",
      "loss: 2.207556  [48192/60000]\n",
      "loss: 2.198712  [48256/60000]\n",
      "loss: 2.182824  [48320/60000]\n",
      "loss: 2.212195  [48384/60000]\n",
      "loss: 2.174761  [48448/60000]\n",
      "loss: 2.217707  [48512/60000]\n",
      "loss: 2.220992  [48576/60000]\n",
      "loss: 2.193298  [48640/60000]\n",
      "loss: 2.193628  [48704/60000]\n",
      "loss: 2.200357  [48768/60000]\n",
      "loss: 2.201399  [48832/60000]\n",
      "loss: 2.189974  [48896/60000]\n",
      "loss: 2.180476  [48960/60000]\n",
      "loss: 2.206897  [49024/60000]\n",
      "loss: 2.194756  [49088/60000]\n",
      "loss: 2.203685  [49152/60000]\n",
      "loss: 2.202123  [49216/60000]\n",
      "loss: 2.196857  [49280/60000]\n",
      "loss: 2.185701  [49344/60000]\n",
      "loss: 2.183372  [49408/60000]\n",
      "loss: 2.206805  [49472/60000]\n",
      "loss: 2.203052  [49536/60000]\n",
      "loss: 2.191447  [49600/60000]\n",
      "loss: 2.205182  [49664/60000]\n",
      "loss: 2.201150  [49728/60000]\n",
      "loss: 2.180423  [49792/60000]\n",
      "loss: 2.177538  [49856/60000]\n",
      "loss: 2.183561  [49920/60000]\n",
      "loss: 2.200621  [49984/60000]\n",
      "loss: 2.211284  [50048/60000]\n",
      "loss: 2.166459  [50112/60000]\n",
      "loss: 2.200186  [50176/60000]\n",
      "loss: 2.218785  [50240/60000]\n",
      "loss: 2.203549  [50304/60000]\n",
      "loss: 2.201333  [50368/60000]\n",
      "loss: 2.201441  [50432/60000]\n",
      "loss: 2.211341  [50496/60000]\n",
      "loss: 2.209311  [50560/60000]\n",
      "loss: 2.178658  [50624/60000]\n",
      "loss: 2.197627  [50688/60000]\n",
      "loss: 2.188856  [50752/60000]\n",
      "loss: 2.206430  [50816/60000]\n",
      "loss: 2.188291  [50880/60000]\n",
      "loss: 2.185562  [50944/60000]\n",
      "loss: 2.211935  [51008/60000]\n",
      "loss: 2.221605  [51072/60000]\n",
      "loss: 2.210569  [51136/60000]\n",
      "loss: 2.184306  [51200/60000]\n",
      "loss: 2.199898  [51264/60000]\n",
      "loss: 2.199844  [51328/60000]\n",
      "loss: 2.199407  [51392/60000]\n",
      "loss: 2.190082  [51456/60000]\n",
      "loss: 2.222101  [51520/60000]\n",
      "loss: 2.178275  [51584/60000]\n",
      "loss: 2.188045  [51648/60000]\n",
      "loss: 2.221618  [51712/60000]\n",
      "loss: 2.181613  [51776/60000]\n",
      "loss: 2.167273  [51840/60000]\n",
      "loss: 2.184793  [51904/60000]\n",
      "loss: 2.182890  [51968/60000]\n",
      "loss: 2.179367  [52032/60000]\n",
      "loss: 2.192026  [52096/60000]\n",
      "loss: 2.206052  [52160/60000]\n",
      "loss: 2.201106  [52224/60000]\n",
      "loss: 2.181200  [52288/60000]\n",
      "loss: 2.175685  [52352/60000]\n",
      "loss: 2.188032  [52416/60000]\n",
      "loss: 2.196562  [52480/60000]\n",
      "loss: 2.194787  [52544/60000]\n",
      "loss: 2.212627  [52608/60000]\n",
      "loss: 2.201395  [52672/60000]\n",
      "loss: 2.180919  [52736/60000]\n",
      "loss: 2.189094  [52800/60000]\n",
      "loss: 2.194269  [52864/60000]\n",
      "loss: 2.188776  [52928/60000]\n",
      "loss: 2.189732  [52992/60000]\n",
      "loss: 2.183021  [53056/60000]\n",
      "loss: 2.207925  [53120/60000]\n",
      "loss: 2.208980  [53184/60000]\n",
      "loss: 2.195039  [53248/60000]\n",
      "loss: 2.196725  [53312/60000]\n",
      "loss: 2.218650  [53376/60000]\n",
      "loss: 2.211733  [53440/60000]\n",
      "loss: 2.205685  [53504/60000]\n",
      "loss: 2.165376  [53568/60000]\n",
      "loss: 2.182555  [53632/60000]\n",
      "loss: 2.220160  [53696/60000]\n",
      "loss: 2.186038  [53760/60000]\n",
      "loss: 2.192338  [53824/60000]\n",
      "loss: 2.197107  [53888/60000]\n",
      "loss: 2.189515  [53952/60000]\n",
      "loss: 2.185167  [54016/60000]\n",
      "loss: 2.209229  [54080/60000]\n",
      "loss: 2.190511  [54144/60000]\n",
      "loss: 2.198678  [54208/60000]\n",
      "loss: 2.208163  [54272/60000]\n",
      "loss: 2.179259  [54336/60000]\n",
      "loss: 2.210195  [54400/60000]\n",
      "loss: 2.158802  [54464/60000]\n",
      "loss: 2.175701  [54528/60000]\n",
      "loss: 2.168445  [54592/60000]\n",
      "loss: 2.190810  [54656/60000]\n",
      "loss: 2.182723  [54720/60000]\n",
      "loss: 2.165533  [54784/60000]\n",
      "loss: 2.191008  [54848/60000]\n",
      "loss: 2.199196  [54912/60000]\n",
      "loss: 2.203031  [54976/60000]\n",
      "loss: 2.191087  [55040/60000]\n",
      "loss: 2.156718  [55104/60000]\n",
      "loss: 2.196831  [55168/60000]\n",
      "loss: 2.194023  [55232/60000]\n",
      "loss: 2.191330  [55296/60000]\n",
      "loss: 2.188099  [55360/60000]\n",
      "loss: 2.183125  [55424/60000]\n",
      "loss: 2.187852  [55488/60000]\n",
      "loss: 2.200847  [55552/60000]\n",
      "loss: 2.196651  [55616/60000]\n",
      "loss: 2.199011  [55680/60000]\n",
      "loss: 2.181536  [55744/60000]\n",
      "loss: 2.205976  [55808/60000]\n",
      "loss: 2.183062  [55872/60000]\n",
      "loss: 2.199219  [55936/60000]\n",
      "loss: 2.176645  [56000/60000]\n",
      "loss: 2.196891  [56064/60000]\n",
      "loss: 2.204898  [56128/60000]\n",
      "loss: 2.182188  [56192/60000]\n",
      "loss: 2.193938  [56256/60000]\n",
      "loss: 2.187393  [56320/60000]\n",
      "loss: 2.193871  [56384/60000]\n",
      "loss: 2.149534  [56448/60000]\n",
      "loss: 2.168481  [56512/60000]\n",
      "loss: 2.204538  [56576/60000]\n",
      "loss: 2.193533  [56640/60000]\n",
      "loss: 2.183794  [56704/60000]\n",
      "loss: 2.169561  [56768/60000]\n",
      "loss: 2.199722  [56832/60000]\n",
      "loss: 2.189898  [56896/60000]\n",
      "loss: 2.188701  [56960/60000]\n",
      "loss: 2.175323  [57024/60000]\n",
      "loss: 2.202796  [57088/60000]\n",
      "loss: 2.192071  [57152/60000]\n",
      "loss: 2.175472  [57216/60000]\n",
      "loss: 2.193486  [57280/60000]\n",
      "loss: 2.192891  [57344/60000]\n",
      "loss: 2.208305  [57408/60000]\n",
      "loss: 2.182909  [57472/60000]\n",
      "loss: 2.208219  [57536/60000]\n",
      "loss: 2.188427  [57600/60000]\n",
      "loss: 2.188216  [57664/60000]\n",
      "loss: 2.169164  [57728/60000]\n",
      "loss: 2.182251  [57792/60000]\n",
      "loss: 2.175458  [57856/60000]\n",
      "loss: 2.177642  [57920/60000]\n",
      "loss: 2.175216  [57984/60000]\n",
      "loss: 2.191290  [58048/60000]\n",
      "loss: 2.189166  [58112/60000]\n",
      "loss: 2.194121  [58176/60000]\n",
      "loss: 2.170358  [58240/60000]\n",
      "loss: 2.186894  [58304/60000]\n",
      "loss: 2.206638  [58368/60000]\n",
      "loss: 2.193534  [58432/60000]\n",
      "loss: 2.173395  [58496/60000]\n",
      "loss: 2.173998  [58560/60000]\n",
      "loss: 2.183964  [58624/60000]\n",
      "loss: 2.202213  [58688/60000]\n",
      "loss: 2.187377  [58752/60000]\n",
      "loss: 2.194913  [58816/60000]\n",
      "loss: 2.179339  [58880/60000]\n",
      "loss: 2.207344  [58944/60000]\n",
      "loss: 2.176926  [59008/60000]\n",
      "loss: 2.155079  [59072/60000]\n",
      "loss: 2.190694  [59136/60000]\n",
      "loss: 2.182075  [59200/60000]\n",
      "loss: 2.169862  [59264/60000]\n",
      "loss: 2.174717  [59328/60000]\n",
      "loss: 2.168276  [59392/60000]\n",
      "loss: 2.178107  [59456/60000]\n",
      "loss: 2.167321  [59520/60000]\n",
      "loss: 2.185556  [59584/60000]\n",
      "loss: 2.186520  [59648/60000]\n",
      "loss: 2.194886  [59712/60000]\n",
      "loss: 2.158517  [59776/60000]\n",
      "loss: 2.181852  [59840/60000]\n",
      "loss: 2.174654  [59904/60000]\n",
      "loss: 2.213598  [29984/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 2.179843 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.174191  [   64/60000]\n",
      "loss: 2.185237  [  128/60000]\n",
      "loss: 2.178418  [  192/60000]\n",
      "loss: 2.169914  [  256/60000]\n",
      "loss: 2.183260  [  320/60000]\n",
      "loss: 2.177673  [  384/60000]\n",
      "loss: 2.187954  [  448/60000]\n",
      "loss: 2.189231  [  512/60000]\n",
      "loss: 2.191895  [  576/60000]\n",
      "loss: 2.205909  [  640/60000]\n",
      "loss: 2.182030  [  704/60000]\n",
      "loss: 2.192117  [  768/60000]\n",
      "loss: 2.163738  [  832/60000]\n",
      "loss: 2.185459  [  896/60000]\n",
      "loss: 2.175129  [  960/60000]\n",
      "loss: 2.208380  [ 1024/60000]\n",
      "loss: 2.174421  [ 1088/60000]\n",
      "loss: 2.166643  [ 1152/60000]\n",
      "loss: 2.175660  [ 1216/60000]\n",
      "loss: 2.142774  [ 1280/60000]\n",
      "loss: 2.175368  [ 1344/60000]\n",
      "loss: 2.195806  [ 1408/60000]\n",
      "loss: 2.175683  [ 1472/60000]\n",
      "loss: 2.196609  [ 1536/60000]\n",
      "loss: 2.173998  [ 1600/60000]\n",
      "loss: 2.172867  [ 1664/60000]\n",
      "loss: 2.179528  [ 1728/60000]\n",
      "loss: 2.191724  [ 1792/60000]\n",
      "loss: 2.150552  [ 1856/60000]\n",
      "loss: 2.202899  [ 1920/60000]\n",
      "loss: 2.195379  [ 1984/60000]\n",
      "loss: 2.159358  [ 2048/60000]\n",
      "loss: 2.163883  [ 2112/60000]\n",
      "loss: 2.178274  [ 2176/60000]\n",
      "loss: 2.198786  [ 2240/60000]\n",
      "loss: 2.172516  [ 2304/60000]\n",
      "loss: 2.182213  [ 2368/60000]\n",
      "loss: 2.175679  [ 2432/60000]\n",
      "loss: 2.169785  [ 2496/60000]\n",
      "loss: 2.196503  [ 2560/60000]\n",
      "loss: 2.168587  [ 2624/60000]\n",
      "loss: 2.178117  [ 2688/60000]\n",
      "loss: 2.184378  [ 2752/60000]\n",
      "loss: 2.177794  [ 2816/60000]\n",
      "loss: 2.188635  [ 2880/60000]\n",
      "loss: 2.158571  [ 2944/60000]\n",
      "loss: 2.190387  [ 3008/60000]\n",
      "loss: 2.169340  [ 3072/60000]\n",
      "loss: 2.186873  [ 3136/60000]\n",
      "loss: 2.190098  [ 3200/60000]\n",
      "loss: 2.181983  [ 3264/60000]\n",
      "loss: 2.154232  [ 3328/60000]\n",
      "loss: 2.202769  [ 3392/60000]\n",
      "loss: 2.159994  [ 3456/60000]\n",
      "loss: 2.162374  [ 3520/60000]\n",
      "loss: 2.188958  [ 3584/60000]\n",
      "loss: 2.143585  [ 3648/60000]\n",
      "loss: 2.180357  [ 3712/60000]\n",
      "loss: 2.180877  [ 3776/60000]\n",
      "loss: 2.165428  [ 3840/60000]\n",
      "loss: 2.176917  [ 3904/60000]\n",
      "loss: 2.175599  [ 3968/60000]\n",
      "loss: 2.199566  [ 4032/60000]\n",
      "loss: 2.172019  [ 4096/60000]\n",
      "loss: 2.163875  [ 4160/60000]\n",
      "loss: 2.190874  [ 4224/60000]\n",
      "loss: 2.157106  [ 4288/60000]\n",
      "loss: 2.169997  [ 4352/60000]\n",
      "loss: 2.184100  [ 4416/60000]\n",
      "loss: 2.173233  [ 4480/60000]\n",
      "loss: 2.169238  [ 4544/60000]\n",
      "loss: 2.162234  [ 4608/60000]\n",
      "loss: 2.197723  [ 4672/60000]\n",
      "loss: 2.203278  [ 4736/60000]\n",
      "loss: 2.187065  [ 4800/60000]\n",
      "loss: 2.172291  [ 4864/60000]\n",
      "loss: 2.149075  [ 4928/60000]\n",
      "loss: 2.172816  [ 4992/60000]\n",
      "loss: 2.180250  [ 5056/60000]\n",
      "loss: 2.165790  [ 5120/60000]\n",
      "loss: 2.178720  [ 5184/60000]\n",
      "loss: 2.161246  [ 5248/60000]\n",
      "loss: 2.157522  [ 5312/60000]\n",
      "loss: 2.169493  [ 5376/60000]\n",
      "loss: 2.166914  [ 5440/60000]\n",
      "loss: 2.163190  [ 5504/60000]\n",
      "loss: 2.161194  [ 5568/60000]\n",
      "loss: 2.220026  [ 5632/60000]\n",
      "loss: 2.165076  [ 5696/60000]\n",
      "loss: 2.171147  [ 5760/60000]\n",
      "loss: 2.162826  [ 5824/60000]\n",
      "loss: 2.185152  [ 5888/60000]\n",
      "loss: 2.168405  [ 5952/60000]\n",
      "loss: 2.177505  [ 6016/60000]\n",
      "loss: 2.177557  [ 6080/60000]\n",
      "loss: 2.170926  [ 6144/60000]\n",
      "loss: 2.194859  [ 6208/60000]\n",
      "loss: 2.157438  [ 6272/60000]\n",
      "loss: 2.181248  [ 6336/60000]\n",
      "loss: 2.132513  [ 6400/60000]\n",
      "loss: 2.186664  [ 6464/60000]\n",
      "loss: 2.169152  [ 6528/60000]\n",
      "loss: 2.162504  [ 6592/60000]\n",
      "loss: 2.188212  [ 6656/60000]\n",
      "loss: 2.177833  [ 6720/60000]\n",
      "loss: 2.166965  [ 6784/60000]\n",
      "loss: 2.154747  [ 6848/60000]\n",
      "loss: 2.161946  [ 6912/60000]\n",
      "loss: 2.170190  [ 6976/60000]\n",
      "loss: 2.154651  [ 7040/60000]\n",
      "loss: 2.174164  [ 7104/60000]\n",
      "loss: 2.183149  [ 7168/60000]\n",
      "loss: 2.187308  [ 7232/60000]\n",
      "loss: 2.151513  [ 7296/60000]\n",
      "loss: 2.176967  [ 7360/60000]\n",
      "loss: 2.187451  [ 7424/60000]\n",
      "loss: 2.168402  [ 7488/60000]\n",
      "loss: 2.194285  [ 7552/60000]\n",
      "loss: 2.164469  [ 7616/60000]\n",
      "loss: 2.174309  [ 7680/60000]\n",
      "loss: 2.151900  [ 7744/60000]\n",
      "loss: 2.170465  [ 7808/60000]\n",
      "loss: 2.173242  [ 7872/60000]\n",
      "loss: 2.189716  [ 7936/60000]\n",
      "loss: 2.176728  [ 8000/60000]\n",
      "loss: 2.147058  [ 8064/60000]\n",
      "loss: 2.176478  [ 8128/60000]\n",
      "loss: 2.172537  [ 8192/60000]\n",
      "loss: 2.168979  [ 8256/60000]\n",
      "loss: 2.158375  [ 8320/60000]\n",
      "loss: 2.155438  [ 8384/60000]\n",
      "loss: 2.178098  [ 8448/60000]\n",
      "loss: 2.186697  [ 8512/60000]\n",
      "loss: 2.185417  [ 8576/60000]\n",
      "loss: 2.175736  [ 8640/60000]\n",
      "loss: 2.149899  [ 8704/60000]\n",
      "loss: 2.152731  [ 8768/60000]\n",
      "loss: 2.156991  [ 8832/60000]\n",
      "loss: 2.175356  [ 8896/60000]\n",
      "loss: 2.153735  [ 8960/60000]\n",
      "loss: 2.164133  [ 9024/60000]\n",
      "loss: 2.169054  [ 9088/60000]\n",
      "loss: 2.146134  [ 9152/60000]\n",
      "loss: 2.154558  [ 9216/60000]\n",
      "loss: 2.167762  [ 9280/60000]\n",
      "loss: 2.164446  [ 9344/60000]\n",
      "loss: 2.179224  [ 9408/60000]\n",
      "loss: 2.189119  [ 9472/60000]\n",
      "loss: 2.157680  [ 9536/60000]\n",
      "loss: 2.160178  [ 9600/60000]\n",
      "loss: 2.151592  [ 9664/60000]\n",
      "loss: 2.171780  [ 9728/60000]\n",
      "loss: 2.171024  [ 9792/60000]\n",
      "loss: 2.166423  [ 9856/60000]\n",
      "loss: 2.160752  [ 9920/60000]\n",
      "loss: 2.186350  [ 9984/60000]\n",
      "loss: 2.113364  [10048/60000]\n",
      "loss: 2.166475  [10112/60000]\n",
      "loss: 2.165035  [10176/60000]\n",
      "loss: 2.158917  [10240/60000]\n",
      "loss: 2.180806  [10304/60000]\n",
      "loss: 2.164598  [10368/60000]\n",
      "loss: 2.175241  [10432/60000]\n",
      "loss: 2.166854  [10496/60000]\n",
      "loss: 2.177215  [10560/60000]\n",
      "loss: 2.171031  [10624/60000]\n",
      "loss: 2.164769  [10688/60000]\n",
      "loss: 2.149727  [10752/60000]\n",
      "loss: 2.140423  [10816/60000]\n",
      "loss: 2.149173  [10880/60000]\n",
      "loss: 2.140428  [10944/60000]\n",
      "loss: 2.169304  [11008/60000]\n",
      "loss: 2.168725  [11072/60000]\n",
      "loss: 2.176026  [11136/60000]\n",
      "loss: 2.158736  [11200/60000]\n",
      "loss: 2.167538  [11264/60000]\n",
      "loss: 2.160100  [11328/60000]\n",
      "loss: 2.157377  [11392/60000]\n",
      "loss: 2.154647  [11456/60000]\n",
      "loss: 2.161043  [11520/60000]\n",
      "loss: 2.174302  [11584/60000]\n",
      "loss: 2.187712  [11648/60000]\n",
      "loss: 2.173350  [11712/60000]\n",
      "loss: 2.158014  [11776/60000]\n",
      "loss: 2.163594  [11840/60000]\n",
      "loss: 2.167093  [11904/60000]\n",
      "loss: 2.155134  [11968/60000]\n",
      "loss: 2.152099  [12032/60000]\n",
      "loss: 2.157096  [12096/60000]\n",
      "loss: 2.140944  [12160/60000]\n",
      "loss: 2.133173  [12224/60000]\n",
      "loss: 2.158616  [12288/60000]\n",
      "loss: 2.164867  [12352/60000]\n",
      "loss: 2.169780  [12416/60000]\n",
      "loss: 2.174246  [12480/60000]\n",
      "loss: 2.162320  [12544/60000]\n",
      "loss: 2.184058  [12608/60000]\n",
      "loss: 2.147589  [12672/60000]\n",
      "loss: 2.146178  [12736/60000]\n",
      "loss: 2.165411  [12800/60000]\n",
      "loss: 2.178226  [12864/60000]\n",
      "loss: 2.168616  [12928/60000]\n",
      "loss: 2.152704  [12992/60000]\n",
      "loss: 2.153641  [13056/60000]\n",
      "loss: 2.174724  [13120/60000]\n",
      "loss: 2.178926  [13184/60000]\n",
      "loss: 2.166904  [13248/60000]\n",
      "loss: 2.158123  [13312/60000]\n",
      "loss: 2.156462  [13376/60000]\n",
      "loss: 2.165237  [13440/60000]\n",
      "loss: 2.182886  [13504/60000]\n",
      "loss: 2.128175  [13568/60000]\n",
      "loss: 2.171699  [13632/60000]\n",
      "loss: 2.135878  [13696/60000]\n",
      "loss: 2.155180  [13760/60000]\n",
      "loss: 2.152250  [13824/60000]\n",
      "loss: 2.159899  [13888/60000]\n",
      "loss: 2.161503  [13952/60000]\n",
      "loss: 2.156664  [14016/60000]\n",
      "loss: 2.162367  [14080/60000]\n",
      "loss: 2.159758  [14144/60000]\n",
      "loss: 2.154561  [14208/60000]\n",
      "loss: 2.128854  [14272/60000]\n",
      "loss: 2.140993  [14336/60000]\n",
      "loss: 2.176921  [14400/60000]\n",
      "loss: 2.143928  [14464/60000]\n",
      "loss: 2.160943  [14528/60000]\n",
      "loss: 2.180526  [14592/60000]\n",
      "loss: 2.184186  [14656/60000]\n",
      "loss: 2.157828  [14720/60000]\n",
      "loss: 2.148360  [14784/60000]\n",
      "loss: 2.181556  [14848/60000]\n",
      "loss: 2.186456  [14912/60000]\n",
      "loss: 2.150782  [14976/60000]\n",
      "loss: 2.178863  [15040/60000]\n",
      "loss: 2.149328  [15104/60000]\n",
      "loss: 2.135767  [15168/60000]\n",
      "loss: 2.180874  [15232/60000]\n",
      "loss: 2.162882  [15296/60000]\n",
      "loss: 2.145609  [15360/60000]\n",
      "loss: 2.152570  [15424/60000]\n",
      "loss: 2.156519  [15488/60000]\n",
      "loss: 2.158179  [15552/60000]\n",
      "loss: 2.176237  [15616/60000]\n",
      "loss: 2.182806  [15680/60000]\n",
      "loss: 2.163746  [15744/60000]\n",
      "loss: 2.156400  [15808/60000]\n",
      "loss: 2.157552  [15872/60000]\n",
      "loss: 2.139665  [15936/60000]\n",
      "loss: 2.141097  [16000/60000]\n",
      "loss: 2.150857  [16064/60000]\n",
      "loss: 2.138577  [16128/60000]\n",
      "loss: 2.112620  [16192/60000]\n",
      "loss: 2.196934  [16256/60000]\n",
      "loss: 2.182145  [16320/60000]\n",
      "loss: 2.159032  [16384/60000]\n",
      "loss: 2.149038  [16448/60000]\n",
      "loss: 2.182850  [16512/60000]\n",
      "loss: 2.144892  [16576/60000]\n",
      "loss: 2.120219  [16640/60000]\n",
      "loss: 2.146561  [16704/60000]\n",
      "loss: 2.143672  [16768/60000]\n",
      "loss: 2.154281  [16832/60000]\n",
      "loss: 2.176370  [16896/60000]\n",
      "loss: 2.164713  [16960/60000]\n",
      "loss: 2.163394  [17024/60000]\n",
      "loss: 2.168967  [17088/60000]\n",
      "loss: 2.129147  [17152/60000]\n",
      "loss: 2.148543  [17216/60000]\n",
      "loss: 2.147321  [17280/60000]\n",
      "loss: 2.144808  [17344/60000]\n",
      "loss: 2.199146  [17408/60000]\n",
      "loss: 2.160965  [17472/60000]\n",
      "loss: 2.149331  [17536/60000]\n",
      "loss: 2.145433  [17600/60000]\n",
      "loss: 2.169119  [17664/60000]\n",
      "loss: 2.168173  [17728/60000]\n",
      "loss: 2.151889  [17792/60000]\n",
      "loss: 2.140063  [17856/60000]\n",
      "loss: 2.182530  [17920/60000]\n",
      "loss: 2.143901  [17984/60000]\n",
      "loss: 2.174306  [18048/60000]\n",
      "loss: 2.174514  [18112/60000]\n",
      "loss: 2.151405  [18176/60000]\n",
      "loss: 2.167829  [18240/60000]\n",
      "loss: 2.147967  [18304/60000]\n",
      "loss: 2.180202  [18368/60000]\n",
      "loss: 2.184193  [18432/60000]\n",
      "loss: 2.148089  [18496/60000]\n",
      "loss: 2.128896  [18560/60000]\n",
      "loss: 2.148271  [18624/60000]\n",
      "loss: 2.132949  [18688/60000]\n",
      "loss: 2.147845  [18752/60000]\n",
      "loss: 2.164137  [18816/60000]\n",
      "loss: 2.154241  [18880/60000]\n",
      "loss: 2.157187  [18944/60000]\n",
      "loss: 2.130040  [19008/60000]\n",
      "loss: 2.155161  [19072/60000]\n",
      "loss: 2.186729  [19136/60000]\n",
      "loss: 2.168704  [19200/60000]\n",
      "loss: 2.137364  [19264/60000]\n",
      "loss: 2.153674  [19328/60000]\n",
      "loss: 2.144282  [19392/60000]\n",
      "loss: 2.156869  [19456/60000]\n",
      "loss: 2.132587  [19520/60000]\n",
      "loss: 2.150538  [19584/60000]\n",
      "loss: 2.146069  [19648/60000]\n",
      "loss: 2.137044  [19712/60000]\n",
      "loss: 2.130290  [19776/60000]\n",
      "loss: 2.119965  [19840/60000]\n",
      "loss: 2.151988  [19904/60000]\n",
      "loss: 2.133116  [19968/60000]\n",
      "loss: 2.118498  [20032/60000]\n",
      "loss: 2.126221  [20096/60000]\n",
      "loss: 2.156818  [20160/60000]\n",
      "loss: 2.139076  [20224/60000]\n",
      "loss: 2.170937  [20288/60000]\n",
      "loss: 2.178225  [20352/60000]\n",
      "loss: 2.156891  [20416/60000]\n",
      "loss: 2.151276  [20480/60000]\n",
      "loss: 2.143222  [20544/60000]\n",
      "loss: 2.128125  [20608/60000]\n",
      "loss: 2.158494  [20672/60000]\n",
      "loss: 2.168855  [20736/60000]\n",
      "loss: 2.153069  [20800/60000]\n",
      "loss: 2.140906  [20864/60000]\n",
      "loss: 2.153914  [20928/60000]\n",
      "loss: 2.138000  [20992/60000]\n",
      "loss: 2.111626  [21056/60000]\n",
      "loss: 2.144948  [21120/60000]\n",
      "loss: 2.121284  [21184/60000]\n",
      "loss: 2.151583  [21248/60000]\n",
      "loss: 2.163759  [21312/60000]\n",
      "loss: 2.180600  [21376/60000]\n",
      "loss: 2.162667  [21440/60000]\n",
      "loss: 2.160905  [21504/60000]\n",
      "loss: 2.138442  [21568/60000]\n",
      "loss: 2.135259  [21632/60000]\n",
      "loss: 2.164229  [21696/60000]\n",
      "loss: 2.149721  [21760/60000]\n",
      "loss: 2.167653  [21824/60000]\n",
      "loss: 2.167284  [21888/60000]\n",
      "loss: 2.141870  [21952/60000]\n",
      "loss: 2.147184  [22016/60000]\n",
      "loss: 2.165985  [22080/60000]\n",
      "loss: 2.130647  [22144/60000]\n",
      "loss: 2.162515  [22208/60000]\n",
      "loss: 2.092361  [22272/60000]\n",
      "loss: 2.132390  [22336/60000]\n",
      "loss: 2.145679  [22400/60000]\n",
      "loss: 2.139609  [22464/60000]\n",
      "loss: 2.148632  [22528/60000]\n",
      "loss: 2.147813  [22592/60000]\n",
      "loss: 2.154755  [22656/60000]\n",
      "loss: 2.150666  [22720/60000]\n",
      "loss: 2.158034  [22784/60000]\n",
      "loss: 2.140819  [22848/60000]\n",
      "loss: 2.177508  [22912/60000]\n",
      "loss: 2.149134  [22976/60000]\n",
      "loss: 2.141971  [23040/60000]\n",
      "loss: 2.165064  [23104/60000]\n",
      "loss: 2.139113  [23168/60000]\n",
      "loss: 2.151923  [23232/60000]\n",
      "loss: 2.138405  [23296/60000]\n",
      "loss: 2.142446  [23360/60000]\n",
      "loss: 2.140058  [23424/60000]\n",
      "loss: 2.153057  [23488/60000]\n",
      "loss: 2.161403  [23552/60000]\n",
      "loss: 2.144524  [23616/60000]\n",
      "loss: 2.132647  [23680/60000]\n",
      "loss: 2.125556  [23744/60000]\n",
      "loss: 2.126260  [23808/60000]\n",
      "loss: 2.153364  [23872/60000]\n",
      "loss: 2.145610  [23936/60000]\n",
      "loss: 2.108739  [24000/60000]\n",
      "loss: 2.109259  [24064/60000]\n",
      "loss: 2.158650  [24128/60000]\n",
      "loss: 2.112452  [24192/60000]\n",
      "loss: 2.139972  [24256/60000]\n",
      "loss: 2.140901  [24320/60000]\n",
      "loss: 2.078049  [24384/60000]\n",
      "loss: 2.143431  [24448/60000]\n",
      "loss: 2.149266  [24512/60000]\n",
      "loss: 2.142377  [24576/60000]\n",
      "loss: 2.106584  [24640/60000]\n",
      "loss: 2.108352  [24704/60000]\n",
      "loss: 2.138887  [24768/60000]\n",
      "loss: 2.146818  [24832/60000]\n",
      "loss: 2.131407  [24896/60000]\n",
      "loss: 2.119187  [24960/60000]\n",
      "loss: 2.136531  [25024/60000]\n",
      "loss: 2.147939  [25088/60000]\n",
      "loss: 2.179195  [25152/60000]\n",
      "loss: 2.152482  [25216/60000]\n",
      "loss: 2.157495  [25280/60000]\n",
      "loss: 2.118956  [25344/60000]\n",
      "loss: 2.136474  [25408/60000]\n",
      "loss: 2.131884  [25472/60000]\n",
      "loss: 2.134153  [25536/60000]\n",
      "loss: 2.127650  [25600/60000]\n",
      "loss: 2.133533  [25664/60000]\n",
      "loss: 2.166514  [25728/60000]\n",
      "loss: 2.126621  [25792/60000]\n",
      "loss: 2.142736  [25856/60000]\n",
      "loss: 2.151376  [25920/60000]\n",
      "loss: 2.154872  [25984/60000]\n",
      "loss: 2.109090  [26048/60000]\n",
      "loss: 2.140070  [26112/60000]\n",
      "loss: 2.119408  [26176/60000]\n",
      "loss: 2.155461  [26240/60000]\n",
      "loss: 2.150817  [26304/60000]\n",
      "loss: 2.185218  [26368/60000]\n",
      "loss: 2.139037  [26432/60000]\n",
      "loss: 2.136499  [26496/60000]\n",
      "loss: 2.126331  [26560/60000]\n",
      "loss: 2.108582  [26624/60000]\n",
      "loss: 2.130815  [26688/60000]\n",
      "loss: 2.122975  [26752/60000]\n",
      "loss: 2.129909  [26816/60000]\n",
      "loss: 2.134451  [26880/60000]\n",
      "loss: 2.101443  [26944/60000]\n",
      "loss: 2.165195  [27008/60000]\n",
      "loss: 2.157048  [27072/60000]\n",
      "loss: 2.141537  [27136/60000]\n",
      "loss: 2.151911  [27200/60000]\n",
      "loss: 2.104646  [27264/60000]\n",
      "loss: 2.128648  [27328/60000]\n",
      "loss: 2.133790  [27392/60000]\n",
      "loss: 2.145033  [27456/60000]\n",
      "loss: 2.146216  [27520/60000]\n",
      "loss: 2.133398  [27584/60000]\n",
      "loss: 2.124308  [27648/60000]\n",
      "loss: 2.158279  [27712/60000]\n",
      "loss: 2.143064  [27776/60000]\n",
      "loss: 2.129736  [27840/60000]\n",
      "loss: 2.138342  [27904/60000]\n",
      "loss: 2.128415  [27968/60000]\n",
      "loss: 2.160410  [28032/60000]\n",
      "loss: 2.147246  [28096/60000]\n",
      "loss: 2.151544  [28160/60000]\n",
      "loss: 2.118824  [28224/60000]\n",
      "loss: 2.156657  [28288/60000]\n",
      "loss: 2.137784  [28352/60000]\n",
      "loss: 2.141616  [28416/60000]\n",
      "loss: 2.145420  [28480/60000]\n",
      "loss: 2.116198  [28544/60000]\n",
      "loss: 2.123672  [28608/60000]\n",
      "loss: 2.117600  [28672/60000]\n",
      "loss: 2.141448  [28736/60000]\n",
      "loss: 2.115189  [28800/60000]\n",
      "loss: 2.136913  [28864/60000]\n",
      "loss: 2.140038  [28928/60000]\n",
      "loss: 2.101682  [28992/60000]\n",
      "loss: 2.154156  [29056/60000]\n",
      "loss: 2.105601  [29120/60000]\n",
      "loss: 2.139242  [29184/60000]\n",
      "loss: 2.137231  [29248/60000]\n",
      "loss: 2.099485  [29312/60000]\n",
      "loss: 2.119354  [29376/60000]\n",
      "loss: 2.133026  [29440/60000]\n",
      "loss: 2.110266  [29504/60000]\n",
      "loss: 2.132526  [29568/60000]\n",
      "loss: 2.165313  [29632/60000]\n",
      "loss: 2.126133  [29696/60000]\n",
      "loss: 2.122132  [29760/60000]\n",
      "loss: 2.123625  [29824/60000]\n",
      "loss: 2.139479  [29888/60000]\n",
      "loss: 2.132574  [29952/60000]\n",
      "loss: 2.124391  [30016/60000]\n",
      "loss: 2.116925  [30080/60000]\n",
      "loss: 2.123848  [30144/60000]\n",
      "loss: 2.113260  [30208/60000]\n",
      "loss: 2.134472  [30272/60000]\n",
      "loss: 2.116074  [30336/60000]\n",
      "loss: 2.137956  [30400/60000]\n",
      "loss: 2.140493  [30464/60000]\n",
      "loss: 2.132940  [30528/60000]\n",
      "loss: 2.090542  [30592/60000]\n",
      "loss: 2.128848  [30656/60000]\n",
      "loss: 2.116554  [30720/60000]\n",
      "loss: 2.137682  [30784/60000]\n",
      "loss: 2.145337  [30848/60000]\n",
      "loss: 2.134075  [30912/60000]\n",
      "loss: 2.134690  [30976/60000]\n",
      "loss: 2.124983  [31040/60000]\n",
      "loss: 2.182151  [31104/60000]\n",
      "loss: 2.127210  [31168/60000]\n",
      "loss: 2.152624  [31232/60000]\n",
      "loss: 2.137607  [31296/60000]\n",
      "loss: 2.107398  [31360/60000]\n",
      "loss: 2.124381  [31424/60000]\n",
      "loss: 2.127630  [31488/60000]\n",
      "loss: 2.135101  [31552/60000]\n",
      "loss: 2.115437  [31616/60000]\n",
      "loss: 2.127598  [31680/60000]\n",
      "loss: 2.169240  [31744/60000]\n",
      "loss: 2.112388  [31808/60000]\n",
      "loss: 2.133005  [31872/60000]\n",
      "loss: 2.102663  [31936/60000]\n",
      "loss: 2.090117  [32000/60000]\n",
      "loss: 2.131478  [32064/60000]\n",
      "loss: 2.128774  [32128/60000]\n",
      "loss: 2.151700  [32192/60000]\n",
      "loss: 2.107339  [32256/60000]\n",
      "loss: 2.141993  [32320/60000]\n",
      "loss: 2.134571  [32384/60000]\n",
      "loss: 2.110084  [32448/60000]\n",
      "loss: 2.118455  [32512/60000]\n",
      "loss: 2.133385  [32576/60000]\n",
      "loss: 2.135337  [32640/60000]\n",
      "loss: 2.140744  [32704/60000]\n",
      "loss: 2.113101  [32768/60000]\n",
      "loss: 2.119091  [32832/60000]\n",
      "loss: 2.165843  [32896/60000]\n",
      "loss: 2.120693  [32960/60000]\n",
      "loss: 2.118386  [33024/60000]\n",
      "loss: 2.156690  [33088/60000]\n",
      "loss: 2.140189  [33152/60000]\n",
      "loss: 2.139719  [33216/60000]\n",
      "loss: 2.111006  [33280/60000]\n",
      "loss: 2.096452  [33344/60000]\n",
      "loss: 2.127295  [33408/60000]\n",
      "loss: 2.122861  [33472/60000]\n",
      "loss: 2.138648  [33536/60000]\n",
      "loss: 2.114231  [33600/60000]\n",
      "loss: 2.097460  [33664/60000]\n",
      "loss: 2.133970  [33728/60000]\n",
      "loss: 2.092594  [33792/60000]\n",
      "loss: 2.125925  [33856/60000]\n",
      "loss: 2.142542  [33920/60000]\n",
      "loss: 2.165247  [33984/60000]\n",
      "loss: 2.119959  [34048/60000]\n",
      "loss: 2.102533  [34112/60000]\n",
      "loss: 2.143898  [34176/60000]\n",
      "loss: 2.141412  [34240/60000]\n",
      "loss: 2.135319  [34304/60000]\n",
      "loss: 2.117643  [34368/60000]\n",
      "loss: 2.137180  [34432/60000]\n",
      "loss: 2.087111  [34496/60000]\n",
      "loss: 2.125541  [34560/60000]\n",
      "loss: 2.137642  [34624/60000]\n",
      "loss: 2.121513  [34688/60000]\n",
      "loss: 2.114901  [34752/60000]\n",
      "loss: 2.122372  [34816/60000]\n",
      "loss: 2.100154  [34880/60000]\n",
      "loss: 2.135418  [34944/60000]\n",
      "loss: 2.160338  [35008/60000]\n",
      "loss: 2.126171  [35072/60000]\n",
      "loss: 2.085860  [35136/60000]\n",
      "loss: 2.086779  [35200/60000]\n",
      "loss: 2.136406  [35264/60000]\n",
      "loss: 2.118453  [35328/60000]\n",
      "loss: 2.096102  [35392/60000]\n",
      "loss: 2.124267  [35456/60000]\n",
      "loss: 2.126512  [35520/60000]\n",
      "loss: 2.119402  [35584/60000]\n",
      "loss: 2.126417  [35648/60000]\n",
      "loss: 2.112104  [35712/60000]\n",
      "loss: 2.114603  [35776/60000]\n",
      "loss: 2.132185  [35840/60000]\n",
      "loss: 2.078899  [35904/60000]\n",
      "loss: 2.120765  [35968/60000]\n",
      "loss: 2.122657  [36032/60000]\n",
      "loss: 2.086370  [36096/60000]\n",
      "loss: 2.118027  [36160/60000]\n",
      "loss: 2.135087  [36224/60000]\n",
      "loss: 2.147640  [36288/60000]\n",
      "loss: 2.105084  [36352/60000]\n",
      "loss: 2.127039  [36416/60000]\n",
      "loss: 2.140072  [36480/60000]\n",
      "loss: 2.129508  [36544/60000]\n",
      "loss: 2.104548  [36608/60000]\n",
      "loss: 2.130100  [36672/60000]\n",
      "loss: 2.105093  [36736/60000]\n",
      "loss: 2.102732  [36800/60000]\n",
      "loss: 2.114190  [36864/60000]\n",
      "loss: 2.119085  [36928/60000]\n",
      "loss: 2.106169  [36992/60000]\n",
      "loss: 2.129573  [37056/60000]\n",
      "loss: 2.141412  [37120/60000]\n",
      "loss: 2.130259  [37184/60000]\n",
      "loss: 2.120965  [37248/60000]\n",
      "loss: 2.113719  [37312/60000]\n",
      "loss: 2.127031  [37376/60000]\n",
      "loss: 2.133171  [37440/60000]\n",
      "loss: 2.107373  [37504/60000]\n",
      "loss: 2.077991  [37568/60000]\n",
      "loss: 2.104097  [37632/60000]\n",
      "loss: 2.100143  [37696/60000]\n",
      "loss: 2.113141  [37760/60000]\n",
      "loss: 2.142988  [37824/60000]\n",
      "loss: 2.108413  [37888/60000]\n",
      "loss: 2.097182  [37952/60000]\n",
      "loss: 2.109619  [38016/60000]\n",
      "loss: 2.112763  [38080/60000]\n",
      "loss: 2.099848  [38144/60000]\n",
      "loss: 2.122374  [38208/60000]\n",
      "loss: 2.104967  [38272/60000]\n",
      "loss: 2.095661  [38336/60000]\n",
      "loss: 2.108452  [38400/60000]\n",
      "loss: 2.129593  [38464/60000]\n",
      "loss: 2.112326  [38528/60000]\n",
      "loss: 2.155172  [38592/60000]\n",
      "loss: 2.114130  [38656/60000]\n",
      "loss: 2.145926  [38720/60000]\n",
      "loss: 2.147072  [38784/60000]\n",
      "loss: 2.098435  [38848/60000]\n",
      "loss: 2.115405  [38912/60000]\n",
      "loss: 2.095283  [38976/60000]\n",
      "loss: 2.142274  [39040/60000]\n",
      "loss: 2.086076  [39104/60000]\n",
      "loss: 2.130959  [39168/60000]\n",
      "loss: 2.078089  [39232/60000]\n",
      "loss: 2.145803  [39296/60000]\n",
      "loss: 2.148897  [39360/60000]\n",
      "loss: 2.101077  [39424/60000]\n",
      "loss: 2.120527  [39488/60000]\n",
      "loss: 2.121052  [39552/60000]\n",
      "loss: 2.131423  [39616/60000]\n",
      "loss: 2.117104  [39680/60000]\n",
      "loss: 2.043766  [39744/60000]\n",
      "loss: 2.107538  [39808/60000]\n",
      "loss: 2.116396  [39872/60000]\n",
      "loss: 2.102866  [39936/60000]\n",
      "loss: 2.089296  [40000/60000]\n",
      "loss: 2.130219  [40064/60000]\n",
      "loss: 2.110919  [40128/60000]\n",
      "loss: 2.125711  [40192/60000]\n",
      "loss: 2.133742  [40256/60000]\n",
      "loss: 2.113694  [40320/60000]\n",
      "loss: 2.094979  [40384/60000]\n",
      "loss: 2.064751  [40448/60000]\n",
      "loss: 2.113592  [40512/60000]\n",
      "loss: 2.085263  [40576/60000]\n",
      "loss: 2.116980  [40640/60000]\n",
      "loss: 2.121749  [40704/60000]\n",
      "loss: 2.108185  [40768/60000]\n",
      "loss: 2.122727  [40832/60000]\n",
      "loss: 2.127584  [40896/60000]\n",
      "loss: 2.129469  [40960/60000]\n",
      "loss: 2.048865  [41024/60000]\n",
      "loss: 2.130085  [41088/60000]\n",
      "loss: 2.110833  [41152/60000]\n",
      "loss: 2.083119  [41216/60000]\n",
      "loss: 2.099868  [41280/60000]\n",
      "loss: 2.112117  [41344/60000]\n",
      "loss: 2.137650  [41408/60000]\n",
      "loss: 2.080461  [41472/60000]\n",
      "loss: 2.120089  [41536/60000]\n",
      "loss: 2.102898  [41600/60000]\n",
      "loss: 2.118436  [41664/60000]\n",
      "loss: 2.106690  [41728/60000]\n",
      "loss: 2.110267  [41792/60000]\n",
      "loss: 2.104455  [41856/60000]\n",
      "loss: 2.108413  [41920/60000]\n",
      "loss: 2.078575  [41984/60000]\n",
      "loss: 2.123951  [42048/60000]\n",
      "loss: 2.113390  [42112/60000]\n",
      "loss: 2.135940  [42176/60000]\n",
      "loss: 2.120068  [42240/60000]\n",
      "loss: 2.112837  [42304/60000]\n",
      "loss: 2.065121  [42368/60000]\n",
      "loss: 2.122260  [42432/60000]\n",
      "loss: 2.112971  [42496/60000]\n",
      "loss: 2.073523  [42560/60000]\n",
      "loss: 2.106287  [42624/60000]\n",
      "loss: 2.094121  [42688/60000]\n",
      "loss: 2.103153  [42752/60000]\n",
      "loss: 2.083782  [42816/60000]\n",
      "loss: 2.109441  [42880/60000]\n",
      "loss: 2.109029  [42944/60000]\n",
      "loss: 2.101656  [43008/60000]\n",
      "loss: 2.062102  [43072/60000]\n",
      "loss: 2.129052  [43136/60000]\n",
      "loss: 2.091656  [43200/60000]\n",
      "loss: 2.122568  [43264/60000]\n",
      "loss: 2.095710  [43328/60000]\n",
      "loss: 2.064393  [43392/60000]\n",
      "loss: 2.093844  [43456/60000]\n",
      "loss: 2.107799  [43520/60000]\n",
      "loss: 2.112214  [43584/60000]\n",
      "loss: 2.119678  [43648/60000]\n",
      "loss: 2.088155  [43712/60000]\n",
      "loss: 2.097479  [43776/60000]\n",
      "loss: 2.110455  [43840/60000]\n",
      "loss: 2.083875  [43904/60000]\n",
      "loss: 2.078814  [43968/60000]\n",
      "loss: 2.067502  [44032/60000]\n",
      "loss: 2.094795  [44096/60000]\n",
      "loss: 2.098122  [44160/60000]\n",
      "loss: 2.141911  [44224/60000]\n",
      "loss: 2.123609  [44288/60000]\n",
      "loss: 2.112165  [44352/60000]\n",
      "loss: 2.107450  [44416/60000]\n",
      "loss: 2.135448  [44480/60000]\n",
      "loss: 2.112430  [44544/60000]\n",
      "loss: 2.110121  [44608/60000]\n",
      "loss: 2.105358  [44672/60000]\n",
      "loss: 2.069088  [44736/60000]\n",
      "loss: 2.122133  [44800/60000]\n",
      "loss: 2.099156  [44864/60000]\n",
      "loss: 2.122856  [44928/60000]\n",
      "loss: 2.107756  [44992/60000]\n",
      "loss: 2.086975  [45056/60000]\n",
      "loss: 2.111433  [45120/60000]\n",
      "loss: 2.111668  [45184/60000]\n",
      "loss: 2.137731  [45248/60000]\n",
      "loss: 2.115286  [45312/60000]\n",
      "loss: 2.066555  [45376/60000]\n",
      "loss: 2.106538  [45440/60000]\n",
      "loss: 2.089326  [45504/60000]\n",
      "loss: 2.064889  [45568/60000]\n",
      "loss: 2.091310  [45632/60000]\n",
      "loss: 2.101307  [45696/60000]\n",
      "loss: 2.103730  [45760/60000]\n",
      "loss: 2.093642  [45824/60000]\n",
      "loss: 2.115543  [45888/60000]\n",
      "loss: 2.093019  [45952/60000]\n",
      "loss: 2.139890  [46016/60000]\n",
      "loss: 2.117773  [46080/60000]\n",
      "loss: 2.078554  [46144/60000]\n",
      "loss: 2.093236  [46208/60000]\n",
      "loss: 2.129975  [46272/60000]\n",
      "loss: 2.087460  [46336/60000]\n",
      "loss: 2.127144  [46400/60000]\n",
      "loss: 2.071998  [46464/60000]\n",
      "loss: 2.090921  [46528/60000]\n",
      "loss: 2.107458  [46592/60000]\n",
      "loss: 2.081955  [46656/60000]\n",
      "loss: 2.100360  [46720/60000]\n",
      "loss: 2.092212  [46784/60000]\n",
      "loss: 2.119155  [46848/60000]\n",
      "loss: 2.078763  [46912/60000]\n",
      "loss: 2.103169  [46976/60000]\n",
      "loss: 2.107714  [47040/60000]\n",
      "loss: 2.105421  [47104/60000]\n",
      "loss: 2.084073  [47168/60000]\n",
      "loss: 2.102252  [47232/60000]\n",
      "loss: 2.078954  [47296/60000]\n",
      "loss: 2.070010  [47360/60000]\n",
      "loss: 2.118731  [47424/60000]\n",
      "loss: 2.091446  [47488/60000]\n",
      "loss: 2.084777  [47552/60000]\n",
      "loss: 2.079206  [47616/60000]\n",
      "loss: 2.058237  [47680/60000]\n",
      "loss: 2.105991  [47744/60000]\n",
      "loss: 2.059855  [47808/60000]\n",
      "loss: 2.114013  [47872/60000]\n",
      "loss: 2.153261  [47936/60000]\n",
      "loss: 2.092672  [48000/60000]\n",
      "loss: 2.111951  [48064/60000]\n",
      "loss: 2.085051  [48128/60000]\n",
      "loss: 2.108742  [48192/60000]\n",
      "loss: 2.114961  [48256/60000]\n",
      "loss: 2.079859  [48320/60000]\n",
      "loss: 2.135902  [48384/60000]\n",
      "loss: 2.110391  [48448/60000]\n",
      "loss: 2.107743  [48512/60000]\n",
      "loss: 2.064929  [48576/60000]\n",
      "loss: 2.103517  [48640/60000]\n",
      "loss: 2.092297  [48704/60000]\n",
      "loss: 2.091923  [48768/60000]\n",
      "loss: 2.065771  [48832/60000]\n",
      "loss: 2.094405  [48896/60000]\n",
      "loss: 2.117559  [48960/60000]\n",
      "loss: 2.130531  [49024/60000]\n",
      "loss: 2.104097  [49088/60000]\n",
      "loss: 2.083754  [49152/60000]\n",
      "loss: 2.071644  [49216/60000]\n",
      "loss: 2.116335  [49280/60000]\n",
      "loss: 2.113774  [49344/60000]\n",
      "loss: 2.094657  [49408/60000]\n",
      "loss: 2.127065  [49472/60000]\n",
      "loss: 2.112103  [49536/60000]\n",
      "loss: 2.099950  [49600/60000]\n",
      "loss: 2.070913  [49664/60000]\n",
      "loss: 2.105722  [49728/60000]\n",
      "loss: 2.055089  [49792/60000]\n",
      "loss: 2.084247  [49856/60000]\n",
      "loss: 2.079154  [49920/60000]\n",
      "loss: 2.127590  [49984/60000]\n",
      "loss: 2.083293  [50048/60000]\n",
      "loss: 2.094588  [50112/60000]\n",
      "loss: 2.051551  [50176/60000]\n",
      "loss: 2.079662  [50240/60000]\n",
      "loss: 2.113394  [50304/60000]\n",
      "loss: 2.097867  [50368/60000]\n",
      "loss: 2.101059  [50432/60000]\n",
      "loss: 2.080486  [50496/60000]\n",
      "loss: 2.085826  [50560/60000]\n",
      "loss: 2.112410  [50624/60000]\n",
      "loss: 2.085051  [50688/60000]\n",
      "loss: 2.096697  [50752/60000]\n",
      "loss: 2.097416  [50816/60000]\n",
      "loss: 2.125501  [50880/60000]\n",
      "loss: 2.088305  [50944/60000]\n",
      "loss: 2.053437  [51008/60000]\n",
      "loss: 2.135752  [51072/60000]\n",
      "loss: 2.064932  [51136/60000]\n",
      "loss: 2.136464  [51200/60000]\n",
      "loss: 2.091127  [51264/60000]\n",
      "loss: 2.131550  [51328/60000]\n",
      "loss: 2.061456  [51392/60000]\n",
      "loss: 2.125253  [51456/60000]\n",
      "loss: 2.055133  [51520/60000]\n",
      "loss: 2.066464  [51584/60000]\n",
      "loss: 2.082402  [51648/60000]\n",
      "loss: 2.097888  [51712/60000]\n",
      "loss: 2.099831  [51776/60000]\n",
      "loss: 2.089115  [51840/60000]\n",
      "loss: 2.041193  [51904/60000]\n",
      "loss: 2.104233  [51968/60000]\n",
      "loss: 2.103976  [52032/60000]\n",
      "loss: 2.098666  [52096/60000]\n",
      "loss: 2.103757  [52160/60000]\n",
      "loss: 2.125715  [52224/60000]\n",
      "loss: 2.065811  [52288/60000]\n",
      "loss: 2.046455  [52352/60000]\n",
      "loss: 2.131152  [52416/60000]\n",
      "loss: 2.099320  [52480/60000]\n",
      "loss: 2.072510  [52544/60000]\n",
      "loss: 2.092397  [52608/60000]\n",
      "loss: 2.083664  [52672/60000]\n",
      "loss: 2.042204  [52736/60000]\n",
      "loss: 2.047984  [52800/60000]\n",
      "loss: 2.078722  [52864/60000]\n",
      "loss: 2.058539  [52928/60000]\n",
      "loss: 2.058275  [52992/60000]\n",
      "loss: 2.087615  [53056/60000]\n",
      "loss: 2.019614  [53120/60000]\n",
      "loss: 2.106225  [53184/60000]\n",
      "loss: 2.058591  [53248/60000]\n",
      "loss: 2.059461  [53312/60000]\n",
      "loss: 2.071858  [53376/60000]\n",
      "loss: 2.048996  [53440/60000]\n",
      "loss: 2.105042  [53504/60000]\n",
      "loss: 2.033693  [53568/60000]\n",
      "loss: 2.105587  [53632/60000]\n",
      "loss: 2.078090  [53696/60000]\n",
      "loss: 2.091431  [53760/60000]\n",
      "loss: 2.071138  [53824/60000]\n",
      "loss: 2.086931  [53888/60000]\n",
      "loss: 2.093936  [53952/60000]\n",
      "loss: 2.095405  [54016/60000]\n",
      "loss: 2.068464  [54080/60000]\n",
      "loss: 2.083220  [54144/60000]\n",
      "loss: 2.079348  [54208/60000]\n",
      "loss: 2.056390  [54272/60000]\n",
      "loss: 2.104188  [54336/60000]\n",
      "loss: 2.091688  [54400/60000]\n",
      "loss: 2.103782  [54464/60000]\n",
      "loss: 2.109512  [54528/60000]\n",
      "loss: 2.067689  [54592/60000]\n",
      "loss: 2.072631  [54656/60000]\n",
      "loss: 2.074803  [54720/60000]\n",
      "loss: 2.093914  [54784/60000]\n",
      "loss: 2.035687  [54848/60000]\n",
      "loss: 2.090154  [54912/60000]\n",
      "loss: 2.075792  [54976/60000]\n",
      "loss: 2.030347  [55040/60000]\n",
      "loss: 2.025548  [55104/60000]\n",
      "loss: 2.065692  [55168/60000]\n",
      "loss: 2.084021  [55232/60000]\n",
      "loss: 2.067050  [55296/60000]\n",
      "loss: 2.099408  [55360/60000]\n",
      "loss: 2.036025  [55424/60000]\n",
      "loss: 2.064918  [55488/60000]\n",
      "loss: 2.083010  [55552/60000]\n",
      "loss: 2.052820  [55616/60000]\n",
      "loss: 2.094445  [55680/60000]\n",
      "loss: 2.058369  [55744/60000]\n",
      "loss: 2.076586  [55808/60000]\n",
      "loss: 2.079512  [55872/60000]\n",
      "loss: 2.082068  [55936/60000]\n",
      "loss: 2.092571  [56000/60000]\n",
      "loss: 2.121053  [56064/60000]\n",
      "loss: 2.068657  [56128/60000]\n",
      "loss: 2.076483  [56192/60000]\n",
      "loss: 2.053743  [56256/60000]\n",
      "loss: 2.036458  [56320/60000]\n",
      "loss: 2.076721  [56384/60000]\n",
      "loss: 2.063934  [56448/60000]\n",
      "loss: 2.093187  [56512/60000]\n",
      "loss: 2.097229  [56576/60000]\n",
      "loss: 2.060273  [56640/60000]\n",
      "loss: 2.067354  [56704/60000]\n",
      "loss: 2.081415  [56768/60000]\n",
      "loss: 2.046847  [56832/60000]\n",
      "loss: 2.031624  [56896/60000]\n",
      "loss: 2.059995  [56960/60000]\n",
      "loss: 2.098396  [57024/60000]\n",
      "loss: 2.093485  [57088/60000]\n",
      "loss: 2.076298  [57152/60000]\n",
      "loss: 2.091890  [57216/60000]\n",
      "loss: 2.088029  [57280/60000]\n",
      "loss: 2.039623  [57344/60000]\n",
      "loss: 2.092754  [57408/60000]\n",
      "loss: 2.109024  [57472/60000]\n",
      "loss: 2.094818  [57536/60000]\n",
      "loss: 2.041618  [57600/60000]\n",
      "loss: 2.053984  [57664/60000]\n",
      "loss: 2.076065  [57728/60000]\n",
      "loss: 2.042193  [57792/60000]\n",
      "loss: 2.062524  [57856/60000]\n",
      "loss: 2.054335  [57920/60000]\n",
      "loss: 2.104591  [57984/60000]\n",
      "loss: 2.040030  [58048/60000]\n",
      "loss: 2.062665  [58112/60000]\n",
      "loss: 2.133082  [58176/60000]\n",
      "loss: 2.032192  [58240/60000]\n",
      "loss: 2.063550  [58304/60000]\n",
      "loss: 2.017301  [58368/60000]\n",
      "loss: 2.100282  [58432/60000]\n",
      "loss: 2.024237  [58496/60000]\n",
      "loss: 2.044719  [58560/60000]\n",
      "loss: 2.069025  [58624/60000]\n",
      "loss: 2.041496  [58688/60000]\n",
      "loss: 2.107485  [58752/60000]\n",
      "loss: 2.071231  [58816/60000]\n",
      "loss: 2.036836  [58880/60000]\n",
      "loss: 2.028976  [58944/60000]\n",
      "loss: 2.094790  [59008/60000]\n",
      "loss: 2.042393  [59072/60000]\n",
      "loss: 2.064118  [59136/60000]\n",
      "loss: 2.085169  [59200/60000]\n",
      "loss: 2.072861  [59264/60000]\n",
      "loss: 2.026826  [59328/60000]\n",
      "loss: 2.088189  [59392/60000]\n",
      "loss: 2.051069  [59456/60000]\n",
      "loss: 2.049839  [59520/60000]\n",
      "loss: 2.029723  [59584/60000]\n",
      "loss: 2.089688  [59648/60000]\n",
      "loss: 2.049039  [59712/60000]\n",
      "loss: 2.082962  [59776/60000]\n",
      "loss: 2.053664  [59840/60000]\n",
      "loss: 2.068130  [59904/60000]\n",
      "loss: 2.016647  [29984/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 2.057811 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.993243  [   64/60000]\n",
      "loss: 2.103847  [  128/60000]\n",
      "loss: 2.068391  [  192/60000]\n",
      "loss: 2.052995  [  256/60000]\n",
      "loss: 2.072990  [  320/60000]\n",
      "loss: 2.057697  [  384/60000]\n",
      "loss: 2.053784  [  448/60000]\n",
      "loss: 2.090780  [  512/60000]\n",
      "loss: 2.047368  [  576/60000]\n",
      "loss: 2.007495  [  640/60000]\n",
      "loss: 2.061691  [  704/60000]\n",
      "loss: 2.043910  [  768/60000]\n",
      "loss: 2.052904  [  832/60000]\n",
      "loss: 2.038933  [  896/60000]\n",
      "loss: 2.007210  [  960/60000]\n",
      "loss: 2.070789  [ 1024/60000]\n",
      "loss: 2.041816  [ 1088/60000]\n",
      "loss: 2.056048  [ 1152/60000]\n",
      "loss: 2.032384  [ 1216/60000]\n",
      "loss: 2.067269  [ 1280/60000]\n",
      "loss: 2.075416  [ 1344/60000]\n",
      "loss: 2.019125  [ 1408/60000]\n",
      "loss: 2.044722  [ 1472/60000]\n",
      "loss: 2.032269  [ 1536/60000]\n",
      "loss: 2.082561  [ 1600/60000]\n",
      "loss: 2.042438  [ 1664/60000]\n",
      "loss: 2.077212  [ 1728/60000]\n",
      "loss: 2.060449  [ 1792/60000]\n",
      "loss: 2.068795  [ 1856/60000]\n",
      "loss: 2.088158  [ 1920/60000]\n",
      "loss: 2.083103  [ 1984/60000]\n",
      "loss: 2.078677  [ 2048/60000]\n",
      "loss: 1.995291  [ 2112/60000]\n",
      "loss: 2.040281  [ 2176/60000]\n",
      "loss: 2.046259  [ 2240/60000]\n",
      "loss: 2.049819  [ 2304/60000]\n",
      "loss: 2.040642  [ 2368/60000]\n",
      "loss: 2.056378  [ 2432/60000]\n",
      "loss: 2.057536  [ 2496/60000]\n",
      "loss: 2.060997  [ 2560/60000]\n",
      "loss: 2.060686  [ 2624/60000]\n",
      "loss: 2.100378  [ 2688/60000]\n",
      "loss: 2.058914  [ 2752/60000]\n",
      "loss: 2.054526  [ 2816/60000]\n",
      "loss: 2.069467  [ 2880/60000]\n",
      "loss: 2.044684  [ 2944/60000]\n",
      "loss: 2.048250  [ 3008/60000]\n",
      "loss: 2.034169  [ 3072/60000]\n",
      "loss: 2.075770  [ 3136/60000]\n",
      "loss: 2.028632  [ 3200/60000]\n",
      "loss: 2.090658  [ 3264/60000]\n",
      "loss: 2.090781  [ 3328/60000]\n",
      "loss: 2.044636  [ 3392/60000]\n",
      "loss: 2.023391  [ 3456/60000]\n",
      "loss: 2.024120  [ 3520/60000]\n",
      "loss: 2.064508  [ 3584/60000]\n",
      "loss: 2.004532  [ 3648/60000]\n",
      "loss: 2.049973  [ 3712/60000]\n",
      "loss: 2.088264  [ 3776/60000]\n",
      "loss: 2.061013  [ 3840/60000]\n",
      "loss: 2.026326  [ 3904/60000]\n",
      "loss: 2.046717  [ 3968/60000]\n",
      "loss: 2.047863  [ 4032/60000]\n",
      "loss: 2.056956  [ 4096/60000]\n",
      "loss: 2.065297  [ 4160/60000]\n",
      "loss: 2.067273  [ 4224/60000]\n",
      "loss: 2.073777  [ 4288/60000]\n",
      "loss: 2.097342  [ 4352/60000]\n",
      "loss: 2.053070  [ 4416/60000]\n",
      "loss: 2.073273  [ 4480/60000]\n",
      "loss: 2.077827  [ 4544/60000]\n",
      "loss: 2.021960  [ 4608/60000]\n",
      "loss: 2.010939  [ 4672/60000]\n",
      "loss: 2.060872  [ 4736/60000]\n",
      "loss: 2.061449  [ 4800/60000]\n",
      "loss: 2.059695  [ 4864/60000]\n",
      "loss: 2.099958  [ 4928/60000]\n",
      "loss: 2.051905  [ 4992/60000]\n",
      "loss: 2.074859  [ 5056/60000]\n",
      "loss: 1.981952  [ 5120/60000]\n",
      "loss: 1.970420  [ 5184/60000]\n",
      "loss: 2.065592  [ 5248/60000]\n",
      "loss: 2.011509  [ 5312/60000]\n",
      "loss: 2.027888  [ 5376/60000]\n",
      "loss: 2.022908  [ 5440/60000]\n",
      "loss: 2.066236  [ 5504/60000]\n",
      "loss: 2.042506  [ 5568/60000]\n",
      "loss: 2.058448  [ 5632/60000]\n",
      "loss: 2.069054  [ 5696/60000]\n",
      "loss: 2.048837  [ 5760/60000]\n",
      "loss: 2.055536  [ 5824/60000]\n",
      "loss: 2.042928  [ 5888/60000]\n",
      "loss: 2.023043  [ 5952/60000]\n",
      "loss: 2.057023  [ 6016/60000]\n",
      "loss: 2.028199  [ 6080/60000]\n",
      "loss: 2.009138  [ 6144/60000]\n",
      "loss: 2.030854  [ 6208/60000]\n",
      "loss: 2.041079  [ 6272/60000]\n",
      "loss: 2.012545  [ 6336/60000]\n",
      "loss: 2.063405  [ 6400/60000]\n",
      "loss: 1.998639  [ 6464/60000]\n",
      "loss: 2.083270  [ 6528/60000]\n",
      "loss: 2.084313  [ 6592/60000]\n",
      "loss: 2.057043  [ 6656/60000]\n",
      "loss: 1.991520  [ 6720/60000]\n",
      "loss: 2.018638  [ 6784/60000]\n",
      "loss: 2.023301  [ 6848/60000]\n",
      "loss: 2.023476  [ 6912/60000]\n",
      "loss: 2.092098  [ 6976/60000]\n",
      "loss: 2.014847  [ 7040/60000]\n",
      "loss: 2.048692  [ 7104/60000]\n",
      "loss: 2.046612  [ 7168/60000]\n",
      "loss: 2.090753  [ 7232/60000]\n",
      "loss: 1.989382  [ 7296/60000]\n",
      "loss: 2.038324  [ 7360/60000]\n",
      "loss: 2.022077  [ 7424/60000]\n",
      "loss: 2.055529  [ 7488/60000]\n",
      "loss: 2.062543  [ 7552/60000]\n",
      "loss: 1.971345  [ 7616/60000]\n",
      "loss: 2.068372  [ 7680/60000]\n",
      "loss: 2.045966  [ 7744/60000]\n",
      "loss: 2.083640  [ 7808/60000]\n",
      "loss: 2.074981  [ 7872/60000]\n",
      "loss: 2.088644  [ 7936/60000]\n",
      "loss: 2.053408  [ 8000/60000]\n",
      "loss: 2.057307  [ 8064/60000]\n",
      "loss: 2.020846  [ 8128/60000]\n",
      "loss: 2.058746  [ 8192/60000]\n",
      "loss: 2.052274  [ 8256/60000]\n",
      "loss: 2.056601  [ 8320/60000]\n",
      "loss: 2.049520  [ 8384/60000]\n",
      "loss: 2.021778  [ 8448/60000]\n",
      "loss: 2.060742  [ 8512/60000]\n",
      "loss: 2.007102  [ 8576/60000]\n",
      "loss: 2.031608  [ 8640/60000]\n",
      "loss: 2.044032  [ 8704/60000]\n",
      "loss: 2.052203  [ 8768/60000]\n",
      "loss: 2.032304  [ 8832/60000]\n",
      "loss: 2.048189  [ 8896/60000]\n",
      "loss: 2.074358  [ 8960/60000]\n",
      "loss: 2.054429  [ 9024/60000]\n",
      "loss: 2.004654  [ 9088/60000]\n",
      "loss: 2.063812  [ 9152/60000]\n",
      "loss: 2.032589  [ 9216/60000]\n",
      "loss: 2.089853  [ 9280/60000]\n",
      "loss: 2.048522  [ 9344/60000]\n",
      "loss: 2.055457  [ 9408/60000]\n",
      "loss: 2.060708  [ 9472/60000]\n",
      "loss: 2.001497  [ 9536/60000]\n",
      "loss: 2.063523  [ 9600/60000]\n",
      "loss: 1.998098  [ 9664/60000]\n",
      "loss: 1.985550  [ 9728/60000]\n",
      "loss: 2.048200  [ 9792/60000]\n",
      "loss: 2.093972  [ 9856/60000]\n",
      "loss: 2.070899  [ 9920/60000]\n",
      "loss: 2.078193  [ 9984/60000]\n",
      "loss: 1.979145  [10048/60000]\n",
      "loss: 2.059537  [10112/60000]\n",
      "loss: 2.013821  [10176/60000]\n",
      "loss: 2.059090  [10240/60000]\n",
      "loss: 2.042935  [10304/60000]\n",
      "loss: 2.022998  [10368/60000]\n",
      "loss: 2.070507  [10432/60000]\n",
      "loss: 2.026473  [10496/60000]\n",
      "loss: 2.031293  [10560/60000]\n",
      "loss: 2.076481  [10624/60000]\n",
      "loss: 2.007472  [10688/60000]\n",
      "loss: 2.031092  [10752/60000]\n",
      "loss: 2.017385  [10816/60000]\n",
      "loss: 2.022466  [10880/60000]\n",
      "loss: 1.967485  [10944/60000]\n",
      "loss: 2.035508  [11008/60000]\n",
      "loss: 2.079436  [11072/60000]\n",
      "loss: 2.065380  [11136/60000]\n",
      "loss: 2.030315  [11200/60000]\n",
      "loss: 2.032378  [11264/60000]\n",
      "loss: 2.065250  [11328/60000]\n",
      "loss: 2.012373  [11392/60000]\n",
      "loss: 1.996420  [11456/60000]\n",
      "loss: 2.013428  [11520/60000]\n",
      "loss: 1.992719  [11584/60000]\n",
      "loss: 2.102841  [11648/60000]\n",
      "loss: 2.009728  [11712/60000]\n",
      "loss: 2.017576  [11776/60000]\n",
      "loss: 2.013777  [11840/60000]\n",
      "loss: 2.003606  [11904/60000]\n",
      "loss: 2.084951  [11968/60000]\n",
      "loss: 1.971365  [12032/60000]\n",
      "loss: 2.044766  [12096/60000]\n",
      "loss: 2.087776  [12160/60000]\n",
      "loss: 2.005444  [12224/60000]\n",
      "loss: 2.000080  [12288/60000]\n",
      "loss: 2.013182  [12352/60000]\n",
      "loss: 2.075728  [12416/60000]\n",
      "loss: 2.040085  [12480/60000]\n",
      "loss: 2.049001  [12544/60000]\n",
      "loss: 2.044014  [12608/60000]\n",
      "loss: 2.046044  [12672/60000]\n",
      "loss: 2.021669  [12736/60000]\n",
      "loss: 2.052726  [12800/60000]\n",
      "loss: 2.087569  [12864/60000]\n",
      "loss: 2.078881  [12928/60000]\n",
      "loss: 2.013043  [12992/60000]\n",
      "loss: 2.056332  [13056/60000]\n",
      "loss: 2.040756  [13120/60000]\n",
      "loss: 2.054032  [13184/60000]\n",
      "loss: 2.035562  [13248/60000]\n",
      "loss: 2.062725  [13312/60000]\n",
      "loss: 2.037436  [13376/60000]\n",
      "loss: 2.030922  [13440/60000]\n",
      "loss: 2.008595  [13504/60000]\n",
      "loss: 2.031878  [13568/60000]\n",
      "loss: 2.056955  [13632/60000]\n",
      "loss: 2.041106  [13696/60000]\n",
      "loss: 2.000682  [13760/60000]\n",
      "loss: 1.991122  [13824/60000]\n",
      "loss: 2.041818  [13888/60000]\n",
      "loss: 2.030410  [13952/60000]\n",
      "loss: 2.036662  [14016/60000]\n",
      "loss: 2.034997  [14080/60000]\n",
      "loss: 2.033588  [14144/60000]\n",
      "loss: 2.020231  [14208/60000]\n",
      "loss: 2.056361  [14272/60000]\n",
      "loss: 2.010888  [14336/60000]\n",
      "loss: 2.025406  [14400/60000]\n",
      "loss: 2.030518  [14464/60000]\n",
      "loss: 1.995956  [14528/60000]\n",
      "loss: 2.066609  [14592/60000]\n",
      "loss: 2.027976  [14656/60000]\n",
      "loss: 2.022153  [14720/60000]\n",
      "loss: 1.973752  [14784/60000]\n",
      "loss: 2.080700  [14848/60000]\n",
      "loss: 1.988133  [14912/60000]\n",
      "loss: 2.012655  [14976/60000]\n",
      "loss: 2.037030  [15040/60000]\n",
      "loss: 2.022422  [15104/60000]\n",
      "loss: 1.991122  [15168/60000]\n",
      "loss: 2.072409  [15232/60000]\n",
      "loss: 1.987262  [15296/60000]\n",
      "loss: 1.951817  [15360/60000]\n",
      "loss: 2.043830  [15424/60000]\n",
      "loss: 2.021996  [15488/60000]\n",
      "loss: 2.037656  [15552/60000]\n",
      "loss: 2.044761  [15616/60000]\n",
      "loss: 1.983828  [15680/60000]\n",
      "loss: 2.057327  [15744/60000]\n",
      "loss: 1.998150  [15808/60000]\n",
      "loss: 2.019252  [15872/60000]\n",
      "loss: 2.026191  [15936/60000]\n",
      "loss: 2.030921  [16000/60000]\n",
      "loss: 2.033425  [16064/60000]\n",
      "loss: 2.013649  [16128/60000]\n",
      "loss: 1.990864  [16192/60000]\n",
      "loss: 2.002203  [16256/60000]\n",
      "loss: 2.041972  [16320/60000]\n",
      "loss: 2.002921  [16384/60000]\n",
      "loss: 1.997760  [16448/60000]\n",
      "loss: 2.042104  [16512/60000]\n",
      "loss: 2.046698  [16576/60000]\n",
      "loss: 1.994670  [16640/60000]\n",
      "loss: 2.020902  [16704/60000]\n",
      "loss: 2.014070  [16768/60000]\n",
      "loss: 2.018554  [16832/60000]\n",
      "loss: 2.026174  [16896/60000]\n",
      "loss: 2.022694  [16960/60000]\n",
      "loss: 2.014717  [17024/60000]\n",
      "loss: 1.985832  [17088/60000]\n",
      "loss: 1.995450  [17152/60000]\n",
      "loss: 2.001337  [17216/60000]\n",
      "loss: 2.031108  [17280/60000]\n",
      "loss: 2.046664  [17344/60000]\n",
      "loss: 2.014105  [17408/60000]\n",
      "loss: 1.991332  [17472/60000]\n",
      "loss: 2.004105  [17536/60000]\n",
      "loss: 2.019991  [17600/60000]\n",
      "loss: 2.035648  [17664/60000]\n",
      "loss: 2.030285  [17728/60000]\n",
      "loss: 1.980911  [17792/60000]\n",
      "loss: 2.008866  [17856/60000]\n",
      "loss: 2.031300  [17920/60000]\n",
      "loss: 1.996879  [17984/60000]\n",
      "loss: 2.007475  [18048/60000]\n",
      "loss: 1.980772  [18112/60000]\n",
      "loss: 2.003832  [18176/60000]\n",
      "loss: 1.999074  [18240/60000]\n",
      "loss: 2.014848  [18304/60000]\n",
      "loss: 2.021701  [18368/60000]\n",
      "loss: 2.029209  [18432/60000]\n",
      "loss: 2.007080  [18496/60000]\n",
      "loss: 1.981015  [18560/60000]\n",
      "loss: 2.021944  [18624/60000]\n",
      "loss: 1.978992  [18688/60000]\n",
      "loss: 2.032279  [18752/60000]\n",
      "loss: 2.015423  [18816/60000]\n",
      "loss: 1.989158  [18880/60000]\n",
      "loss: 2.020995  [18944/60000]\n",
      "loss: 1.928611  [19008/60000]\n",
      "loss: 1.985979  [19072/60000]\n",
      "loss: 2.037804  [19136/60000]\n",
      "loss: 2.046739  [19200/60000]\n",
      "loss: 2.007245  [19264/60000]\n",
      "loss: 1.970489  [19328/60000]\n",
      "loss: 2.004260  [19392/60000]\n",
      "loss: 2.021895  [19456/60000]\n",
      "loss: 2.049666  [19520/60000]\n",
      "loss: 2.005800  [19584/60000]\n",
      "loss: 2.063135  [19648/60000]\n",
      "loss: 2.072658  [19712/60000]\n",
      "loss: 2.007744  [19776/60000]\n",
      "loss: 2.011303  [19840/60000]\n",
      "loss: 2.041286  [19904/60000]\n",
      "loss: 1.949539  [19968/60000]\n",
      "loss: 2.070232  [20032/60000]\n",
      "loss: 2.045369  [20096/60000]\n",
      "loss: 2.015784  [20160/60000]\n",
      "loss: 2.038046  [20224/60000]\n",
      "loss: 2.022933  [20288/60000]\n",
      "loss: 2.043942  [20352/60000]\n",
      "loss: 2.017773  [20416/60000]\n",
      "loss: 2.031109  [20480/60000]\n",
      "loss: 2.051481  [20544/60000]\n",
      "loss: 2.001388  [20608/60000]\n",
      "loss: 2.021091  [20672/60000]\n",
      "loss: 2.016535  [20736/60000]\n",
      "loss: 2.038976  [20800/60000]\n",
      "loss: 2.028367  [20864/60000]\n",
      "loss: 1.986202  [20928/60000]\n",
      "loss: 2.012957  [20992/60000]\n",
      "loss: 1.983383  [21056/60000]\n",
      "loss: 1.995492  [21120/60000]\n",
      "loss: 2.000558  [21184/60000]\n",
      "loss: 1.956249  [21248/60000]\n",
      "loss: 1.970357  [21312/60000]\n",
      "loss: 2.031094  [21376/60000]\n",
      "loss: 1.976231  [21440/60000]\n",
      "loss: 2.011522  [21504/60000]\n",
      "loss: 1.981065  [21568/60000]\n",
      "loss: 2.057045  [21632/60000]\n",
      "loss: 1.956601  [21696/60000]\n",
      "loss: 1.978928  [21760/60000]\n",
      "loss: 1.994806  [21824/60000]\n",
      "loss: 2.001363  [21888/60000]\n",
      "loss: 1.987407  [21952/60000]\n",
      "loss: 2.002626  [22016/60000]\n",
      "loss: 2.035273  [22080/60000]\n",
      "loss: 1.995366  [22144/60000]\n",
      "loss: 1.999313  [22208/60000]\n",
      "loss: 1.983864  [22272/60000]\n",
      "loss: 1.992581  [22336/60000]\n",
      "loss: 2.040255  [22400/60000]\n",
      "loss: 1.965819  [22464/60000]\n",
      "loss: 1.949916  [22528/60000]\n",
      "loss: 2.030473  [22592/60000]\n",
      "loss: 2.041862  [22656/60000]\n",
      "loss: 1.970021  [22720/60000]\n",
      "loss: 1.995429  [22784/60000]\n",
      "loss: 2.030723  [22848/60000]\n",
      "loss: 2.043082  [22912/60000]\n",
      "loss: 1.935677  [22976/60000]\n",
      "loss: 2.008726  [23040/60000]\n",
      "loss: 2.016671  [23104/60000]\n",
      "loss: 2.020647  [23168/60000]\n",
      "loss: 2.003925  [23232/60000]\n",
      "loss: 1.981007  [23296/60000]\n",
      "loss: 1.995317  [23360/60000]\n",
      "loss: 1.978835  [23424/60000]\n",
      "loss: 1.987864  [23488/60000]\n",
      "loss: 1.958199  [23552/60000]\n",
      "loss: 2.022629  [23616/60000]\n",
      "loss: 1.992089  [23680/60000]\n",
      "loss: 2.006895  [23744/60000]\n",
      "loss: 1.959818  [23808/60000]\n",
      "loss: 1.970871  [23872/60000]\n",
      "loss: 1.992377  [23936/60000]\n",
      "loss: 1.996210  [24000/60000]\n",
      "loss: 1.994963  [24064/60000]\n",
      "loss: 2.064175  [24128/60000]\n",
      "loss: 2.020501  [24192/60000]\n",
      "loss: 1.968794  [24256/60000]\n",
      "loss: 1.980024  [24320/60000]\n",
      "loss: 1.939109  [24384/60000]\n",
      "loss: 1.984047  [24448/60000]\n",
      "loss: 2.013857  [24512/60000]\n",
      "loss: 1.940549  [24576/60000]\n",
      "loss: 2.011272  [24640/60000]\n",
      "loss: 2.008190  [24704/60000]\n",
      "loss: 1.998305  [24768/60000]\n",
      "loss: 1.981109  [24832/60000]\n",
      "loss: 1.977602  [24896/60000]\n",
      "loss: 1.991658  [24960/60000]\n",
      "loss: 1.938661  [25024/60000]\n",
      "loss: 2.036858  [25088/60000]\n",
      "loss: 1.962159  [25152/60000]\n",
      "loss: 1.978992  [25216/60000]\n",
      "loss: 2.005742  [25280/60000]\n",
      "loss: 1.998415  [25344/60000]\n",
      "loss: 2.011837  [25408/60000]\n",
      "loss: 1.942907  [25472/60000]\n",
      "loss: 1.987085  [25536/60000]\n",
      "loss: 2.017384  [25600/60000]\n",
      "loss: 1.956163  [25664/60000]\n",
      "loss: 1.973660  [25728/60000]\n",
      "loss: 1.985461  [25792/60000]\n",
      "loss: 2.001482  [25856/60000]\n",
      "loss: 1.938970  [25920/60000]\n",
      "loss: 1.949602  [25984/60000]\n",
      "loss: 1.964501  [26048/60000]\n",
      "loss: 2.029877  [26112/60000]\n",
      "loss: 1.954568  [26176/60000]\n",
      "loss: 2.032844  [26240/60000]\n",
      "loss: 1.950544  [26304/60000]\n",
      "loss: 2.063664  [26368/60000]\n",
      "loss: 1.966816  [26432/60000]\n",
      "loss: 1.971410  [26496/60000]\n",
      "loss: 1.965345  [26560/60000]\n",
      "loss: 1.970771  [26624/60000]\n",
      "loss: 1.968333  [26688/60000]\n",
      "loss: 1.952239  [26752/60000]\n",
      "loss: 1.932879  [26816/60000]\n",
      "loss: 1.998600  [26880/60000]\n",
      "loss: 1.984123  [26944/60000]\n",
      "loss: 2.029425  [27008/60000]\n",
      "loss: 1.988311  [27072/60000]\n",
      "loss: 1.975522  [27136/60000]\n",
      "loss: 1.995213  [27200/60000]\n",
      "loss: 2.002865  [27264/60000]\n",
      "loss: 1.997137  [27328/60000]\n",
      "loss: 1.942510  [27392/60000]\n",
      "loss: 1.958182  [27456/60000]\n",
      "loss: 2.005947  [27520/60000]\n",
      "loss: 1.936940  [27584/60000]\n",
      "loss: 1.946356  [27648/60000]\n",
      "loss: 2.041452  [27712/60000]\n",
      "loss: 1.878478  [27776/60000]\n",
      "loss: 1.943826  [27840/60000]\n",
      "loss: 1.956565  [27904/60000]\n",
      "loss: 1.992499  [27968/60000]\n",
      "loss: 2.014310  [28032/60000]\n",
      "loss: 2.017121  [28096/60000]\n",
      "loss: 2.010260  [28160/60000]\n",
      "loss: 2.024794  [28224/60000]\n",
      "loss: 2.022761  [28288/60000]\n",
      "loss: 1.966179  [28352/60000]\n",
      "loss: 1.995911  [28416/60000]\n",
      "loss: 1.921317  [28480/60000]\n",
      "loss: 2.011643  [28544/60000]\n",
      "loss: 1.940676  [28608/60000]\n",
      "loss: 1.932434  [28672/60000]\n",
      "loss: 1.960720  [28736/60000]\n",
      "loss: 2.012288  [28800/60000]\n",
      "loss: 1.988537  [28864/60000]\n",
      "loss: 1.954199  [28928/60000]\n",
      "loss: 1.962798  [28992/60000]\n",
      "loss: 1.982426  [29056/60000]\n",
      "loss: 1.979809  [29120/60000]\n",
      "loss: 1.947607  [29184/60000]\n",
      "loss: 1.833140  [29248/60000]\n",
      "loss: 2.015398  [29312/60000]\n",
      "loss: 2.025397  [29376/60000]\n",
      "loss: 1.995720  [29440/60000]\n",
      "loss: 1.979283  [29504/60000]\n",
      "loss: 1.985567  [29568/60000]\n",
      "loss: 1.922109  [29632/60000]\n",
      "loss: 1.954937  [29696/60000]\n",
      "loss: 1.977372  [29760/60000]\n",
      "loss: 1.929153  [29824/60000]\n",
      "loss: 1.932235  [29888/60000]\n",
      "loss: 2.018414  [29952/60000]\n",
      "loss: 1.943748  [30016/60000]\n",
      "loss: 1.960214  [30080/60000]\n",
      "loss: 1.994417  [30144/60000]\n",
      "loss: 2.002317  [30208/60000]\n",
      "loss: 1.973667  [30272/60000]\n",
      "loss: 1.965629  [30336/60000]\n",
      "loss: 1.937144  [30400/60000]\n",
      "loss: 1.902621  [30464/60000]\n",
      "loss: 1.924118  [30528/60000]\n",
      "loss: 1.998477  [30592/60000]\n",
      "loss: 1.980891  [30656/60000]\n",
      "loss: 1.936649  [30720/60000]\n",
      "loss: 1.997557  [30784/60000]\n",
      "loss: 1.978065  [30848/60000]\n",
      "loss: 2.006186  [30912/60000]\n",
      "loss: 1.950174  [30976/60000]\n",
      "loss: 1.978118  [31040/60000]\n",
      "loss: 2.028603  [31104/60000]\n",
      "loss: 1.927936  [31168/60000]\n",
      "loss: 1.927066  [31232/60000]\n",
      "loss: 1.969737  [31296/60000]\n",
      "loss: 1.923424  [31360/60000]\n",
      "loss: 1.933500  [31424/60000]\n",
      "loss: 2.021272  [31488/60000]\n",
      "loss: 1.975569  [31552/60000]\n",
      "loss: 2.000299  [31616/60000]\n",
      "loss: 1.995687  [31680/60000]\n",
      "loss: 1.992127  [31744/60000]\n",
      "loss: 1.969977  [31808/60000]\n",
      "loss: 1.971766  [31872/60000]\n",
      "loss: 1.971560  [31936/60000]\n",
      "loss: 1.922760  [32000/60000]\n",
      "loss: 1.926494  [32064/60000]\n",
      "loss: 1.963970  [32128/60000]\n",
      "loss: 1.906059  [32192/60000]\n",
      "loss: 1.955609  [32256/60000]\n",
      "loss: 1.949438  [32320/60000]\n",
      "loss: 2.012016  [32384/60000]\n",
      "loss: 1.947894  [32448/60000]\n",
      "loss: 1.959072  [32512/60000]\n",
      "loss: 1.982071  [32576/60000]\n",
      "loss: 2.016658  [32640/60000]\n",
      "loss: 1.912326  [32704/60000]\n",
      "loss: 1.963994  [32768/60000]\n",
      "loss: 1.978575  [32832/60000]\n",
      "loss: 1.956954  [32896/60000]\n",
      "loss: 2.004243  [32960/60000]\n",
      "loss: 1.930499  [33024/60000]\n",
      "loss: 1.925696  [33088/60000]\n",
      "loss: 1.942943  [33152/60000]\n",
      "loss: 1.917205  [33216/60000]\n",
      "loss: 1.997045  [33280/60000]\n",
      "loss: 1.924961  [33344/60000]\n",
      "loss: 1.942315  [33408/60000]\n",
      "loss: 2.005387  [33472/60000]\n",
      "loss: 1.970472  [33536/60000]\n",
      "loss: 1.977028  [33600/60000]\n",
      "loss: 1.931606  [33664/60000]\n",
      "loss: 1.916815  [33728/60000]\n",
      "loss: 1.979644  [33792/60000]\n",
      "loss: 1.939959  [33856/60000]\n",
      "loss: 1.956997  [33920/60000]\n",
      "loss: 1.939891  [33984/60000]\n",
      "loss: 1.963278  [34048/60000]\n",
      "loss: 1.955224  [34112/60000]\n",
      "loss: 1.961296  [34176/60000]\n",
      "loss: 1.983746  [34240/60000]\n",
      "loss: 2.013856  [34304/60000]\n",
      "loss: 2.021255  [34368/60000]\n",
      "loss: 1.926450  [34432/60000]\n",
      "loss: 1.948872  [34496/60000]\n",
      "loss: 1.928177  [34560/60000]\n",
      "loss: 1.926967  [34624/60000]\n",
      "loss: 1.964682  [34688/60000]\n",
      "loss: 1.991702  [34752/60000]\n",
      "loss: 1.964163  [34816/60000]\n",
      "loss: 1.925194  [34880/60000]\n",
      "loss: 1.917085  [34944/60000]\n",
      "loss: 1.941222  [35008/60000]\n",
      "loss: 1.950467  [35072/60000]\n",
      "loss: 1.902004  [35136/60000]\n",
      "loss: 1.955537  [35200/60000]\n",
      "loss: 1.932721  [35264/60000]\n",
      "loss: 1.934892  [35328/60000]\n",
      "loss: 1.940315  [35392/60000]\n",
      "loss: 1.974252  [35456/60000]\n",
      "loss: 1.953831  [35520/60000]\n",
      "loss: 1.997638  [35584/60000]\n",
      "loss: 1.958578  [35648/60000]\n",
      "loss: 1.957626  [35712/60000]\n",
      "loss: 1.951153  [35776/60000]\n",
      "loss: 1.986979  [35840/60000]\n",
      "loss: 1.973950  [35904/60000]\n",
      "loss: 1.975892  [35968/60000]\n",
      "loss: 2.009914  [36032/60000]\n",
      "loss: 2.023451  [36096/60000]\n",
      "loss: 1.913073  [36160/60000]\n",
      "loss: 1.930157  [36224/60000]\n",
      "loss: 1.976838  [36288/60000]\n",
      "loss: 1.974095  [36352/60000]\n",
      "loss: 1.922180  [36416/60000]\n",
      "loss: 1.995214  [36480/60000]\n",
      "loss: 1.928309  [36544/60000]\n",
      "loss: 1.973831  [36608/60000]\n",
      "loss: 1.994460  [36672/60000]\n",
      "loss: 1.882303  [36736/60000]\n",
      "loss: 1.949142  [36800/60000]\n",
      "loss: 1.952839  [36864/60000]\n",
      "loss: 1.948673  [36928/60000]\n",
      "loss: 1.952296  [36992/60000]\n",
      "loss: 1.936587  [37056/60000]\n",
      "loss: 1.928706  [37120/60000]\n",
      "loss: 1.955829  [37184/60000]\n",
      "loss: 1.980269  [37248/60000]\n",
      "loss: 1.948247  [37312/60000]\n",
      "loss: 1.972665  [37376/60000]\n",
      "loss: 1.963503  [37440/60000]\n",
      "loss: 1.998730  [37504/60000]\n",
      "loss: 1.950714  [37568/60000]\n",
      "loss: 1.987547  [37632/60000]\n",
      "loss: 1.968004  [37696/60000]\n",
      "loss: 1.903680  [37760/60000]\n",
      "loss: 1.936487  [37824/60000]\n",
      "loss: 1.944870  [37888/60000]\n",
      "loss: 1.942880  [37952/60000]\n",
      "loss: 2.001146  [38016/60000]\n",
      "loss: 2.004258  [38080/60000]\n",
      "loss: 1.975268  [38144/60000]\n",
      "loss: 1.942104  [38208/60000]\n",
      "loss: 1.932062  [38272/60000]\n",
      "loss: 1.956584  [38336/60000]\n",
      "loss: 1.961733  [38400/60000]\n",
      "loss: 1.926737  [38464/60000]\n",
      "loss: 1.962108  [38528/60000]\n",
      "loss: 2.002648  [38592/60000]\n",
      "loss: 1.957631  [38656/60000]\n",
      "loss: 1.982781  [38720/60000]\n",
      "loss: 1.917473  [38784/60000]\n",
      "loss: 1.836348  [38848/60000]\n",
      "loss: 1.973047  [38912/60000]\n",
      "loss: 1.974009  [38976/60000]\n",
      "loss: 1.996552  [39040/60000]\n",
      "loss: 1.926658  [39104/60000]\n",
      "loss: 1.932408  [39168/60000]\n",
      "loss: 1.918931  [39232/60000]\n",
      "loss: 1.873010  [39296/60000]\n",
      "loss: 1.939625  [39360/60000]\n",
      "loss: 1.982901  [39424/60000]\n",
      "loss: 1.914559  [39488/60000]\n",
      "loss: 1.988333  [39552/60000]\n",
      "loss: 1.973437  [39616/60000]\n",
      "loss: 2.021752  [39680/60000]\n",
      "loss: 1.911927  [39744/60000]\n",
      "loss: 1.914265  [39808/60000]\n",
      "loss: 1.977990  [39872/60000]\n",
      "loss: 1.945700  [39936/60000]\n",
      "loss: 1.956583  [40000/60000]\n",
      "loss: 1.920753  [40064/60000]\n",
      "loss: 1.923534  [40128/60000]\n",
      "loss: 1.940478  [40192/60000]\n",
      "loss: 1.982898  [40256/60000]\n",
      "loss: 1.896939  [40320/60000]\n",
      "loss: 1.914357  [40384/60000]\n",
      "loss: 1.913772  [40448/60000]\n",
      "loss: 1.947033  [40512/60000]\n",
      "loss: 1.908379  [40576/60000]\n",
      "loss: 1.891886  [40640/60000]\n",
      "loss: 1.883629  [40704/60000]\n",
      "loss: 1.931145  [40768/60000]\n",
      "loss: 1.877480  [40832/60000]\n",
      "loss: 1.909104  [40896/60000]\n",
      "loss: 1.952098  [40960/60000]\n",
      "loss: 1.942497  [41024/60000]\n",
      "loss: 1.937019  [41088/60000]\n",
      "loss: 1.935907  [41152/60000]\n",
      "loss: 1.924601  [41216/60000]\n",
      "loss: 1.918851  [41280/60000]\n",
      "loss: 1.996566  [41344/60000]\n",
      "loss: 1.954419  [41408/60000]\n",
      "loss: 1.912380  [41472/60000]\n",
      "loss: 1.833521  [41536/60000]\n",
      "loss: 1.974446  [41600/60000]\n",
      "loss: 1.879305  [41664/60000]\n",
      "loss: 1.958445  [41728/60000]\n",
      "loss: 1.957595  [41792/60000]\n",
      "loss: 1.923925  [41856/60000]\n",
      "loss: 1.970074  [41920/60000]\n",
      "loss: 1.855677  [41984/60000]\n",
      "loss: 1.954207  [42048/60000]\n",
      "loss: 1.984534  [42112/60000]\n",
      "loss: 1.926032  [42176/60000]\n",
      "loss: 1.955589  [42240/60000]\n",
      "loss: 1.962693  [42304/60000]\n",
      "loss: 1.972543  [42368/60000]\n",
      "loss: 1.973354  [42432/60000]\n",
      "loss: 1.910766  [42496/60000]\n",
      "loss: 1.984957  [42560/60000]\n",
      "loss: 1.933541  [42624/60000]\n",
      "loss: 1.901772  [42688/60000]\n",
      "loss: 1.959930  [42752/60000]\n",
      "loss: 1.883387  [42816/60000]\n",
      "loss: 1.944403  [42880/60000]\n",
      "loss: 1.861986  [42944/60000]\n",
      "loss: 1.943197  [43008/60000]\n",
      "loss: 1.890976  [43072/60000]\n",
      "loss: 1.915719  [43136/60000]\n",
      "loss: 1.913121  [43200/60000]\n",
      "loss: 1.846354  [43264/60000]\n",
      "loss: 1.925173  [43328/60000]\n",
      "loss: 1.999845  [43392/60000]\n",
      "loss: 1.947826  [43456/60000]\n",
      "loss: 1.924166  [43520/60000]\n",
      "loss: 1.883919  [43584/60000]\n",
      "loss: 1.896282  [43648/60000]\n",
      "loss: 1.960336  [43712/60000]\n",
      "loss: 1.894053  [43776/60000]\n",
      "loss: 1.920713  [43840/60000]\n",
      "loss: 1.956017  [43904/60000]\n",
      "loss: 1.968340  [43968/60000]\n",
      "loss: 1.896277  [44032/60000]\n",
      "loss: 1.910050  [44096/60000]\n",
      "loss: 1.923905  [44160/60000]\n",
      "loss: 1.877277  [44224/60000]\n",
      "loss: 1.891415  [44288/60000]\n",
      "loss: 1.914491  [44352/60000]\n",
      "loss: 1.956841  [44416/60000]\n",
      "loss: 1.932800  [44480/60000]\n",
      "loss: 1.900573  [44544/60000]\n",
      "loss: 2.009817  [44608/60000]\n",
      "loss: 1.891288  [44672/60000]\n",
      "loss: 1.903256  [44736/60000]\n",
      "loss: 1.849231  [44800/60000]\n",
      "loss: 1.894773  [44864/60000]\n",
      "loss: 2.000412  [44928/60000]\n",
      "loss: 1.925616  [44992/60000]\n",
      "loss: 1.927275  [45056/60000]\n",
      "loss: 1.980377  [45120/60000]\n",
      "loss: 1.897212  [45184/60000]\n",
      "loss: 1.926147  [45248/60000]\n",
      "loss: 1.923930  [45312/60000]\n",
      "loss: 1.939611  [45376/60000]\n",
      "loss: 1.900737  [45440/60000]\n",
      "loss: 1.945410  [45504/60000]\n",
      "loss: 1.899595  [45568/60000]\n",
      "loss: 1.922015  [45632/60000]\n",
      "loss: 1.887788  [45696/60000]\n",
      "loss: 1.964645  [45760/60000]\n",
      "loss: 1.925140  [45824/60000]\n",
      "loss: 1.882133  [45888/60000]\n",
      "loss: 1.937036  [45952/60000]\n",
      "loss: 1.941515  [46016/60000]\n",
      "loss: 1.846702  [46080/60000]\n",
      "loss: 1.883102  [46144/60000]\n",
      "loss: 1.938618  [46208/60000]\n",
      "loss: 1.933702  [46272/60000]\n",
      "loss: 1.819853  [46336/60000]\n",
      "loss: 1.871532  [46400/60000]\n",
      "loss: 1.941080  [46464/60000]\n",
      "loss: 1.865118  [46528/60000]\n",
      "loss: 1.938148  [46592/60000]\n",
      "loss: 1.934176  [46656/60000]\n",
      "loss: 1.888784  [46720/60000]\n",
      "loss: 1.901350  [46784/60000]\n",
      "loss: 1.966990  [46848/60000]\n",
      "loss: 1.901012  [46912/60000]\n",
      "loss: 1.918433  [46976/60000]\n",
      "loss: 1.964968  [47040/60000]\n",
      "loss: 1.931913  [47104/60000]\n",
      "loss: 1.891149  [47168/60000]\n",
      "loss: 1.888284  [47232/60000]\n",
      "loss: 1.900853  [47296/60000]\n",
      "loss: 1.960208  [47360/60000]\n",
      "loss: 1.906015  [47424/60000]\n",
      "loss: 1.905856  [47488/60000]\n",
      "loss: 1.870635  [47552/60000]\n",
      "loss: 1.925081  [47616/60000]\n",
      "loss: 1.895027  [47680/60000]\n",
      "loss: 1.939427  [47744/60000]\n",
      "loss: 1.791545  [47808/60000]\n",
      "loss: 1.912557  [47872/60000]\n",
      "loss: 1.916450  [47936/60000]\n",
      "loss: 1.939537  [48000/60000]\n",
      "loss: 1.889159  [48064/60000]\n",
      "loss: 1.906698  [48128/60000]\n",
      "loss: 1.921991  [48192/60000]\n",
      "loss: 1.995244  [48256/60000]\n",
      "loss: 1.952018  [48320/60000]\n",
      "loss: 1.899766  [48384/60000]\n",
      "loss: 1.945213  [48448/60000]\n",
      "loss: 1.912970  [48512/60000]\n",
      "loss: 1.931322  [48576/60000]\n",
      "loss: 1.951559  [48640/60000]\n",
      "loss: 1.875428  [48704/60000]\n",
      "loss: 1.904762  [48768/60000]\n",
      "loss: 1.913300  [48832/60000]\n",
      "loss: 1.940400  [48896/60000]\n",
      "loss: 1.851194  [48960/60000]\n",
      "loss: 1.900075  [49024/60000]\n",
      "loss: 1.918941  [49088/60000]\n",
      "loss: 1.908233  [49152/60000]\n",
      "loss: 1.943146  [49216/60000]\n",
      "loss: 1.895764  [49280/60000]\n",
      "loss: 1.895874  [49344/60000]\n",
      "loss: 1.894588  [49408/60000]\n",
      "loss: 1.917368  [49472/60000]\n",
      "loss: 1.973602  [49536/60000]\n",
      "loss: 1.868076  [49600/60000]\n",
      "loss: 1.938488  [49664/60000]\n",
      "loss: 1.911008  [49728/60000]\n",
      "loss: 1.925887  [49792/60000]\n",
      "loss: 1.920204  [49856/60000]\n",
      "loss: 1.965461  [49920/60000]\n",
      "loss: 1.940550  [49984/60000]\n",
      "loss: 1.906152  [50048/60000]\n",
      "loss: 1.924514  [50112/60000]\n",
      "loss: 1.912016  [50176/60000]\n",
      "loss: 1.870745  [50240/60000]\n",
      "loss: 1.917711  [50304/60000]\n",
      "loss: 1.824777  [50368/60000]\n",
      "loss: 1.941861  [50432/60000]\n",
      "loss: 1.873855  [50496/60000]\n",
      "loss: 1.905808  [50560/60000]\n",
      "loss: 1.944356  [50624/60000]\n",
      "loss: 1.885050  [50688/60000]\n",
      "loss: 1.892017  [50752/60000]\n",
      "loss: 1.908231  [50816/60000]\n",
      "loss: 1.917029  [50880/60000]\n",
      "loss: 1.916815  [50944/60000]\n",
      "loss: 1.876884  [51008/60000]\n",
      "loss: 1.915890  [51072/60000]\n",
      "loss: 1.882584  [51136/60000]\n",
      "loss: 1.853870  [51200/60000]\n",
      "loss: 1.927637  [51264/60000]\n",
      "loss: 1.883382  [51328/60000]\n",
      "loss: 1.872288  [51392/60000]\n",
      "loss: 1.923606  [51456/60000]\n",
      "loss: 1.886287  [51520/60000]\n",
      "loss: 1.918357  [51584/60000]\n",
      "loss: 1.987153  [51648/60000]\n",
      "loss: 1.933336  [51712/60000]\n",
      "loss: 1.859445  [51776/60000]\n",
      "loss: 1.814087  [51840/60000]\n",
      "loss: 1.885601  [51904/60000]\n",
      "loss: 1.880164  [51968/60000]\n",
      "loss: 1.846335  [52032/60000]\n",
      "loss: 1.863696  [52096/60000]\n",
      "loss: 1.894990  [52160/60000]\n",
      "loss: 1.851598  [52224/60000]\n",
      "loss: 1.810920  [52288/60000]\n",
      "loss: 1.900469  [52352/60000]\n",
      "loss: 1.928587  [52416/60000]\n",
      "loss: 1.887806  [52480/60000]\n",
      "loss: 1.925225  [52544/60000]\n",
      "loss: 1.827381  [52608/60000]\n",
      "loss: 1.921667  [52672/60000]\n",
      "loss: 1.855548  [52736/60000]\n",
      "loss: 1.921146  [52800/60000]\n",
      "loss: 1.923138  [52864/60000]\n",
      "loss: 1.891456  [52928/60000]\n",
      "loss: 1.849140  [52992/60000]\n",
      "loss: 1.910548  [53056/60000]\n",
      "loss: 1.908376  [53120/60000]\n",
      "loss: 1.905580  [53184/60000]\n",
      "loss: 1.863272  [53248/60000]\n",
      "loss: 1.932435  [53312/60000]\n",
      "loss: 1.860496  [53376/60000]\n",
      "loss: 1.868856  [53440/60000]\n",
      "loss: 1.859735  [53504/60000]\n",
      "loss: 1.848178  [53568/60000]\n",
      "loss: 1.877595  [53632/60000]\n",
      "loss: 1.830953  [53696/60000]\n",
      "loss: 1.908860  [53760/60000]\n",
      "loss: 1.860880  [53824/60000]\n",
      "loss: 1.865125  [53888/60000]\n",
      "loss: 1.946627  [53952/60000]\n",
      "loss: 1.897021  [54016/60000]\n",
      "loss: 1.870881  [54080/60000]\n",
      "loss: 1.870497  [54144/60000]\n",
      "loss: 1.857347  [54208/60000]\n",
      "loss: 1.909153  [54272/60000]\n",
      "loss: 1.908472  [54336/60000]\n",
      "loss: 1.837645  [54400/60000]\n",
      "loss: 1.822046  [54464/60000]\n",
      "loss: 1.908785  [54528/60000]\n",
      "loss: 1.870685  [54592/60000]\n",
      "loss: 1.865715  [54656/60000]\n",
      "loss: 1.856096  [54720/60000]\n",
      "loss: 1.849487  [54784/60000]\n",
      "loss: 1.854980  [54848/60000]\n",
      "loss: 1.930008  [54912/60000]\n",
      "loss: 1.831296  [54976/60000]\n",
      "loss: 1.900223  [55040/60000]\n",
      "loss: 1.884974  [55104/60000]\n",
      "loss: 1.835821  [55168/60000]\n",
      "loss: 1.939867  [55232/60000]\n",
      "loss: 1.819161  [55296/60000]\n",
      "loss: 1.933498  [55360/60000]\n",
      "loss: 1.975178  [55424/60000]\n",
      "loss: 1.888996  [55488/60000]\n",
      "loss: 1.825748  [55552/60000]\n",
      "loss: 1.904523  [55616/60000]\n",
      "loss: 1.796149  [55680/60000]\n",
      "loss: 1.837943  [55744/60000]\n",
      "loss: 1.902548  [55808/60000]\n",
      "loss: 1.915142  [55872/60000]\n",
      "loss: 1.878625  [55936/60000]\n",
      "loss: 1.898057  [56000/60000]\n",
      "loss: 1.908894  [56064/60000]\n",
      "loss: 1.864305  [56128/60000]\n",
      "loss: 1.818046  [56192/60000]\n",
      "loss: 1.865954  [56256/60000]\n",
      "loss: 1.908154  [56320/60000]\n",
      "loss: 1.831698  [56384/60000]\n",
      "loss: 1.883526  [56448/60000]\n",
      "loss: 1.933749  [56512/60000]\n",
      "loss: 1.857337  [56576/60000]\n",
      "loss: 1.876195  [56640/60000]\n",
      "loss: 1.918447  [56704/60000]\n",
      "loss: 1.831304  [56768/60000]\n",
      "loss: 1.892008  [56832/60000]\n",
      "loss: 1.927586  [56896/60000]\n",
      "loss: 1.810415  [56960/60000]\n",
      "loss: 1.865761  [57024/60000]\n",
      "loss: 1.882028  [57088/60000]\n",
      "loss: 1.772043  [57152/60000]\n",
      "loss: 1.871418  [57216/60000]\n",
      "loss: 1.811668  [57280/60000]\n",
      "loss: 1.898553  [57344/60000]\n",
      "loss: 1.858421  [57408/60000]\n",
      "loss: 1.834782  [57472/60000]\n",
      "loss: 1.876587  [57536/60000]\n",
      "loss: 1.832329  [57600/60000]\n",
      "loss: 1.839981  [57664/60000]\n",
      "loss: 1.851206  [57728/60000]\n",
      "loss: 1.804767  [57792/60000]\n",
      "loss: 1.836884  [57856/60000]\n",
      "loss: 1.829789  [57920/60000]\n",
      "loss: 1.829759  [57984/60000]\n",
      "loss: 1.880414  [58048/60000]\n",
      "loss: 1.843456  [58112/60000]\n",
      "loss: 1.862939  [58176/60000]\n",
      "loss: 1.867060  [58240/60000]\n",
      "loss: 1.922423  [58304/60000]\n",
      "loss: 1.874064  [58368/60000]\n",
      "loss: 1.856119  [58432/60000]\n",
      "loss: 1.936530  [58496/60000]\n",
      "loss: 1.901161  [58560/60000]\n",
      "loss: 1.829731  [58624/60000]\n",
      "loss: 1.815933  [58688/60000]\n",
      "loss: 1.892508  [58752/60000]\n",
      "loss: 1.771635  [58816/60000]\n",
      "loss: 1.844598  [58880/60000]\n",
      "loss: 1.939489  [58944/60000]\n",
      "loss: 1.883871  [59008/60000]\n",
      "loss: 1.854412  [59072/60000]\n",
      "loss: 1.832631  [59136/60000]\n",
      "loss: 1.853277  [59200/60000]\n",
      "loss: 1.921714  [59264/60000]\n",
      "loss: 1.883772  [59328/60000]\n",
      "loss: 1.880480  [59392/60000]\n",
      "loss: 1.842249  [59456/60000]\n",
      "loss: 1.848941  [59520/60000]\n",
      "loss: 1.845238  [59584/60000]\n",
      "loss: 1.868060  [59648/60000]\n",
      "loss: 1.898269  [59712/60000]\n",
      "loss: 1.796315  [59776/60000]\n",
      "loss: 1.871712  [59840/60000]\n",
      "loss: 1.819096  [59904/60000]\n",
      "loss: 1.795430  [29984/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 1.849422 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.913522  [   64/60000]\n",
      "loss: 1.858534  [  128/60000]\n",
      "loss: 1.864690  [  192/60000]\n",
      "loss: 1.934813  [  256/60000]\n",
      "loss: 1.943716  [  320/60000]\n",
      "loss: 1.877014  [  384/60000]\n",
      "loss: 1.848716  [  448/60000]\n",
      "loss: 1.932383  [  512/60000]\n",
      "loss: 1.876196  [  576/60000]\n",
      "loss: 1.826985  [  640/60000]\n",
      "loss: 1.895565  [  704/60000]\n",
      "loss: 1.877512  [  768/60000]\n",
      "loss: 1.813367  [  832/60000]\n",
      "loss: 1.832696  [  896/60000]\n",
      "loss: 1.908192  [  960/60000]\n",
      "loss: 1.862086  [ 1024/60000]\n",
      "loss: 1.943572  [ 1088/60000]\n",
      "loss: 1.814180  [ 1152/60000]\n",
      "loss: 1.921990  [ 1216/60000]\n",
      "loss: 1.885550  [ 1280/60000]\n",
      "loss: 1.912050  [ 1344/60000]\n",
      "loss: 1.837339  [ 1408/60000]\n",
      "loss: 1.825079  [ 1472/60000]\n",
      "loss: 1.857514  [ 1536/60000]\n",
      "loss: 1.809529  [ 1600/60000]\n",
      "loss: 1.820925  [ 1664/60000]\n",
      "loss: 1.836421  [ 1728/60000]\n",
      "loss: 1.925562  [ 1792/60000]\n",
      "loss: 1.900135  [ 1856/60000]\n",
      "loss: 1.801531  [ 1920/60000]\n",
      "loss: 1.812461  [ 1984/60000]\n",
      "loss: 1.888725  [ 2048/60000]\n",
      "loss: 1.888014  [ 2112/60000]\n",
      "loss: 1.838199  [ 2176/60000]\n",
      "loss: 1.821464  [ 2240/60000]\n",
      "loss: 1.882589  [ 2304/60000]\n",
      "loss: 1.750109  [ 2368/60000]\n",
      "loss: 1.906744  [ 2432/60000]\n",
      "loss: 1.798708  [ 2496/60000]\n",
      "loss: 1.918514  [ 2560/60000]\n",
      "loss: 1.864003  [ 2624/60000]\n",
      "loss: 1.852915  [ 2688/60000]\n",
      "loss: 1.851375  [ 2752/60000]\n",
      "loss: 1.908534  [ 2816/60000]\n",
      "loss: 1.881758  [ 2880/60000]\n",
      "loss: 1.827487  [ 2944/60000]\n",
      "loss: 1.873713  [ 3008/60000]\n",
      "loss: 1.880416  [ 3072/60000]\n",
      "loss: 1.826265  [ 3136/60000]\n",
      "loss: 1.827569  [ 3200/60000]\n",
      "loss: 1.906309  [ 3264/60000]\n",
      "loss: 1.852916  [ 3328/60000]\n",
      "loss: 1.782705  [ 3392/60000]\n",
      "loss: 1.824789  [ 3456/60000]\n",
      "loss: 1.875726  [ 3520/60000]\n",
      "loss: 1.872218  [ 3584/60000]\n",
      "loss: 1.880292  [ 3648/60000]\n",
      "loss: 1.835958  [ 3712/60000]\n",
      "loss: 1.867768  [ 3776/60000]\n",
      "loss: 1.860321  [ 3840/60000]\n",
      "loss: 1.799562  [ 3904/60000]\n",
      "loss: 1.860888  [ 3968/60000]\n",
      "loss: 1.834829  [ 4032/60000]\n",
      "loss: 1.823910  [ 4096/60000]\n",
      "loss: 1.798864  [ 4160/60000]\n",
      "loss: 1.890328  [ 4224/60000]\n",
      "loss: 1.885795  [ 4288/60000]\n",
      "loss: 1.846890  [ 4352/60000]\n",
      "loss: 1.880665  [ 4416/60000]\n",
      "loss: 1.801853  [ 4480/60000]\n",
      "loss: 1.844233  [ 4544/60000]\n",
      "loss: 1.837583  [ 4608/60000]\n",
      "loss: 1.758455  [ 4672/60000]\n",
      "loss: 1.835898  [ 4736/60000]\n",
      "loss: 1.792122  [ 4800/60000]\n",
      "loss: 1.833130  [ 4864/60000]\n",
      "loss: 1.932899  [ 4928/60000]\n",
      "loss: 1.880632  [ 4992/60000]\n",
      "loss: 1.782431  [ 5056/60000]\n",
      "loss: 1.901710  [ 5120/60000]\n",
      "loss: 1.817009  [ 5184/60000]\n",
      "loss: 1.785996  [ 5248/60000]\n",
      "loss: 1.877015  [ 5312/60000]\n",
      "loss: 1.867146  [ 5376/60000]\n",
      "loss: 1.888180  [ 5440/60000]\n",
      "loss: 1.824099  [ 5504/60000]\n",
      "loss: 1.858818  [ 5568/60000]\n",
      "loss: 1.850568  [ 5632/60000]\n",
      "loss: 1.794919  [ 5696/60000]\n",
      "loss: 1.831942  [ 5760/60000]\n",
      "loss: 1.919355  [ 5824/60000]\n",
      "loss: 1.788728  [ 5888/60000]\n",
      "loss: 1.820303  [ 5952/60000]\n",
      "loss: 1.872402  [ 6016/60000]\n",
      "loss: 1.810781  [ 6080/60000]\n",
      "loss: 1.775443  [ 6144/60000]\n",
      "loss: 1.839361  [ 6208/60000]\n",
      "loss: 1.830887  [ 6272/60000]\n",
      "loss: 1.895988  [ 6336/60000]\n",
      "loss: 1.838633  [ 6400/60000]\n",
      "loss: 1.840873  [ 6464/60000]\n",
      "loss: 1.877982  [ 6528/60000]\n",
      "loss: 1.879890  [ 6592/60000]\n",
      "loss: 1.792631  [ 6656/60000]\n",
      "loss: 1.801988  [ 6720/60000]\n",
      "loss: 1.795645  [ 6784/60000]\n",
      "loss: 1.831330  [ 6848/60000]\n",
      "loss: 1.805794  [ 6912/60000]\n",
      "loss: 1.906945  [ 6976/60000]\n",
      "loss: 1.836134  [ 7040/60000]\n",
      "loss: 1.812454  [ 7104/60000]\n",
      "loss: 1.854704  [ 7168/60000]\n",
      "loss: 1.884010  [ 7232/60000]\n",
      "loss: 1.787875  [ 7296/60000]\n",
      "loss: 1.835339  [ 7360/60000]\n",
      "loss: 1.743838  [ 7424/60000]\n",
      "loss: 1.816646  [ 7488/60000]\n",
      "loss: 1.825959  [ 7552/60000]\n",
      "loss: 1.792468  [ 7616/60000]\n",
      "loss: 1.825342  [ 7680/60000]\n",
      "loss: 1.833926  [ 7744/60000]\n",
      "loss: 1.870051  [ 7808/60000]\n",
      "loss: 1.839833  [ 7872/60000]\n",
      "loss: 1.842467  [ 7936/60000]\n",
      "loss: 1.871349  [ 8000/60000]\n",
      "loss: 1.745233  [ 8064/60000]\n",
      "loss: 1.793226  [ 8128/60000]\n",
      "loss: 1.783510  [ 8192/60000]\n",
      "loss: 1.834866  [ 8256/60000]\n",
      "loss: 1.880838  [ 8320/60000]\n",
      "loss: 1.861300  [ 8384/60000]\n",
      "loss: 1.863699  [ 8448/60000]\n",
      "loss: 1.785927  [ 8512/60000]\n",
      "loss: 1.832270  [ 8576/60000]\n",
      "loss: 1.805344  [ 8640/60000]\n",
      "loss: 1.803871  [ 8704/60000]\n",
      "loss: 1.856734  [ 8768/60000]\n",
      "loss: 1.851599  [ 8832/60000]\n",
      "loss: 1.825794  [ 8896/60000]\n",
      "loss: 1.835044  [ 8960/60000]\n",
      "loss: 1.849157  [ 9024/60000]\n",
      "loss: 1.722813  [ 9088/60000]\n",
      "loss: 1.869151  [ 9152/60000]\n",
      "loss: 1.828199  [ 9216/60000]\n",
      "loss: 1.837719  [ 9280/60000]\n",
      "loss: 1.816120  [ 9344/60000]\n",
      "loss: 1.877946  [ 9408/60000]\n",
      "loss: 1.817968  [ 9472/60000]\n",
      "loss: 1.829126  [ 9536/60000]\n",
      "loss: 1.801210  [ 9600/60000]\n",
      "loss: 1.795612  [ 9664/60000]\n",
      "loss: 1.755550  [ 9728/60000]\n",
      "loss: 1.781785  [ 9792/60000]\n",
      "loss: 1.842542  [ 9856/60000]\n",
      "loss: 1.791778  [ 9920/60000]\n",
      "loss: 1.820617  [ 9984/60000]\n",
      "loss: 1.829219  [10048/60000]\n",
      "loss: 1.817666  [10112/60000]\n",
      "loss: 1.778591  [10176/60000]\n",
      "loss: 1.791736  [10240/60000]\n",
      "loss: 1.822681  [10304/60000]\n",
      "loss: 1.770103  [10368/60000]\n",
      "loss: 1.887553  [10432/60000]\n",
      "loss: 1.862362  [10496/60000]\n",
      "loss: 1.853594  [10560/60000]\n",
      "loss: 1.898463  [10624/60000]\n",
      "loss: 1.795456  [10688/60000]\n",
      "loss: 1.765336  [10752/60000]\n",
      "loss: 1.878082  [10816/60000]\n",
      "loss: 1.842707  [10880/60000]\n",
      "loss: 1.788722  [10944/60000]\n",
      "loss: 1.863068  [11008/60000]\n",
      "loss: 1.781315  [11072/60000]\n",
      "loss: 1.809174  [11136/60000]\n",
      "loss: 1.858363  [11200/60000]\n",
      "loss: 1.818869  [11264/60000]\n",
      "loss: 1.848343  [11328/60000]\n",
      "loss: 1.835595  [11392/60000]\n",
      "loss: 1.820838  [11456/60000]\n",
      "loss: 1.747773  [11520/60000]\n",
      "loss: 1.854024  [11584/60000]\n",
      "loss: 1.769315  [11648/60000]\n",
      "loss: 1.785038  [11712/60000]\n",
      "loss: 1.803918  [11776/60000]\n",
      "loss: 1.809752  [11840/60000]\n",
      "loss: 1.821715  [11904/60000]\n",
      "loss: 1.812959  [11968/60000]\n",
      "loss: 1.798406  [12032/60000]\n",
      "loss: 1.735492  [12096/60000]\n",
      "loss: 1.838625  [12160/60000]\n",
      "loss: 1.819598  [12224/60000]\n",
      "loss: 1.810425  [12288/60000]\n",
      "loss: 1.832860  [12352/60000]\n",
      "loss: 1.874078  [12416/60000]\n",
      "loss: 1.752049  [12480/60000]\n",
      "loss: 1.843527  [12544/60000]\n",
      "loss: 1.857456  [12608/60000]\n",
      "loss: 1.757801  [12672/60000]\n",
      "loss: 1.850281  [12736/60000]\n",
      "loss: 1.821977  [12800/60000]\n",
      "loss: 1.828496  [12864/60000]\n",
      "loss: 1.813793  [12928/60000]\n",
      "loss: 1.868795  [12992/60000]\n",
      "loss: 1.748123  [13056/60000]\n",
      "loss: 1.805152  [13120/60000]\n",
      "loss: 1.763689  [13184/60000]\n",
      "loss: 1.811522  [13248/60000]\n",
      "loss: 1.731367  [13312/60000]\n",
      "loss: 1.810881  [13376/60000]\n",
      "loss: 1.776117  [13440/60000]\n",
      "loss: 1.770575  [13504/60000]\n",
      "loss: 1.785886  [13568/60000]\n",
      "loss: 1.769246  [13632/60000]\n",
      "loss: 1.809819  [13696/60000]\n",
      "loss: 1.822729  [13760/60000]\n",
      "loss: 1.747992  [13824/60000]\n",
      "loss: 1.811737  [13888/60000]\n",
      "loss: 1.904956  [13952/60000]\n",
      "loss: 1.791800  [14016/60000]\n",
      "loss: 1.824089  [14080/60000]\n",
      "loss: 1.786124  [14144/60000]\n",
      "loss: 1.827363  [14208/60000]\n",
      "loss: 1.802346  [14272/60000]\n",
      "loss: 1.802583  [14336/60000]\n",
      "loss: 1.794542  [14400/60000]\n",
      "loss: 1.838721  [14464/60000]\n",
      "loss: 1.743867  [14528/60000]\n",
      "loss: 1.754867  [14592/60000]\n",
      "loss: 1.756843  [14656/60000]\n",
      "loss: 1.755588  [14720/60000]\n",
      "loss: 1.743460  [14784/60000]\n",
      "loss: 1.805999  [14848/60000]\n",
      "loss: 1.833254  [14912/60000]\n",
      "loss: 1.826585  [14976/60000]\n",
      "loss: 1.777782  [15040/60000]\n",
      "loss: 1.793032  [15104/60000]\n",
      "loss: 1.809490  [15168/60000]\n",
      "loss: 1.810818  [15232/60000]\n",
      "loss: 1.769750  [15296/60000]\n",
      "loss: 1.847024  [15360/60000]\n",
      "loss: 1.777459  [15424/60000]\n",
      "loss: 1.799513  [15488/60000]\n",
      "loss: 1.781398  [15552/60000]\n",
      "loss: 1.799696  [15616/60000]\n",
      "loss: 1.797574  [15680/60000]\n",
      "loss: 1.718991  [15744/60000]\n",
      "loss: 1.756464  [15808/60000]\n",
      "loss: 1.836103  [15872/60000]\n",
      "loss: 1.756367  [15936/60000]\n",
      "loss: 1.737199  [16000/60000]\n",
      "loss: 1.758819  [16064/60000]\n",
      "loss: 1.800954  [16128/60000]\n",
      "loss: 1.774854  [16192/60000]\n",
      "loss: 1.803281  [16256/60000]\n",
      "loss: 1.827145  [16320/60000]\n",
      "loss: 1.804693  [16384/60000]\n",
      "loss: 1.782399  [16448/60000]\n",
      "loss: 1.756356  [16512/60000]\n",
      "loss: 1.706552  [16576/60000]\n",
      "loss: 1.762676  [16640/60000]\n",
      "loss: 1.770588  [16704/60000]\n",
      "loss: 1.672386  [16768/60000]\n",
      "loss: 1.781159  [16832/60000]\n",
      "loss: 1.762720  [16896/60000]\n",
      "loss: 1.764547  [16960/60000]\n",
      "loss: 1.758205  [17024/60000]\n",
      "loss: 1.671119  [17088/60000]\n",
      "loss: 1.805197  [17152/60000]\n",
      "loss: 1.761247  [17216/60000]\n",
      "loss: 1.803399  [17280/60000]\n",
      "loss: 1.832257  [17344/60000]\n",
      "loss: 1.811102  [17408/60000]\n",
      "loss: 1.766511  [17472/60000]\n",
      "loss: 1.742507  [17536/60000]\n",
      "loss: 1.746534  [17600/60000]\n",
      "loss: 1.809390  [17664/60000]\n",
      "loss: 1.755242  [17728/60000]\n",
      "loss: 1.756103  [17792/60000]\n",
      "loss: 1.690886  [17856/60000]\n",
      "loss: 1.674421  [17920/60000]\n",
      "loss: 1.763548  [17984/60000]\n",
      "loss: 1.712846  [18048/60000]\n",
      "loss: 1.767747  [18112/60000]\n",
      "loss: 1.782340  [18176/60000]\n",
      "loss: 1.773120  [18240/60000]\n",
      "loss: 1.760491  [18304/60000]\n",
      "loss: 1.847352  [18368/60000]\n",
      "loss: 1.742244  [18432/60000]\n",
      "loss: 1.791622  [18496/60000]\n",
      "loss: 1.799449  [18560/60000]\n",
      "loss: 1.804349  [18624/60000]\n",
      "loss: 1.786008  [18688/60000]\n",
      "loss: 1.830000  [18752/60000]\n",
      "loss: 1.751655  [18816/60000]\n",
      "loss: 1.845416  [18880/60000]\n",
      "loss: 1.722808  [18944/60000]\n",
      "loss: 1.853978  [19008/60000]\n",
      "loss: 1.740602  [19072/60000]\n",
      "loss: 1.756551  [19136/60000]\n",
      "loss: 1.739987  [19200/60000]\n",
      "loss: 1.809660  [19264/60000]\n",
      "loss: 1.825805  [19328/60000]\n",
      "loss: 1.778534  [19392/60000]\n",
      "loss: 1.691305  [19456/60000]\n",
      "loss: 1.820600  [19520/60000]\n",
      "loss: 1.794768  [19584/60000]\n",
      "loss: 1.722408  [19648/60000]\n",
      "loss: 1.814089  [19712/60000]\n",
      "loss: 1.738638  [19776/60000]\n",
      "loss: 1.848518  [19840/60000]\n",
      "loss: 1.779747  [19904/60000]\n",
      "loss: 1.719445  [19968/60000]\n",
      "loss: 1.765848  [20032/60000]\n",
      "loss: 1.793575  [20096/60000]\n",
      "loss: 1.752758  [20160/60000]\n",
      "loss: 1.708221  [20224/60000]\n",
      "loss: 1.755110  [20288/60000]\n",
      "loss: 1.717308  [20352/60000]\n",
      "loss: 1.707450  [20416/60000]\n",
      "loss: 1.777634  [20480/60000]\n",
      "loss: 1.834103  [20544/60000]\n",
      "loss: 1.757123  [20608/60000]\n",
      "loss: 1.811937  [20672/60000]\n",
      "loss: 1.846492  [20736/60000]\n",
      "loss: 1.797664  [20800/60000]\n",
      "loss: 1.783957  [20864/60000]\n",
      "loss: 1.795942  [20928/60000]\n",
      "loss: 1.814749  [20992/60000]\n",
      "loss: 1.769521  [21056/60000]\n",
      "loss: 1.772774  [21120/60000]\n",
      "loss: 1.828634  [21184/60000]\n",
      "loss: 1.768685  [21248/60000]\n",
      "loss: 1.799760  [21312/60000]\n",
      "loss: 1.794911  [21376/60000]\n",
      "loss: 1.743438  [21440/60000]\n",
      "loss: 1.807295  [21504/60000]\n",
      "loss: 1.730852  [21568/60000]\n",
      "loss: 1.796471  [21632/60000]\n",
      "loss: 1.710481  [21696/60000]\n",
      "loss: 1.718258  [21760/60000]\n",
      "loss: 1.724544  [21824/60000]\n",
      "loss: 1.773510  [21888/60000]\n",
      "loss: 1.774537  [21952/60000]\n",
      "loss: 1.747480  [22016/60000]\n",
      "loss: 1.724847  [22080/60000]\n",
      "loss: 1.699929  [22144/60000]\n",
      "loss: 1.679464  [22208/60000]\n",
      "loss: 1.771906  [22272/60000]\n",
      "loss: 1.631673  [22336/60000]\n",
      "loss: 1.741349  [22400/60000]\n",
      "loss: 1.862695  [22464/60000]\n",
      "loss: 1.716856  [22528/60000]\n",
      "loss: 1.824120  [22592/60000]\n",
      "loss: 1.763081  [22656/60000]\n",
      "loss: 1.738942  [22720/60000]\n",
      "loss: 1.739466  [22784/60000]\n",
      "loss: 1.809038  [22848/60000]\n",
      "loss: 1.707395  [22912/60000]\n",
      "loss: 1.759378  [22976/60000]\n",
      "loss: 1.788497  [23040/60000]\n",
      "loss: 1.753647  [23104/60000]\n",
      "loss: 1.783610  [23168/60000]\n",
      "loss: 1.709979  [23232/60000]\n",
      "loss: 1.742269  [23296/60000]\n",
      "loss: 1.746388  [23360/60000]\n",
      "loss: 1.831174  [23424/60000]\n",
      "loss: 1.675857  [23488/60000]\n",
      "loss: 1.670995  [23552/60000]\n",
      "loss: 1.785028  [23616/60000]\n",
      "loss: 1.707797  [23680/60000]\n",
      "loss: 1.753885  [23744/60000]\n",
      "loss: 1.811623  [23808/60000]\n",
      "loss: 1.708408  [23872/60000]\n",
      "loss: 1.819597  [23936/60000]\n",
      "loss: 1.774617  [24000/60000]\n",
      "loss: 1.726066  [24064/60000]\n",
      "loss: 1.720272  [24128/60000]\n",
      "loss: 1.804331  [24192/60000]\n",
      "loss: 1.699172  [24256/60000]\n",
      "loss: 1.672559  [24320/60000]\n",
      "loss: 1.701292  [24384/60000]\n",
      "loss: 1.806296  [24448/60000]\n",
      "loss: 1.761224  [24512/60000]\n",
      "loss: 1.726046  [24576/60000]\n",
      "loss: 1.722799  [24640/60000]\n",
      "loss: 1.696265  [24704/60000]\n",
      "loss: 1.772866  [24768/60000]\n",
      "loss: 1.796331  [24832/60000]\n",
      "loss: 1.767085  [24896/60000]\n",
      "loss: 1.824739  [24960/60000]\n",
      "loss: 1.757679  [25024/60000]\n",
      "loss: 1.776724  [25088/60000]\n",
      "loss: 1.738390  [25152/60000]\n",
      "loss: 1.748346  [25216/60000]\n",
      "loss: 1.730669  [25280/60000]\n",
      "loss: 1.663594  [25344/60000]\n",
      "loss: 1.779973  [25408/60000]\n",
      "loss: 1.730287  [25472/60000]\n",
      "loss: 1.716778  [25536/60000]\n",
      "loss: 1.730173  [25600/60000]\n",
      "loss: 1.701716  [25664/60000]\n",
      "loss: 1.696187  [25728/60000]\n",
      "loss: 1.775881  [25792/60000]\n",
      "loss: 1.736745  [25856/60000]\n",
      "loss: 1.677758  [25920/60000]\n",
      "loss: 1.769328  [25984/60000]\n",
      "loss: 1.779229  [26048/60000]\n",
      "loss: 1.778789  [26112/60000]\n",
      "loss: 1.670914  [26176/60000]\n",
      "loss: 1.733910  [26240/60000]\n",
      "loss: 1.718559  [26304/60000]\n",
      "loss: 1.821704  [26368/60000]\n",
      "loss: 1.748932  [26432/60000]\n",
      "loss: 1.715519  [26496/60000]\n",
      "loss: 1.742281  [26560/60000]\n",
      "loss: 1.725910  [26624/60000]\n",
      "loss: 1.734810  [26688/60000]\n",
      "loss: 1.666203  [26752/60000]\n",
      "loss: 1.682857  [26816/60000]\n",
      "loss: 1.804634  [26880/60000]\n",
      "loss: 1.676263  [26944/60000]\n",
      "loss: 1.714796  [27008/60000]\n",
      "loss: 1.716478  [27072/60000]\n",
      "loss: 1.654971  [27136/60000]\n",
      "loss: 1.825919  [27200/60000]\n",
      "loss: 1.791137  [27264/60000]\n",
      "loss: 1.794745  [27328/60000]\n",
      "loss: 1.781602  [27392/60000]\n",
      "loss: 1.661221  [27456/60000]\n",
      "loss: 1.737968  [27520/60000]\n",
      "loss: 1.733781  [27584/60000]\n",
      "loss: 1.797348  [27648/60000]\n",
      "loss: 1.654268  [27712/60000]\n",
      "loss: 1.776250  [27776/60000]\n",
      "loss: 1.737117  [27840/60000]\n",
      "loss: 1.767532  [27904/60000]\n",
      "loss: 1.702241  [27968/60000]\n",
      "loss: 1.729180  [28032/60000]\n",
      "loss: 1.717807  [28096/60000]\n",
      "loss: 1.802677  [28160/60000]\n",
      "loss: 1.746580  [28224/60000]\n",
      "loss: 1.723823  [28288/60000]\n",
      "loss: 1.719195  [28352/60000]\n",
      "loss: 1.742475  [28416/60000]\n",
      "loss: 1.657454  [28480/60000]\n",
      "loss: 1.683335  [28544/60000]\n",
      "loss: 1.753274  [28608/60000]\n",
      "loss: 1.789715  [28672/60000]\n",
      "loss: 1.694670  [28736/60000]\n",
      "loss: 1.711827  [28800/60000]\n",
      "loss: 1.692753  [28864/60000]\n",
      "loss: 1.654021  [28928/60000]\n",
      "loss: 1.667370  [28992/60000]\n",
      "loss: 1.757771  [29056/60000]\n",
      "loss: 1.670566  [29120/60000]\n",
      "loss: 1.703852  [29184/60000]\n",
      "loss: 1.690267  [29248/60000]\n",
      "loss: 1.741922  [29312/60000]\n",
      "loss: 1.722159  [29376/60000]\n",
      "loss: 1.694657  [29440/60000]\n",
      "loss: 1.774241  [29504/60000]\n",
      "loss: 1.674599  [29568/60000]\n",
      "loss: 1.654872  [29632/60000]\n",
      "loss: 1.789755  [29696/60000]\n",
      "loss: 1.704278  [29760/60000]\n",
      "loss: 1.743752  [29824/60000]\n",
      "loss: 1.751640  [29888/60000]\n",
      "loss: 1.666140  [29952/60000]\n",
      "loss: 1.696972  [30016/60000]\n",
      "loss: 1.738520  [30080/60000]\n",
      "loss: 1.704420  [30144/60000]\n",
      "loss: 1.729351  [30208/60000]\n",
      "loss: 1.698572  [30272/60000]\n",
      "loss: 1.690880  [30336/60000]\n",
      "loss: 1.714502  [30400/60000]\n",
      "loss: 1.726718  [30464/60000]\n",
      "loss: 1.726027  [30528/60000]\n",
      "loss: 1.755838  [30592/60000]\n",
      "loss: 1.751689  [30656/60000]\n",
      "loss: 1.646540  [30720/60000]\n",
      "loss: 1.752689  [30784/60000]\n",
      "loss: 1.742118  [30848/60000]\n",
      "loss: 1.623763  [30912/60000]\n",
      "loss: 1.750590  [30976/60000]\n",
      "loss: 1.711542  [31040/60000]\n",
      "loss: 1.718295  [31104/60000]\n",
      "loss: 1.710454  [31168/60000]\n",
      "loss: 1.718578  [31232/60000]\n",
      "loss: 1.683678  [31296/60000]\n",
      "loss: 1.739055  [31360/60000]\n",
      "loss: 1.720229  [31424/60000]\n",
      "loss: 1.650937  [31488/60000]\n",
      "loss: 1.690915  [31552/60000]\n",
      "loss: 1.646469  [31616/60000]\n",
      "loss: 1.755156  [31680/60000]\n",
      "loss: 1.695864  [31744/60000]\n",
      "loss: 1.674121  [31808/60000]\n",
      "loss: 1.655024  [31872/60000]\n",
      "loss: 1.660208  [31936/60000]\n",
      "loss: 1.789908  [32000/60000]\n",
      "loss: 1.738686  [32064/60000]\n",
      "loss: 1.641066  [32128/60000]\n",
      "loss: 1.674112  [32192/60000]\n",
      "loss: 1.809125  [32256/60000]\n",
      "loss: 1.676103  [32320/60000]\n",
      "loss: 1.708516  [32384/60000]\n",
      "loss: 1.672509  [32448/60000]\n",
      "loss: 1.691284  [32512/60000]\n",
      "loss: 1.796637  [32576/60000]\n",
      "loss: 1.745743  [32640/60000]\n",
      "loss: 1.648889  [32704/60000]\n",
      "loss: 1.715886  [32768/60000]\n",
      "loss: 1.706118  [32832/60000]\n",
      "loss: 1.714995  [32896/60000]\n",
      "loss: 1.699742  [32960/60000]\n",
      "loss: 1.670531  [33024/60000]\n",
      "loss: 1.762454  [33088/60000]\n",
      "loss: 1.662339  [33152/60000]\n",
      "loss: 1.712491  [33216/60000]\n",
      "loss: 1.678236  [33280/60000]\n",
      "loss: 1.713964  [33344/60000]\n",
      "loss: 1.657693  [33408/60000]\n",
      "loss: 1.769193  [33472/60000]\n",
      "loss: 1.697294  [33536/60000]\n",
      "loss: 1.667539  [33600/60000]\n",
      "loss: 1.712790  [33664/60000]\n",
      "loss: 1.713732  [33728/60000]\n",
      "loss: 1.812115  [33792/60000]\n",
      "loss: 1.735467  [33856/60000]\n",
      "loss: 1.710853  [33920/60000]\n",
      "loss: 1.690610  [33984/60000]\n",
      "loss: 1.714360  [34048/60000]\n",
      "loss: 1.654974  [34112/60000]\n",
      "loss: 1.720444  [34176/60000]\n",
      "loss: 1.661666  [34240/60000]\n",
      "loss: 1.692290  [34304/60000]\n",
      "loss: 1.703466  [34368/60000]\n",
      "loss: 1.678601  [34432/60000]\n",
      "loss: 1.683923  [34496/60000]\n",
      "loss: 1.737906  [34560/60000]\n",
      "loss: 1.782048  [34624/60000]\n",
      "loss: 1.657486  [34688/60000]\n",
      "loss: 1.720263  [34752/60000]\n",
      "loss: 1.599887  [34816/60000]\n",
      "loss: 1.712523  [34880/60000]\n",
      "loss: 1.691422  [34944/60000]\n",
      "loss: 1.727219  [35008/60000]\n",
      "loss: 1.730841  [35072/60000]\n",
      "loss: 1.621188  [35136/60000]\n",
      "loss: 1.683773  [35200/60000]\n",
      "loss: 1.684116  [35264/60000]\n",
      "loss: 1.632578  [35328/60000]\n",
      "loss: 1.730628  [35392/60000]\n",
      "loss: 1.624891  [35456/60000]\n",
      "loss: 1.640710  [35520/60000]\n",
      "loss: 1.653389  [35584/60000]\n",
      "loss: 1.676073  [35648/60000]\n",
      "loss: 1.706370  [35712/60000]\n",
      "loss: 1.724874  [35776/60000]\n",
      "loss: 1.664254  [35840/60000]\n",
      "loss: 1.578978  [35904/60000]\n",
      "loss: 1.705936  [35968/60000]\n",
      "loss: 1.795581  [36032/60000]\n",
      "loss: 1.671009  [36096/60000]\n",
      "loss: 1.744787  [36160/60000]\n",
      "loss: 1.722977  [36224/60000]\n",
      "loss: 1.630953  [36288/60000]\n",
      "loss: 1.675663  [36352/60000]\n",
      "loss: 1.565412  [36416/60000]\n",
      "loss: 1.708194  [36480/60000]\n",
      "loss: 1.543432  [36544/60000]\n",
      "loss: 1.633174  [36608/60000]\n",
      "loss: 1.725406  [36672/60000]\n",
      "loss: 1.728404  [36736/60000]\n",
      "loss: 1.661484  [36800/60000]\n",
      "loss: 1.717899  [36864/60000]\n",
      "loss: 1.706868  [36928/60000]\n",
      "loss: 1.722439  [36992/60000]\n",
      "loss: 1.631168  [37056/60000]\n",
      "loss: 1.655139  [37120/60000]\n",
      "loss: 1.715470  [37184/60000]\n",
      "loss: 1.634768  [37248/60000]\n",
      "loss: 1.672062  [37312/60000]\n",
      "loss: 1.493859  [37376/60000]\n",
      "loss: 1.777289  [37440/60000]\n",
      "loss: 1.771168  [37504/60000]\n",
      "loss: 1.669170  [37568/60000]\n",
      "loss: 1.696456  [37632/60000]\n",
      "loss: 1.589539  [37696/60000]\n",
      "loss: 1.539193  [37760/60000]\n",
      "loss: 1.718461  [37824/60000]\n",
      "loss: 1.686965  [37888/60000]\n",
      "loss: 1.666796  [37952/60000]\n",
      "loss: 1.731803  [38016/60000]\n",
      "loss: 1.659399  [38080/60000]\n",
      "loss: 1.672433  [38144/60000]\n",
      "loss: 1.674935  [38208/60000]\n",
      "loss: 1.661198  [38272/60000]\n",
      "loss: 1.683688  [38336/60000]\n",
      "loss: 1.666964  [38400/60000]\n",
      "loss: 1.616383  [38464/60000]\n",
      "loss: 1.710671  [38528/60000]\n",
      "loss: 1.734200  [38592/60000]\n",
      "loss: 1.652470  [38656/60000]\n",
      "loss: 1.685198  [38720/60000]\n",
      "loss: 1.707545  [38784/60000]\n",
      "loss: 1.659746  [38848/60000]\n",
      "loss: 1.672849  [38912/60000]\n",
      "loss: 1.647221  [38976/60000]\n",
      "loss: 1.563956  [39040/60000]\n",
      "loss: 1.718729  [39104/60000]\n",
      "loss: 1.695107  [39168/60000]\n",
      "loss: 1.770583  [39232/60000]\n",
      "loss: 1.616882  [39296/60000]\n",
      "loss: 1.646542  [39360/60000]\n",
      "loss: 1.673448  [39424/60000]\n",
      "loss: 1.751949  [39488/60000]\n",
      "loss: 1.618721  [39552/60000]\n",
      "loss: 1.679001  [39616/60000]\n",
      "loss: 1.679616  [39680/60000]\n",
      "loss: 1.614262  [39744/60000]\n",
      "loss: 1.579653  [39808/60000]\n",
      "loss: 1.693937  [39872/60000]\n",
      "loss: 1.657832  [39936/60000]\n",
      "loss: 1.603592  [40000/60000]\n",
      "loss: 1.729458  [40064/60000]\n",
      "loss: 1.666723  [40128/60000]\n",
      "loss: 1.684821  [40192/60000]\n",
      "loss: 1.741261  [40256/60000]\n",
      "loss: 1.708601  [40320/60000]\n",
      "loss: 1.583925  [40384/60000]\n",
      "loss: 1.656604  [40448/60000]\n",
      "loss: 1.696925  [40512/60000]\n",
      "loss: 1.612677  [40576/60000]\n",
      "loss: 1.617715  [40640/60000]\n",
      "loss: 1.685014  [40704/60000]\n",
      "loss: 1.591119  [40768/60000]\n",
      "loss: 1.664717  [40832/60000]\n",
      "loss: 1.563864  [40896/60000]\n",
      "loss: 1.640747  [40960/60000]\n",
      "loss: 1.660245  [41024/60000]\n",
      "loss: 1.728564  [41088/60000]\n",
      "loss: 1.641653  [41152/60000]\n",
      "loss: 1.613626  [41216/60000]\n",
      "loss: 1.702436  [41280/60000]\n",
      "loss: 1.730030  [41344/60000]\n",
      "loss: 1.703351  [41408/60000]\n",
      "loss: 1.701245  [41472/60000]\n",
      "loss: 1.649921  [41536/60000]\n",
      "loss: 1.682911  [41600/60000]\n",
      "loss: 1.624044  [41664/60000]\n",
      "loss: 1.766844  [41728/60000]\n",
      "loss: 1.699146  [41792/60000]\n",
      "loss: 1.614694  [41856/60000]\n",
      "loss: 1.618595  [41920/60000]\n",
      "loss: 1.657215  [41984/60000]\n",
      "loss: 1.589979  [42048/60000]\n",
      "loss: 1.736259  [42112/60000]\n",
      "loss: 1.676230  [42176/60000]\n",
      "loss: 1.553516  [42240/60000]\n",
      "loss: 1.695301  [42304/60000]\n",
      "loss: 1.631047  [42368/60000]\n",
      "loss: 1.621563  [42432/60000]\n",
      "loss: 1.698321  [42496/60000]\n",
      "loss: 1.675651  [42560/60000]\n",
      "loss: 1.725488  [42624/60000]\n",
      "loss: 1.637738  [42688/60000]\n",
      "loss: 1.614697  [42752/60000]\n",
      "loss: 1.710417  [42816/60000]\n",
      "loss: 1.662218  [42880/60000]\n",
      "loss: 1.640159  [42944/60000]\n",
      "loss: 1.576402  [43008/60000]\n",
      "loss: 1.758375  [43072/60000]\n",
      "loss: 1.542369  [43136/60000]\n",
      "loss: 1.636944  [43200/60000]\n",
      "loss: 1.672373  [43264/60000]\n",
      "loss: 1.669345  [43328/60000]\n",
      "loss: 1.739162  [43392/60000]\n",
      "loss: 1.634633  [43456/60000]\n",
      "loss: 1.635692  [43520/60000]\n",
      "loss: 1.602629  [43584/60000]\n",
      "loss: 1.645337  [43648/60000]\n",
      "loss: 1.679689  [43712/60000]\n",
      "loss: 1.627585  [43776/60000]\n",
      "loss: 1.751289  [43840/60000]\n",
      "loss: 1.686247  [43904/60000]\n",
      "loss: 1.552386  [43968/60000]\n",
      "loss: 1.696767  [44032/60000]\n",
      "loss: 1.761666  [44096/60000]\n",
      "loss: 1.695029  [44160/60000]\n",
      "loss: 1.721163  [44224/60000]\n",
      "loss: 1.627327  [44288/60000]\n",
      "loss: 1.642234  [44352/60000]\n",
      "loss: 1.657388  [44416/60000]\n",
      "loss: 1.741240  [44480/60000]\n",
      "loss: 1.704558  [44544/60000]\n",
      "loss: 1.645724  [44608/60000]\n",
      "loss: 1.680660  [44672/60000]\n",
      "loss: 1.730243  [44736/60000]\n",
      "loss: 1.670869  [44800/60000]\n",
      "loss: 1.561819  [44864/60000]\n",
      "loss: 1.641967  [44928/60000]\n",
      "loss: 1.611476  [44992/60000]\n",
      "loss: 1.596314  [45056/60000]\n",
      "loss: 1.588789  [45120/60000]\n",
      "loss: 1.728423  [45184/60000]\n",
      "loss: 1.659386  [45248/60000]\n",
      "loss: 1.672339  [45312/60000]\n",
      "loss: 1.692821  [45376/60000]\n",
      "loss: 1.680506  [45440/60000]\n",
      "loss: 1.662827  [45504/60000]\n",
      "loss: 1.564665  [45568/60000]\n",
      "loss: 1.638264  [45632/60000]\n",
      "loss: 1.640381  [45696/60000]\n",
      "loss: 1.608348  [45760/60000]\n",
      "loss: 1.681861  [45824/60000]\n",
      "loss: 1.632593  [45888/60000]\n",
      "loss: 1.636574  [45952/60000]\n",
      "loss: 1.672798  [46016/60000]\n",
      "loss: 1.592031  [46080/60000]\n",
      "loss: 1.623751  [46144/60000]\n",
      "loss: 1.587967  [46208/60000]\n",
      "loss: 1.699778  [46272/60000]\n",
      "loss: 1.619764  [46336/60000]\n",
      "loss: 1.553161  [46400/60000]\n",
      "loss: 1.641600  [46464/60000]\n",
      "loss: 1.620332  [46528/60000]\n",
      "loss: 1.535915  [46592/60000]\n",
      "loss: 1.536249  [46656/60000]\n",
      "loss: 1.624032  [46720/60000]\n",
      "loss: 1.614605  [46784/60000]\n",
      "loss: 1.627809  [46848/60000]\n",
      "loss: 1.707978  [46912/60000]\n",
      "loss: 1.627071  [46976/60000]\n",
      "loss: 1.591732  [47040/60000]\n",
      "loss: 1.679249  [47104/60000]\n",
      "loss: 1.662463  [47168/60000]\n",
      "loss: 1.669902  [47232/60000]\n",
      "loss: 1.571718  [47296/60000]\n",
      "loss: 1.585081  [47360/60000]\n",
      "loss: 1.615646  [47424/60000]\n",
      "loss: 1.596760  [47488/60000]\n",
      "loss: 1.694171  [47552/60000]\n",
      "loss: 1.566176  [47616/60000]\n",
      "loss: 1.722816  [47680/60000]\n",
      "loss: 1.687028  [47744/60000]\n",
      "loss: 1.649243  [47808/60000]\n",
      "loss: 1.580547  [47872/60000]\n",
      "loss: 1.707623  [47936/60000]\n",
      "loss: 1.673650  [48000/60000]\n",
      "loss: 1.682903  [48064/60000]\n",
      "loss: 1.594549  [48128/60000]\n",
      "loss: 1.653230  [48192/60000]\n",
      "loss: 1.671234  [48256/60000]\n",
      "loss: 1.655295  [48320/60000]\n",
      "loss: 1.600705  [48384/60000]\n",
      "loss: 1.636089  [48448/60000]\n",
      "loss: 1.569509  [48512/60000]\n",
      "loss: 1.612679  [48576/60000]\n",
      "loss: 1.695348  [48640/60000]\n",
      "loss: 1.657184  [48704/60000]\n",
      "loss: 1.644226  [48768/60000]\n",
      "loss: 1.626584  [48832/60000]\n",
      "loss: 1.641729  [48896/60000]\n",
      "loss: 1.645089  [48960/60000]\n",
      "loss: 1.716896  [49024/60000]\n",
      "loss: 1.685827  [49088/60000]\n",
      "loss: 1.631405  [49152/60000]\n",
      "loss: 1.582795  [49216/60000]\n",
      "loss: 1.579768  [49280/60000]\n",
      "loss: 1.692988  [49344/60000]\n",
      "loss: 1.660916  [49408/60000]\n",
      "loss: 1.642594  [49472/60000]\n",
      "loss: 1.689747  [49536/60000]\n",
      "loss: 1.562548  [49600/60000]\n",
      "loss: 1.615425  [49664/60000]\n",
      "loss: 1.682518  [49728/60000]\n",
      "loss: 1.552878  [49792/60000]\n",
      "loss: 1.720921  [49856/60000]\n",
      "loss: 1.637165  [49920/60000]\n",
      "loss: 1.505720  [49984/60000]\n",
      "loss: 1.627614  [50048/60000]\n",
      "loss: 1.623944  [50112/60000]\n",
      "loss: 1.622601  [50176/60000]\n",
      "loss: 1.610480  [50240/60000]\n",
      "loss: 1.661919  [50304/60000]\n",
      "loss: 1.635777  [50368/60000]\n",
      "loss: 1.600904  [50432/60000]\n",
      "loss: 1.643349  [50496/60000]\n",
      "loss: 1.544334  [50560/60000]\n",
      "loss: 1.615870  [50624/60000]\n",
      "loss: 1.605772  [50688/60000]\n",
      "loss: 1.701907  [50752/60000]\n",
      "loss: 1.634092  [50816/60000]\n",
      "loss: 1.647995  [50880/60000]\n",
      "loss: 1.681030  [50944/60000]\n",
      "loss: 1.579043  [51008/60000]\n",
      "loss: 1.576021  [51072/60000]\n",
      "loss: 1.617488  [51136/60000]\n",
      "loss: 1.676028  [51200/60000]\n",
      "loss: 1.650259  [51264/60000]\n",
      "loss: 1.697426  [51328/60000]\n",
      "loss: 1.632961  [51392/60000]\n",
      "loss: 1.574244  [51456/60000]\n",
      "loss: 1.542506  [51520/60000]\n",
      "loss: 1.665533  [51584/60000]\n",
      "loss: 1.717328  [51648/60000]\n",
      "loss: 1.513882  [51712/60000]\n",
      "loss: 1.668182  [51776/60000]\n",
      "loss: 1.652481  [51840/60000]\n",
      "loss: 1.682272  [51904/60000]\n",
      "loss: 1.547527  [51968/60000]\n",
      "loss: 1.577853  [52032/60000]\n",
      "loss: 1.620210  [52096/60000]\n",
      "loss: 1.713462  [52160/60000]\n",
      "loss: 1.580026  [52224/60000]\n",
      "loss: 1.606955  [52288/60000]\n",
      "loss: 1.571143  [52352/60000]\n",
      "loss: 1.610778  [52416/60000]\n",
      "loss: 1.655202  [52480/60000]\n",
      "loss: 1.586186  [52544/60000]\n",
      "loss: 1.632196  [52608/60000]\n",
      "loss: 1.521417  [52672/60000]\n",
      "loss: 1.533532  [52736/60000]\n",
      "loss: 1.633187  [52800/60000]\n",
      "loss: 1.620520  [52864/60000]\n",
      "loss: 1.517502  [52928/60000]\n",
      "loss: 1.508328  [52992/60000]\n",
      "loss: 1.567633  [53056/60000]\n",
      "loss: 1.597593  [53120/60000]\n",
      "loss: 1.679589  [53184/60000]\n",
      "loss: 1.651021  [53248/60000]\n",
      "loss: 1.639576  [53312/60000]\n",
      "loss: 1.642997  [53376/60000]\n",
      "loss: 1.633834  [53440/60000]\n",
      "loss: 1.585640  [53504/60000]\n",
      "loss: 1.583662  [53568/60000]\n",
      "loss: 1.520145  [53632/60000]\n",
      "loss: 1.501803  [53696/60000]\n",
      "loss: 1.559267  [53760/60000]\n",
      "loss: 1.447174  [53824/60000]\n",
      "loss: 1.580080  [53888/60000]\n",
      "loss: 1.623793  [53952/60000]\n",
      "loss: 1.578613  [54016/60000]\n",
      "loss: 1.560304  [54080/60000]\n",
      "loss: 1.529830  [54144/60000]\n",
      "loss: 1.505659  [54208/60000]\n",
      "loss: 1.606365  [54272/60000]\n",
      "loss: 1.583049  [54336/60000]\n",
      "loss: 1.651605  [54400/60000]\n",
      "loss: 1.599173  [54464/60000]\n",
      "loss: 1.590713  [54528/60000]\n",
      "loss: 1.613951  [54592/60000]\n",
      "loss: 1.505333  [54656/60000]\n",
      "loss: 1.569180  [54720/60000]\n",
      "loss: 1.605438  [54784/60000]\n",
      "loss: 1.593441  [54848/60000]\n",
      "loss: 1.576099  [54912/60000]\n",
      "loss: 1.563041  [54976/60000]\n",
      "loss: 1.557371  [55040/60000]\n",
      "loss: 1.647306  [55104/60000]\n",
      "loss: 1.602607  [55168/60000]\n",
      "loss: 1.586626  [55232/60000]\n",
      "loss: 1.538706  [55296/60000]\n",
      "loss: 1.642043  [55360/60000]\n",
      "loss: 1.615428  [55424/60000]\n",
      "loss: 1.677096  [55488/60000]\n",
      "loss: 1.562764  [55552/60000]\n",
      "loss: 1.581375  [55616/60000]\n",
      "loss: 1.589514  [55680/60000]\n",
      "loss: 1.558149  [55744/60000]\n",
      "loss: 1.650747  [55808/60000]\n",
      "loss: 1.665030  [55872/60000]\n",
      "loss: 1.646409  [55936/60000]\n",
      "loss: 1.486541  [56000/60000]\n",
      "loss: 1.595004  [56064/60000]\n",
      "loss: 1.499161  [56128/60000]\n",
      "loss: 1.518083  [56192/60000]\n",
      "loss: 1.590825  [56256/60000]\n",
      "loss: 1.579195  [56320/60000]\n",
      "loss: 1.581563  [56384/60000]\n",
      "loss: 1.540788  [56448/60000]\n",
      "loss: 1.558368  [56512/60000]\n",
      "loss: 1.566846  [56576/60000]\n",
      "loss: 1.632235  [56640/60000]\n",
      "loss: 1.510101  [56704/60000]\n",
      "loss: 1.569889  [56768/60000]\n",
      "loss: 1.625520  [56832/60000]\n",
      "loss: 1.484798  [56896/60000]\n",
      "loss: 1.570783  [56960/60000]\n",
      "loss: 1.595327  [57024/60000]\n",
      "loss: 1.580504  [57088/60000]\n",
      "loss: 1.474980  [57152/60000]\n",
      "loss: 1.520731  [57216/60000]\n",
      "loss: 1.592771  [57280/60000]\n",
      "loss: 1.526701  [57344/60000]\n",
      "loss: 1.559282  [57408/60000]\n",
      "loss: 1.578681  [57472/60000]\n",
      "loss: 1.497424  [57536/60000]\n",
      "loss: 1.534128  [57600/60000]\n",
      "loss: 1.678976  [57664/60000]\n",
      "loss: 1.589658  [57728/60000]\n",
      "loss: 1.681999  [57792/60000]\n",
      "loss: 1.495495  [57856/60000]\n",
      "loss: 1.586078  [57920/60000]\n",
      "loss: 1.640121  [57984/60000]\n",
      "loss: 1.654323  [58048/60000]\n",
      "loss: 1.536793  [58112/60000]\n",
      "loss: 1.668244  [58176/60000]\n",
      "loss: 1.558691  [58240/60000]\n",
      "loss: 1.602866  [58304/60000]\n",
      "loss: 1.523867  [58368/60000]\n",
      "loss: 1.699680  [58432/60000]\n",
      "loss: 1.535717  [58496/60000]\n",
      "loss: 1.683820  [58560/60000]\n",
      "loss: 1.460230  [58624/60000]\n",
      "loss: 1.539618  [58688/60000]\n",
      "loss: 1.507496  [58752/60000]\n",
      "loss: 1.578968  [58816/60000]\n",
      "loss: 1.599483  [58880/60000]\n",
      "loss: 1.570264  [58944/60000]\n",
      "loss: 1.603007  [59008/60000]\n",
      "loss: 1.512794  [59072/60000]\n",
      "loss: 1.658112  [59136/60000]\n",
      "loss: 1.623585  [59200/60000]\n",
      "loss: 1.609565  [59264/60000]\n",
      "loss: 1.579123  [59328/60000]\n",
      "loss: 1.596682  [59392/60000]\n",
      "loss: 1.521711  [59456/60000]\n",
      "loss: 1.536731  [59520/60000]\n",
      "loss: 1.582366  [59584/60000]\n",
      "loss: 1.504837  [59648/60000]\n",
      "loss: 1.539072  [59712/60000]\n",
      "loss: 1.515786  [59776/60000]\n",
      "loss: 1.578974  [59840/60000]\n",
      "loss: 1.523360  [59904/60000]\n",
      "loss: 1.518979  [29984/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 1.550010 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(loaded_train, model, loss_function, optimizer)\n",
    "    test(loaded_test, model, loss_function)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d4105e-0adc-4564-b4a4-7225f42e9d20",
   "metadata": {},
   "source": [
    "Notice that in each epoch, we print the loss function at every 100 batches in the training loop, and it keeps getting lower. Also, after each epoch, we can see the accuracy getting higher as the average loss decreases.\n",
    "\n",
    "If we had set more epochs--let's say 10, 50, or even 100--chances are we'd see even better results, but the outputs would be much longer and much harder to visualize and understand.\n",
    "\n",
    "With our model finally trained, it's easy to save it and load it when necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ab632ee-0e0b-42ca-89b6-844bf472d72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76089/3215243966.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"model.pth\")\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"model.pth\")\n",
    "model = torch.load(\"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803e652-eb58-4030-b822-d42abcabb56d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conclusion\n",
    "In this article, we covered the basics of using PyTorch for deep learning, including:\n",
    "\n",
    "- Tensors and how to use them\n",
    "- How to load and prepare the data\n",
    "- Neural networks and how to define them on PyTorch\n",
    "- How to train your first image classification model\n",
    "- Saving and loading models on PyTorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
